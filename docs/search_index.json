[
["index.html", "STAT 771: My notes Intro", " STAT 771: My notes Ralph Møller Trane Fall 2018 (compiled 2018-10-31) Intro "],
["lecture-notes.html", "Chapter 1 Lecture Notes ", " Chapter 1 Lecture Notes "],
["lecture-1-96.html", "Lecture 1: 9/6", " Lecture 1: 9/6 Goals for the first few lectures: Develop basic understanding of floating point numbers (fp numbers) Develop some basic notions of errors and their consequences References: David Goldberg (1991) John D. Cook (2009) Hingham (2002) "],
["positional-numeral-system.html", "1.1 Positional numeral system", " 1.1 Positional numeral system We assume we have a decimal representation of numbers. I.e. that it exists. It is not within the scope of this class to prove this. Now, this is NOT the optimal way for a computer to represent numbers. For various reasons, there are more desirable ways to store numbers. So we need a different way of representing the numbers. Ingredients for different representation: A base, refered to as \\(\\beta\\). It holds that \\(\\beta \\in {2,3,4,...}\\) A significand: a sequence of digits: \\(d_0.d_1d_2d_3d_4...\\), where \\(d_j \\in \\{0, 1,...,\\beta-1\\}\\) An exponent: \\(e \\in \\mathbb{Z}\\). The representation \\(d_0.d_1d_2... \\times \\beta^e\\) means \\(\\left(d_0 + d_1\\cdot \\beta^{-1} + ... + d_{p-1}\\beta^{-(p-1)}\\right)\\cdot \\beta^e\\). 1.1.1 Floating Point Format Definition 1 A fp is one that can be represented in a base \\(\\beta\\) with a fixed digit \\(p\\) (precision), and whose exponent is between \\(e_{min}\\) and \\(e_{max}\\). Example 1 Let \\(\\beta = 10, p = 3, e_{min} = -1, e_{max} = 1\\). Want to represent \\(0.1\\). Several options: Let d_0=0, d_1 = 0, d_2 = 1, e = 1. Let d_0=0, d_1 = 1, d_2 = 0, e = 0. d_0 = 1, d_1 = 0, d_2 = 0, e = -1. If we fill into the equation above, we get 0.1: \\[\\begin{align*} i: &amp; \\left(0 + 0\\cdot 10^{-1} + 1\\cdot 10^{-2}\\right)\\cdot 10^{1} \\\\ ii: &amp; \\left(0 + 1\\cdot 10^{-1} + 1\\cdot 10^{-2}\\right)\\cdot 10^{0} \\\\ iii:&amp; \\left(1 + 0\\cdot 10^{-1} + 1\\cdot 10^{-2}\\right)\\cdot 10^{-1} \\end{align*}\\] Definition 2 A fp number is said to be normalized if \\(d_0 \\neq 0\\). Exercise 1 What is the total number of values that can be represented in the normalized fp format with base \\(\\beta, p, e_{min}, e_{max}\\)? We count the different values each of the elements of a fp can take: \\(d_0\\) can be from 1 to \\(\\beta-1\\), so \\(\\beta-1\\) different values. \\(d_1,...,d_{p-1}\\) each takes a value in \\(\\{0,1,...,\\beta-1\\}\\). Hence, we can choose the digits \\(d_1,...,d_{p-1}\\) in \\(\\beta^{p-1}\\) different ways. \\(e\\) can take \\(e_{max} - e_{min} + 1\\) different values (all integers from \\(e_{min}\\) to \\(e_{max}\\), both included, hence the \\(+1\\)). So, in total, there are \\((\\beta-1)\\cdot \\beta^{p-1}\\cdot (e_{max} - e_{min} + 1)\\) different values that can be represented in the normalized fp format with base \\(\\beta\\), precision \\(p\\), and \\(e_{min}, e_{max}\\) given. 1.1.2 IEEE Standards IEEE have standards for how to deal with approximations and errors. For our purposes, a bit is a single unit of storage on a computer, which can either be 0 or 1. Hence, we’ll be focusing on fp formats where \\(\\beta = 2\\). 1.1.2.1 The 16 bit standard (half precision standard). The 16 bits of storage are used in the following way, when following the 16 bit standard: 1 bit for the sign 0 = positive 1 = negative 5 bits for the exponent 00000 is reserved for 0 11111 is reserved for \\(\\infty\\) 30 exponents left: \\(2^5 - 2 = 30\\) the 16 bit standard dictates that the used exponents are \\(-14,...,15\\). Note: \\(0\\) is also included in this list of 30 exponents. This is because the \\(00000\\) representation is reserved for integers, while \\(01111\\) is used with non-integers. 11 bit for the significand. 10 are actually stored – we always work with normalized FP numbers, i.e. \\(\\beta_0 = 1\\). Question: What are smallest and largest positive numbers that can be represented? Answer: Smallest non-normalized number would be the one with the smallest possible exponent, and all digits of the significand are 0 except the very last one. So, the smallest non-normalized FP number in the 16 bit standard would be \\[ \\left(0 + 0\\cdot 2^{-1} + ... + 0\\cdot 2^{-9} + 1\\cdot 2^{-10}\\right)\\cdot 2^{-14} = 2^{-24} \\approx 5.96\\cdot 10^{-8} \\] The smallest normalized number is the one with all digits \\(0\\) (except for the leading digit, of course, which has to be \\(1\\) for it to be normalized), and \\(e = -14\\). So the smallest normalized FP number: \\[ \\left(1 + 0\\cdot 2^{-1} + ... + 0\\cdot 2^{-10}\\right)\\cdot 2^{-14} = 2^{-14} \\approx 6.10\\cdot 10^{-5} \\] Finally, the largest (finite) FP number in the 16 bit standard is the one where the exponent is as large as possible (\\(e = 15\\)), and all digits are \\(1\\). So \\[ \\left(1 + 1\\cdot 2^{-1} + ... + 1\\cdot 2^{-10}\\right)\\cdot 2^{15} = 65504 \\] Lecture 2: 9/11 1.1.2.2 The 32 bit standard (single precision) The 32 bits of storage are used in the following way, when following the 32 bit standard: 1 bit for the sign 0 = positive 1 = negative 8 bits for the exponent 00000000 is reserved for 0 11111111 is reserved for \\(\\infty\\) exponents left: \\(2^8 - 2 = 254\\) the 32 bit standard dictates that the used exponents are \\(-126,...,127\\). Note: \\(0\\) is also included in this list of the 254 exponents. This is because the \\(00000000\\) representation is reserved for integers, while \\(01111111\\) (I think this is the representation for \\(0\\) here…) is used with non-integers. 24 bit for the significand. 23 are actually stored – we always work with normalized FP numbers, i.e. \\(\\beta_0 = 1\\). Question: What are smallest and largest positive numbers that can be represented in the 32 bit standard? Answer: Smallest non-normalized number would be the one with the smallest possible exponent, and all digits of the significand are 0 except the very last one. So, the smallest non-normalized FP number in the 32 bit standard would be \\[ \\left(0 + 0\\cdot 2^{-1} + ... + 0\\cdot 2^{-22} + 1\\cdot 2^{-23}\\right)\\cdot 2^{-126} = 2^{-149} \\approx 1.40\\cdot 10^{-45} \\] The smallest normalized number is the one with all digits \\(0\\) (except for the leading digit, of course, which has to be \\(1\\) for it to be normalized), and \\(e = -126\\). So the smallest normalized FP number: \\[ \\left(1 + 0\\cdot 2^{-1} + ... + 0\\cdot 2^{-23}\\right)\\cdot 2^{-126} = 2^{-126} \\approx 1.18\\cdot 10^{-38} \\] Finally, the largest (finite) FP number in the 32 bit standard is the one where the exponent is as large as possible (\\(e = 127\\)), and all digits are \\(1\\). So \\[ \\left(1 + 1\\cdot 2^{-1} + ... + 1\\cdot 2^{-126}\\right)\\cdot 2^{127} = 3.40\\cdot 10^{38} \\] 1.1.2.3 The 64 bit standard (double precision) The 64 bits of storage are used in the following way, when following the 64 bit standard: 1 bit for the sign 0 = positive 1 = negative 11 bits for the exponent 00000000 is reserved for 0 11111111 is reserved for \\(\\infty\\) exponents left: \\(2^11 - 2 = 2046\\) the 64 bit standard dictates that the used exponents are \\(-1024,...,1023\\). Note: \\(0\\) is also included in this list of the 254 exponents. This is because the \\(00000000\\) representation is reserved for integers, while \\(01111111\\) (I think this is the representation for \\(0\\) here…) is used with non-integers. 53 bit for the significand. 52 are actually stored – we always work with normalized FP numbers, i.e. \\(\\beta_0 = 1\\). 1.1.3 Errors 1.1.3.1 Units in the Last Place (ULP) 1.1.3.2 Absolute and Relative Error Let \\(fl: \\mathbb{R}_{\\geq 0} \\rightarrow \\mathcal{S}\\) be a function that takes a real value and return a FP number. Then we define the absolute and relative error as follows: Definition 3 Let \\(z \\in \\mathbb{R}_{\\geq 0}\\). The absolute error is defined as \\[ \\left | fl(z) - z \\right | . \\] The relative error is defined as \\[ \\left | \\frac{fl(z)-z}{z} \\right | \\] Lemma 1 If \\(z\\) has exponent \\(e\\), then the maximum absolute error is \\(\\frac{\\beta^{e-p+1}}{2}\\). Proof. Lemma 2 If \\(z\\) has exponent \\(e\\), then the maximum relative error is \\(\\frac{\\beta^{1-p}}{2}\\). Proof. If \\(z\\) has exponent \\(e\\), then \\(\\beta^{e}\\leq z\\). Using this with 1, we get that \\[ \\left | \\frac{fl(z)-z}{z} \\right | \\leq \\frac{\\beta^{e-p+1}}{2\\beta^e} = \\frac{\\beta^{1-p}}{2}. \\] Note: the upper bound of the relative error is called the machine epsilon. This can be obtained in Julia using the function eps. 1.1.3.3 The Fundamental Axiom … is that for any of the four arithmetic operations (\\(+, -, \\cdot, /\\)), we have the following error bound: \\[ fl(x \\text{op} y) = (x \\text{op} y)(1+\\delta), \\] with \\(|\\delta| \\leq u\\), where \\(u\\) is commonly \\(2\\cdot \\epsilon\\). (NOTE: NEED TO CLARIFY IF THE ABOVE IS CORRECT!) Example: Matrix storage. Let \\(A \\in \\mathbb{R}^{m\\times n}\\). Then: \\[ \\left| fl(A) - A \\right | \\leq u \\left | A \\right | \\] Example: Dot product. Let \\(x,y \\in \\mathbb{R}^{n}\\). Recall that the dot product of \\(x\\) and \\(y\\) is definted as \\(x&#39;y = \\sum_{i=1}^{n} x_i \\cdot y_i\\). This can be calculated in the following way: fl = function(x,y) # Get length of x n = length(x) # Check that length of y is equal to length of x. If not, throw error. if(length(y) != n) return &quot;ERROR: y does not have same dimension as x&quot; end # s will be the result of the dot product calculation s = 0 for i = 1:n s += x[i]*y[i] end return(s) end Next we want to prove the following lemma: Lemma 3 Let \\(x,y \\in \\mathbb{R}^n\\), and \\(n\\cdot u \\leq 0.01\\). Then \\[ \\left | fl(x&#39;y) - x&#39;y \\right | \\leq 1.01 \\cdot n\\cdot u \\cdot \\left|x\\right|&#39;\\left|y\\right| \\] Lecture 3: 9/13 To prove the lemma above, we will need another lemma… Lemma 4 If \\(|\\delta_i| \\leq u, \\forall i=1,\\ldots,n\\) s.t. \\(n\\cdot u &lt; 2\\). Let \\(1 + \\eta = \\prod_{i=1}^{n}(1 + \\delta_i)\\). Then \\[ |\\eta | \\leq \\frac{n\\cdot u}{1-\\tfrac{n\\cdot u}{2}} \\] Proof. Using the definition of \\(\\nu\\), we can rewrite it to get \\[ | \\eta | = \\left | \\prod_{i=1}^n (1 + \\delta_i) - 1 \\right |. \\] By induction, we will show that the expression above is less than or equal to \\((1 + u)^n - 1\\). [TO BE COMPLETED!] Since \\(1+u \\leq e^{u}\\) for all \\(u \\in \\mathbb{R}\\), we have that \\[\\begin{align*} | \\eta | &amp;\\leq e^{n\\cdot u} - 1 \\\\ &amp;\\leq n\\cdot u + \\frac{(n\\cdot u)^2}{2!} + \\frac{(n\\cdot u)^3}{3!} + \\ldots \\text{(used the Taylor expansion)}\\\\ &amp;\\leq n\\cdot u + \\frac{(n\\cdot u)^2}{2^1} + \\frac{(n\\cdot u)^3}{2^2} + \\frac{(n\\cdot u)^4}{2^3} + \\ldots (\\text{used that } x! &gt; 2^{x-1} \\text{ for } x &gt; 1) \\\\ &amp;= \\sum_{k=0}^{\\infty} n\\cdot u \\left(\\frac{n\\cdot u}{2}\\right)^k \\text{\\small (identify this as a geometric series with } r = \\tfrac{n\\cdot u}{2} \\text{, \\small which is less than 1 by assumption)} \\\\ &amp;= \\frac{n\\cdot u}{1-\\tfrac{n\\cdot u}{2}}, \\end{align*}\\] which is exactly what we wanted. With this in hand, we will prove the previously stated lemma. Proof. Let \\(s_p\\) denote the value of \\(s\\) after the \\(p\\)’th iteration of the algorithm described above. Then, since we’re assuming the Fundamental Axiom, we have that \\(s_1 = fl(x_1y_y) = x_1 y_1 (1 + \\delta_1)\\), where \\(|\\delta_1| \\leq u\\). We can similarly find \\(s_p\\) as \\[\\begin{align*} s_p &amp;= fl(s_{p-1} + fl(x_p y_p)) \\\\ &amp;= (s_{p-1} + fl(x_p y_p))(1 + \\epsilon_p) (\\text{where } |\\epsilon_p| \\leq u) \\\\ &amp;= (s_{p-1} + x_py_p(1 + \\delta_p))(1 + \\epsilon_p) (\\text{where } |\\delta_p| \\leq u). \\end{align*}\\] Let \\(\\epsilon_1 = 0\\). \\(s_p\\) is a recursive formula, and can be rewritten as follows: \\[ s_p = \\sum_{i=1}^p x_iy_i (1 + \\delta_i)\\prod_{j=1}^p (1 + \\epsilon_j). \\] So, \\[\\begin{align*} | s_n - x^\\prime y | &amp;= \\left | \\sum_{i=1}^n (x_i y_i)(1 + \\delta_i)\\prod_{j=1}^p (1 + \\epsilon_j) - \\sum_{i=1}^{n}x_iy_i\\right | \\\\ &amp;= \\left | \\sum_{i=1}^n (x_i y_i)\\left((1 + \\delta_i)\\prod_{j=1}^p (1 + \\epsilon_j) - 1 \\right) \\right | \\\\ &amp;\\leq \\sum_{i=1}^n \\left |x_i y_i \\right |\\left | (1 + \\delta_i)\\prod_{j=1}^p (1 + \\epsilon_j) - 1 \\right| . \\end{align*}\\] We now use 4 to get: \\[\\begin{align*} \\sum_{i=1}^n \\left |x_i y_i \\right |\\left | (1 + \\delta_i)\\prod_{j=1}^p (1 + \\epsilon_j) - 1 \\right| &amp;\\leq \\frac{nu}{1-\\tfrac{nu}{2}} \\sum_{i=1}^n \\left |x_i y_i \\right | \\\\ &amp;\\leq \\frac{nu}{0.995} \\sum_{i=1}^n |x_i| |y_i | \\\\ &amp;\\leq 1.01 \\cdot nu \\cdot |x|^\\prime |y| \\end{align*}\\] 1.1.4 Square Linear Systems In the following, let \\(A \\in \\mathbb{R}^{n \\times m}\\) be an invertible matrix, and assume \\(Ax = b\\) for a \\(b \\neq 0\\). This implies that \\(x = A^{-1}b\\). Theorem 1 Let \\(\\kappa_\\infty = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_\\infty \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert_\\infty\\). Assume we can store \\(A\\) with precision \\(E\\) (i.e. as \\(A+E\\)), where \\(\\left \\vert \\left \\vert E \\right \\vert \\right \\vert_\\infty \\leq u \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_\\infty\\), and \\(b\\) with precision \\(e\\) (i.e. as \\(b+e\\)), where \\(\\left \\vert \\left \\vert e \\right \\vert \\right \\vert_\\infty \\leq u \\left \\vert \\left \\vert b \\right \\vert \\right \\vert_\\infty\\). If \\(\\left \\vert \\left \\vert A+E \\right \\vert \\right \\vert\\hat{x} = b+e\\) and \\(u\\cdot \\kappa_\\infty &lt; 1\\), then \\[ \\frac{\\left \\vert \\left \\vert x-\\hat{x} \\right \\vert \\right \\vert_\\infty}{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert} \\leq \\frac{2\\cdot u \\cdot \\kappa_\\infty}{1-u\\cdot \\kappa_\\infty} \\] Lemma 5 Let \\(I\\in \\mathbb{R}^{n \\times n}\\) be the identity matrix, and \\(F \\in \\mathbb{R}^{n\\times n}\\) s.t. \\(\\left \\vert \\left \\vert F \\right \\vert \\right \\vert_p &lt; 1\\) for some \\(p \\in [1,\\infty]\\). Then \\(I-F\\) is invertible, and \\[ \\left \\vert \\left \\vert (I-F)^{-1} \\right \\vert \\right \\vert_p \\leq \\frac{1}{1-\\left \\vert \\left \\vert F \\right \\vert \\right \\vert_p} \\] Proof. HOMEWORK Lemma 6 Suppose \\(\\exists \\epsilon &gt; 0\\) s.t. \\(\\left \\vert \\left \\vert \\Delta A \\right \\vert \\right \\vert \\leq \\epsilon \\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\) and \\(\\left \\vert \\left \\vert \\Delta b \\right \\vert \\right \\vert \\leq \\epsilon \\left \\vert \\left \\vert b \\right \\vert \\right \\vert\\), and \\(y\\) s.t. \\((A+\\Delta A)y = b+\\Delta b\\). If \\(\\epsilon \\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert = r &lt; 1\\), then \\(A+\\Delta A\\) is invertible and \\[\\frac{\\left \\vert \\left \\vert y \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert} \\leq \\frac{1+r}{1-r}.\\] Proof. Note that \\(A+\\Delta A = A\\left (I + A^{-1}\\Delta A\\right ) = A\\left (I - \\left(- A^{-1}\\Delta A\\right)\\right)\\). Since \\(\\left \\vert \\left \\vert -A^{-1}\\Delta A \\right \\vert \\right \\vert = \\left \\vert \\left \\vert A^{-1}\\Delta A \\right \\vert \\right \\vert \\leq \\epsilon \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert\\cdot \\left \\vert \\left \\vert A \\right \\vert \\right \\vert &lt; 1\\) (by assumptions), Lemma 5 gives us that \\(I + A^{-1}\\Delta A\\) is invertible. Since \\(A\\) is also invertible (again, by assumption), \\(A+\\Delta A\\) is invertible (product of two invertible matrices is invertible). Performing some linear algebra: \\[\\begin{align*} (A + \\Delta A) &amp;= b + \\Delta b \\Leftrightarrow \\\\ A(I + A^{-1} \\Delta A) y &amp;= b + \\Delta b \\Leftrightarrow \\\\ (I + A^{-1} \\Delta A) y &amp;= A^{-1}b + A^{-1}\\Delta b \\Leftrightarrow \\\\ y &amp;= (I + A^{-1} \\Delta A)^{-1} A^{-1}b + A^{-1}\\Delta b. \\end{align*}\\] Remember that \\(A^{-1}b = x\\). From the definition of \\(r\\) we have that \\(\\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert = \\frac{r}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert}\\). These two identities with the assumption that \\(\\left \\vert \\left \\vert \\Delta b \\right \\vert \\right \\vert \\leq \\epsilon b\\) gives us \\[ \\begin{aligned} \\left \\vert \\left \\vert y \\right \\vert \\right \\vert &amp;\\leq \\left \\vert \\left \\vert (I + A^{-1}\\Delta A)^{-1} \\right \\vert \\right \\vert \\left( \\left \\vert \\left \\vert x \\right \\vert \\right \\vert + \\left \\vert \\left \\vert A^{-1}\\Delta b \\right \\vert \\right \\vert\\right) \\\\ &amp;\\leq \\frac{1}{1-\\left \\vert \\left \\vert A^{-1}\\Delta A \\right \\vert \\right \\vert} \\left( \\left \\vert \\left \\vert x \\right \\vert \\right \\vert + \\frac{r}{\\epsilon \\left \\vert \\left \\vert A \\right \\vert \\right \\vert} \\cdot \\left \\vert \\left \\vert \\Delta b \\right \\vert \\right \\vert \\right) \\\\ &amp;\\leq \\frac{1}{1-r} \\left( \\left \\vert \\left \\vert x \\right \\vert \\right \\vert + \\frac{r}{\\epsilon \\left \\vert \\left \\vert A \\right \\vert \\right \\vert} \\cdot \\epsilon \\left \\vert \\left \\vert b \\right \\vert \\right \\vert \\right) \\\\ &amp;= \\frac{1}{1-r} \\left( \\left \\vert \\left \\vert x \\right \\vert \\right \\vert + \\frac{r \\cdot \\left \\vert \\left \\vert b \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert} \\right). \\end{aligned} \\] Finally, recall that \\(Ax=b\\), hence \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\cdot\\left \\vert \\left \\vert x \\right \\vert \\right \\vert \\geq \\left \\vert \\left \\vert b \\right \\vert \\right \\vert\\), so \\(\\left \\vert \\left \\vert x \\right \\vert \\right \\vert \\geq \\frac{\\left \\vert \\left \\vert b \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert}\\). So, \\[ \\begin{aligned} \\left \\vert \\left \\vert y \\right \\vert \\right \\vert \\leq \\frac{1}{1-r} \\left( \\left \\vert \\left \\vert x \\right \\vert \\right \\vert + r \\cdot \\left \\vert \\left \\vert x \\right \\vert \\right \\vert \\right) \\Leftrightarrow \\\\ \\frac{\\left \\vert \\left \\vert y \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert} \\leq \\frac{1+r}{1-r}. \\end{aligned} \\] Lemma 7 \\[\\frac{\\left \\vert \\left \\vert y - x \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert} \\leq \\frac{2\\epsilon \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert\\cdot \\left \\vert \\left \\vert A \\right \\vert \\right \\vert}{1-r}.\\] Proof. \\[ \\begin{aligned} \\left(A+\\Delta A\\right)y &amp;= b + \\Delta b \\Leftrightarrow \\\\ Ay - b &amp;= \\Delta b - \\Delta Ay \\Leftrightarrow \\\\ y - A^{-1}b &amp;= A^{-1} \\Delta b - A^{-1}\\Delta A y \\Leftrightarrow \\\\ y - x &amp;= A^{-1} \\Delta b - A^{-1}\\Delta A y \\Leftrightarrow \\\\ \\left \\vert \\left \\vert y - x \\right \\vert \\right \\vert &amp;\\leq \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\left \\vert \\left \\vert \\Delta b \\right \\vert \\right \\vert + \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\left \\vert \\left \\vert \\Delta A \\right \\vert \\right \\vert\\left \\vert \\left \\vert y \\right \\vert \\right \\vert \\\\ %\\Leftrightarrow \\\\ &amp;\\leq \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\epsilon \\left \\vert \\left \\vert b \\right \\vert \\right \\vert + \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\epsilon\\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\left \\vert \\left \\vert y \\right \\vert \\right \\vert \\\\ %\\Leftrightarrow \\\\ &amp;\\leq \\epsilon \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\left \\vert \\left \\vert x \\right \\vert \\right \\vert + \\epsilon\\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert\\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\left \\vert \\left \\vert y \\right \\vert \\right \\vert \\\\ % \\Leftrightarrow \\\\ &amp;\\leq \\epsilon \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\left(\\left \\vert \\left \\vert x \\right \\vert \\right \\vert + \\left \\vert \\left \\vert y \\right \\vert \\right \\vert\\right) \\\\ &amp;= \\epsilon \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\left \\vert \\left \\vert A \\right \\vert \\right \\vert\\left(\\left \\vert \\left \\vert x \\right \\vert \\right \\vert + \\frac{1+r}{1-r}\\left \\vert \\left \\vert x \\right \\vert \\right \\vert\\right) \\Leftrightarrow \\\\ \\frac{\\left \\vert \\left \\vert y-x \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert} &amp;\\leq \\epsilon \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\left \\vert \\left \\vert A \\right \\vert \\right \\vert \\left(\\frac{1-r}{1-r} + \\frac{1+r}{1-r}\\right) \\\\ &amp;= 2\\epsilon \\left \\vert \\left \\vert A^{-1} \\right \\vert \\right \\vert \\left \\vert \\left \\vert A \\right \\vert \\right \\vert \\frac{1}{1-r} \\end{aligned} \\] "],
["orthogonalization.html", "1.2 Orthogonalization", " 1.2 Orthogonalization Goals Introduce and prove the existence of QR decomposition Overview of the algorithm to perfor QR decomposition Solve least squares problems “Large” data problems Outline Motivating problems and solutions with QR Gram-Schmidt procedure, existence of QR Householder, Givens “Large” least squares problems datadown 1.2.1 Motivating problems Example 2 (Motivating Problem 1 (Consistent Linear System)) Assume \\(A \\in \\mathbb{R}^{n\\times m}, n \\geq m, \\text{rank}(A) = m\\), and \\(b \\in \\text{range}(A) \\subset R^m\\). Find \\(x \\in \\mathbb{R}^m\\) s.t. \\(Ax = b\\). Example 3 (Motivating Problem 2 (Least Squares Regression)) Assume \\(A \\in \\mathbb{R}^{n\\times m}, n \\geq m, \\text{rank}(A) = m\\), and \\(b \\in R^n\\). Find \\(x \\in \\mathbb{R}^m\\) s.t. \\[x \\in \\text{argmin}_{y \\in \\mathbb{R}^m} \\left \\vert \\left \\vert Ay-b \\right \\vert \\right \\vert_2.\\] Example 4 (Motivating Problem 3 (Underdetermined Linear System) Assume \\(A \\in \\mathbb{R}^{n\\times m}, n \\geq m, \\text{rank}(A) &lt; m\\), and \\(b \\in \\text{range}(A)\\). Find \\(x \\in \\mathbb{R}^m\\) s.t. \\[x \\in \\text{argmin}_{y \\in \\mathbb{R}^m} \\left\\{ \\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 \\left | Ay = b \\right\\} \\right . .\\] Example 5 (Motivating Problem 4 (Underdetermined Least Squares Regression)) Assume \\(A \\in \\mathbb{R}^{n\\times m}, n \\geq m, \\text{rank}(A) &lt; m\\), and \\(b \\in \\mathbb{R}^n\\). Find \\(x \\in \\mathbb{R}^m\\) s.t. \\[x \\in \\text{argmin}_{z \\in \\mathbb{R}^m} \\left\\{ \\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 \\left | \\left \\vert \\left \\vert Ay - b \\right \\vert \\right \\vert_2 = \\min_{y \\in \\mathbb{R}^m} \\left \\vert \\left \\vert Ay-b \\right \\vert \\right \\vert_2 \\right\\} \\right . .\\] Example 6 (Motivating Problem 5 (Constrained Least Squares Regression)) Assume \\(A \\in \\mathbb{R}^{n\\times m}, n \\geq m, \\text{rank}(A) = m\\), and \\(b \\in \\mathbb{R}^n\\). Let \\(C \\in \\mathbb{R}^{p\\times m}, \\text{rank}(C) = p\\), and \\(d \\in \\mathbb{R}^{p}\\). Find \\(x \\in \\mathbb{R}^m\\) s.t. \\[x = \\text{argmin}_{y \\in \\mathbb{R}^m} \\left \\vert \\left \\vert Ay - b \\right \\vert \\right \\vert_2\\quad \\text{s.t.} \\quad Cy = d.\\] Before we take a crack at solving these problems, we will need to get some definitions down. Definition 4 (Permutation Matrix) A permutation matrix is a square matrix such that each column has exactly one element that is \\(1\\), the rest are \\(0\\). Example 7 The following is a permutation matrix: \\[ \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix} \\] Definition 5 (Orthogonal Matrix) A matrix \\(Q\\) is said to be an orthogonal matrix if \\(Q^T Q = Q Q^T = I\\). Note: for an orthogonal matrix \\(Q \\in \\mathbb{R}^{n\\times m}\\), it holds that \\(\\left \\vert \\left \\vert Q_{i*} \\right \\vert \\right \\vert_2 = 1\\) for all \\(i=1,\\ldots,n\\), and \\(\\left \\vert \\left \\vert Q_{*j} \\right \\vert \\right \\vert_2 = 1\\) for all \\(j = 1,\\ldots, m\\).1 Definition 6 (Upper Triangular Matrix) A matrix \\(R\\) is an upper triangular matrix if \\(R_{ij} = 0\\) for all \\(i&gt;j\\). Lecture 4: 9/18 1.2.2 QR Decomposition In order to actually solve the problems listed above, we need the QR Decomposition: Theorem 2 (Existence of QR Decomposition) Let \\(A \\in \\mathbb{R}^{n \\times m}\\) and let \\(r = rank(A)\\). Then there exists: an \\(m\\times m\\) permutation matrix \\(\\Pi\\), an \\(n \\times n\\) orthogonal matrix \\(Q\\), an \\(r \\times r\\) upper triangular matrix \\(R\\), with non-zero diagonal elements (i.e. invertible) an \\(r \\times (m-r)\\) matrix S (if \\(m &gt; r\\)), such that \\[ A = Q \\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix} \\Pi^T. \\] With this in hand, we can solve the motivating problems stated above. Solution (Example 2). We want to find \\(x\\) such that \\(Ax = b\\). We use theorem 2 to rewrite this as \\(Q \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\Pi^T x = b\\). Note that since \\(\\text{rank}(A) = m\\), there is no \\(S\\) matrix. Now, since \\(Q\\) is an orthogonal matrix, we know that \\(Q^{-1} = Q^T\\), so \\[\\begin{equation} \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\Pi^T x = Q^T b = c = \\begin{bmatrix} c_1 \\\\ 0 \\end{bmatrix}. \\tag{1} \\end{equation}\\] So now the equation we are trying to solve becomes \\[ R \\Pi^T x = c_1. \\] Since \\(R\\) is an upper triangular matrix with non-zero diagonal elements, it is invertible. Since \\(\\Pi\\) is a permutation matrix, \\(\\Pi^{-1} = \\Pi^T\\). Using this we can find the solution: \\[ x = \\Pi R^{-1} c_1. \\] Solution (Example 3). We want to find \\(x\\) such that \\(x \\in \\text{argmin}_{y \\in \\mathbb{R}^m}\\left \\vert \\left \\vert Ay-b \\right \\vert \\right \\vert_2\\). Once again, \\(\\text{rank}(A) = m\\), so using theorem 2, we can rewrite the expression we are trying to minimize as \\[ \\min \\left \\vert \\left \\vert Q \\begin{bmatrix} R \\\\ 0 \\end{bmatrix}\\Pi^T y - b \\right \\vert \\right \\vert_2. \\tag{2} \\] Since \\(Q^T = Q^{-1}\\) is orthogonal, \\(\\left \\vert \\left \\vert Q^T x \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert x \\right \\vert \\right \\vert_2\\) for all \\(x\\) (homework exercise 30). So, we get that (2) is the same as \\[ \\min\\left \\vert \\left \\vert \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\Pi^T y - Q^T b \\right \\vert \\right \\vert_2. \\] Now let \\(c = Q^T b\\). Then, \\(c\\) is of the form \\(\\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}\\), where \\(c_2\\) is the last \\(n-r\\) rows (i.e. corresponding to the \\(0\\) rows of \\(\\begin{bmatrix} R \\\\ 0 \\end{bmatrix}\\)). Then \\[ \\min\\left \\vert \\left \\vert \\begin{bmatrix} R \\Pi^T y - c_1 \\\\ -c_2 \\end{bmatrix} \\right \\vert \\right \\vert_2 = \\min \\sqrt{\\left \\vert \\left \\vert R \\Pi^T y - c_1 \\right \\vert \\right \\vert_2^2 + \\left \\vert \\left \\vert c_2 \\right \\vert \\right \\vert_2^2}. \\] Now this is minimized by \\(\\text{argmin}_y \\left \\vert \\left \\vert R \\Pi^T y - c_1 \\right \\vert \\right \\vert_2^2\\). As before, \\(R^{-1}\\) exists since \\(R\\) is upper triangular with non-zero diagonal elements, \\(\\Pi^T = \\Pi^{-1}\\) since \\(\\Pi\\) is a permutation matrix, so \\[ \\begin{aligned} x &amp;= \\text{argmin}_y \\left \\vert \\left \\vert R \\Pi^T y - c_1 \\right \\vert \\right \\vert_2^2 \\Leftrightarrow \\\\ R \\Pi^T x &amp;= c_1 \\Leftrightarrow \\\\ x &amp;= \\Pi R^{-1} c_1. \\end{aligned} \\] Solution (Example 4). In this scenario, \\(\\text{rank}(A) = r &lt; m\\). We are looking for \\(x \\in \\text{argmin}_{y}\\left\\{\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 \\left | Ay = b\\right\\}\\right .\\). Using theorem 2, we can rewrite this as \\(\\text{argmin}_y\\left\\{\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 | Q\\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix} y = b \\right\\}\\), and multiplying by \\(Q^T\\), \\(\\text{argmin}_y\\left\\{\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 \\left | \\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix} y = Q^T b \\right\\} \\right .\\). We introduce the vector \\(c\\) such that \\(Q^T b = \\begin{bmatrix} c &amp; 0 \\end{bmatrix}^T\\) (\\(0\\) entries correspond to \\(0\\) rows in \\(\\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix}\\)). If we furthermore write \\(\\Pi^T y\\) as \\(\\begin{bmatrix} z_1 &amp; z_2 \\end{bmatrix}^T\\). Then, since \\(\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2\\), our problem becomes \\[ \\begin{aligned} x &amp;\\in \\text{argmin}_z \\left\\{\\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 \\left | R z_1 + S z_2 = c \\right\\}\\right. \\\\ x &amp;\\in \\text{argmin}_z \\left\\{\\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 \\left | z_1 = R^{-1} c - R^{-1} S z_2 \\right\\}\\right. \\\\ x &amp;\\in \\text{argmin}_z \\sqrt{\\left \\vert \\left \\vert R^{-1}c - R^{-1} S z_2 \\right \\vert \\right \\vert_2^2 + \\left \\vert \\left \\vert z_2 \\right \\vert \\right \\vert_2^2} \\\\ x &amp;\\in \\text{argmin}_z \\left\\{\\left \\vert \\left \\vert R^{-1}c - R^{-1} S z_2 \\right \\vert \\right \\vert_2^2 + \\left \\vert \\left \\vert z_2 \\right \\vert \\right \\vert_2^2\\right\\}, \\end{aligned} \\] where the last equality is a consequence of the result proved in homework 31. Now, let \\(d = R^{-1}c\\) and \\(P = R^{-1}S\\). Then we can find the minimum of the above expression by differentiating and setting equal to zero: \\[\\begin{align} 0 &amp;= -P^Td + (P^TP + I)z_2 \\rightarrow \\\\ z_2 &amp;= (P^T P + I)^{-1}P^Td. \\end{align}\\] Solution (Example 5). We want to find \\(\\min_z \\left\\{ \\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 \\left \\vert z \\in \\text{argmin}_y \\left \\vert \\left \\vert Ay - b \\right \\vert \\right \\vert_2\\right\\} \\right .\\) Use theorem 2: \\[\\begin{aligned} \\min_z \\left\\{ \\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 \\left \\vert z \\in \\text{argmin}_y \\left \\vert \\left \\vert Ay - b \\right \\vert \\right \\vert_2\\right\\} \\right . &amp;= \\left\\{ \\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 \\left \\vert z \\in \\text{argmin}_y \\left \\vert \\left \\vert \\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix} \\Pi^T y - Q^T b \\right \\vert \\right \\vert_2\\right\\} \\right . \\\\ &amp;= \\left\\{ \\left \\vert \\left \\vert w \\right \\vert \\right \\vert_2 \\left \\vert w \\in \\text{argmin}_y \\left \\vert \\left \\vert \\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix} y - Q^T b \\right \\vert \\right \\vert_2\\right\\} \\right ., \\end{aligned}\\] since \\(\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert \\Pi^T y \\right \\vert \\right \\vert_2\\). This is exactly the problem solved in example 4. In conclusion, \\[ w = \\begin{bmatrix} R^{-1}(c_1 - Sy_y) \\\\ y_2 \\end{bmatrix}. \\] Solution (Example 6). 1.2.3 Existence of QR-decomposition. To prove the existence of the QR-decomposition, we need the Gram-Schmidt process. Lemma 8 (The Gram-Schmidt Process) Let \\(r \\in \\mathbb{N}\\). Given a set of linearly independent vectors \\(\\{a_1, \\ldots, a_r\\}\\), there exists a set of orthonormal vectors \\(\\{q_1, \\ldots, q_r\\}\\) such that \\(\\text{span} \\{q_1, \\ldots, q_r\\} = \\text{span}\\{a_1, \\ldots, a_r\\}\\). The \\(q_i\\)’s are given by… Proof. We will prove this by induction. For \\(i=1\\): let \\(R_{11} = \\left \\vert \\left \\vert a_1 \\right \\vert \\right \\vert_2\\), \\(q_1 = \\frac{1}{R_11}a_1\\). Notice that \\(\\left \\vert \\left \\vert q_1 \\right \\vert \\right \\vert = 1\\). (At this point, it might be beneficial to check out the intuitive side note (??)) Define \\(q^r\\) in the following way: let \\(R_{ir} = q_i^\\prime a_r\\), \\(\\tilde{q}_r = a_r - \\sum_{i=1}^{r-1} R_{ir}q_i\\), and \\(R_{rr} = \\left \\vert \\left \\vert \\tilde{q}_r \\right \\vert \\right \\vert_2\\). Then \\(q_r = \\frac{\\tilde{q}_r}{R_{rr}}\\). (Note: \\(\\tilde{q}_r \\neq 0\\) since the \\(a_i\\)s are linearly independent, and \\(q_i\\) is given as a linear combination of \\(a_1, \\ldots, a_i\\).) Assume the result holds for \\(i \\le r-1\\). I.e. we have vectors \\(q_1, \\ldots, q_{r-1}\\) given as above, and that \\(\\text{span}\\{q_1, \\ldots, q_{r-1}\\} = \\text{span}\\{a_1, \\ldots, a_{r-1}\\}\\), \\(q_i \\cdot q_j\\) for all \\(i,j = 1, \\ldots, r-1\\) with \\(i \\neq j\\), \\(q_i^\\prime \\cdot q_i = 1\\) for all \\(i = 1, \\ldots, r-1\\). Now, we want to show that we can construct a \\(q_r\\) such that \\(\\text{span}\\{q_1, \\ldots, q_r\\} = \\text{span}\\{a_1, \\ldots, a_r\\}\\), \\(q_r \\cdot q_j = 0\\) for all \\(j = 1, \\ldots, r-1\\), \\(q_r^\\prime \\cdot q_r = 1\\). We start from below. By definition of \\(q_r\\): \\(q_r^\\prime q_r = \\frac{\\tilde{q}_r^\\prime \\tilde{q}_r}{R_{rr}^2} = \\frac{\\left \\vert \\left \\vert \\tilde{q}_r \\right \\vert \\right \\vert^2}{R_{rr}^2} = 1\\). Let \\(i &lt; r\\). Then \\[\\begin{aligned} q_i^{\\prime} \\tilde{q}_r &amp;= q_i^\\prime a_r - \\sum_{j=1}^{r-1} R_{jr} q_i^\\prime q_j \\\\ &amp;= q_i^\\prime a_r - R_{ir} q_i^\\prime q_i \\\\ &amp;= q_i^\\prime a_r - R_{ir} = 0 \\text{ (by definition of } R_{ir}\\text{)}. \\end{aligned}\\] We need to show that \\(a_r\\) can be written as a linear combination of \\(q_i\\)s. \\[\\begin{aligned} \\sum_{i=1}^r R_{ir} q_i &amp;= \\sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} q_r \\\\ &amp;= \\sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} \\frac{1}{R_{rr}} \\tilde{q}_r \\\\ &amp;= \\sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} \\frac{1}{R_{rr}} \\left(a_r - \\sum_{i=1}^{r-1} R_{ir}q_i \\right) \\\\ &amp;= \\sum_{i=1}^{r-1} R_{ir} q_i + a_r - \\sum_{i=1}^{r-1} R_{ir}q_i \\\\ &amp;= a_r. \\end{aligned}\\] It is fairly easy to find $q_2$. We want to find it such that $a_2 = R_{12}q_1 + R_{22} q_2$, and $\\norm{q_2}_2 = 1$ and $q_1 \\perp q_2$, i.e. $q_1 \\cdot q_2 = 0$. So, if we multiply the equation by $q_1$, we get that $q_1 a_2 = R_{12}$. Substituting this into the first equation, $q_2 = \\frac{a_2 - R_{12} q_1}{R_{22}}$. Note that this is a circular argument, and hence not a formal way of doing this. Lecture 5: 9/20 (Finished up proof of The Gram-Schmidt Process (8)) If we write up $a_1, \\ldots, a_r$ in a matrix, we see that $$ \\begin{bmatrix} a_1 &amp; \\dots &amp; a_r \\end{bmatrix} = \\begin{bmatrix} q_1 &amp; \\dots &amp; q_r \\end{bmatrix} \\begin{bmatrix} R_{11} &amp; R_{12} &amp; \\dots &amp; R_{1r} \\\\ 0 &amp; R_{22} &amp; \\dots &amp; R_{2r} \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\ldots &amp; 0 &amp; R_{rr} \\end{bmatrix} $$ This is quite similar to the result we are after (the QR-decomposition \\@ref(thm:qr-decomposition)). Proof (Proof of theorem 2). Since \\(\\text{rank}(A) = r\\), \\(A\\) has \\(r\\) linearly independent columns. Hence, there exists a permutation matrix \\(\\Pi\\) such that \\[ A \\Pi = \\begin{bmatrix} a_1 &amp; \\dots &amp; a_r &amp; a_{r+1} \\dots a_m \\end{bmatrix}, \\] where \\(a_1, \\ldots, a_r\\) are linearly independent, and \\(a_{r+1}, \\ldots, a_m\\) are linearly dependent on the first \\(r\\) columns. Using Gram-Schmidt (lemma 8), we know that there exists \\(\\tilde{Q} \\in \\mathbb{R}^{n \\times r}, R \\in \\mathbb{R}^{r \\times r}\\) such that \\(A\\Pi = \\tilde{Q} R\\). Since \\(\\text{span}\\{\\tilde{q}_1, \\ldots, \\tilde{q}_r\\}\\) (columns of \\(\\tilde{Q}\\)) is equal to \\(\\text{span}\\{a_1, \\ldots, a_r\\}\\), there exists an \\(s_{k(j-r+2)}\\) for any \\(j \\in \\{r+1, \\ldots, m\\}\\) and \\(k \\in \\{1, \\ldots, r\\}\\) such that \\(a_j = \\sum_{k=1}^r s_{k(j-r+2)}q_k\\). So, \\[ A\\Pi = \\tilde{Q} \\begin{bmatrix} R &amp; S \\end{bmatrix}. \\] This is almost the form we want, BUT \\(\\tilde{Q}\\) is not orthonormal (it is not square). However, we know that we can pick \\(n-r\\) vectors from \\(\\mathbb{R}^n\\) such that adding these as columns to \\(\\tilde{Q}\\) we get a set of \\(n\\) linearly independent columns. Now, use Gram-Schmidt to normalize. Since the first \\(r\\) columns are already normalized, these will stay the same. The result is a matrix \\(Q\\), where the columns are all length \\(1\\), and they are all linearly independent. I.e. \\(Q^TQ = I\\). So, \\(A\\Pi = Q \\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix}\\), hence \\[ A = Q \\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix} \\Pi^T. \\] Basically, this gives us a way to perform QR decomposition. However, using the Gram-Schmidt procedure is NOT numerical stable. I.e. we might end up with matrices \\(Q, R\\), and \\(S\\) from which we CANNOT recover \\(A\\). To overcome this, there is a different method called the Modified Gram-Schmidt Procedure. Lemma 9 (The Modified Gram-Schmidt Procedure) HOMEWORK Definition 7 (Householder Reflections) A matrix \\(H = I - 2vv^\\prime\\), where \\(\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1\\), is called a Householder Reflection. A Householder reflection takes any vector and reflects it over \\(\\{tv: t \\in \\mathbb{R}\\}\\). Lemma 10 Householder reflections are orthogonal matrices. Proof. By definition of a Householder matrix (7), \\(H = I - 2vv^\\prime\\) for a \\(v\\) with \\(\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1\\). So, \\[\\begin{aligned} H^\\prime H &amp;= (I - 2vv^\\prime)^\\prime (I - 2vv^\\prime) \\\\ &amp;= (I^\\prime - 2(vv^\\prime)^\\prime)(I - 2 vv^\\prime)\\\\ &amp;= (I - 2vv^\\prime)(I - 2 vv^\\prime) \\\\ &amp;= I - 2vv^\\prime - 2vv^\\prime + 4 vv^\\prime v v^\\prime \\quad\\left(\\text{ recall: } v^\\prime v = \\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1\\right)\\\\ &amp;= I - 2vv^\\prime - 2vv^\\prime + 4 vv^\\prime \\\\ &amp;= I. \\end{aligned}\\] So by definition (5), \\(H\\) is an orthogonal matrix. Lemma 11 There exists Householder refelctions \\(H_1, \\cdots, H_r\\) such that \\(H_r \\cdots H_1 \\cdot A\\cdot\\Pi = R\\). Proof. Let \\(A\\Pi = \\begin{bmatrix} a_1 &amp; \\cdots &amp; a_r \\end{bmatrix}\\). Choose \\(H_1\\) s.t. \\(H_1 a_1 = R_{11} e_1 = a_1 - 2v_1v_1^\\prime a_1\\) (last equality due to definition of Householder reflections). This is equivalent to \\(v_1(2v_1^\\prime a_1) = a_1 - R_{11}e_1\\). Now, let \\(v_1 = \\frac{a_1 - R_{11}e_1}{\\left \\vert \\left \\vert a_1 - R_{11}e_2 \\right \\vert \\right \\vert_2}\\). Plug this into the equation for \\(R_{11}e_1\\) above to get \\[ R_{11}e_1 = a_1 - \\frac{(a_1 - R_{11}e_1)}{\\left \\vert \\left \\vert a_1 - R_{11}e_1 \\right \\vert \\right \\vert_2}\\frac{a_1^\\prime a_1 - R_{11} a_1^\\prime e_1}{\\left \\vert \\left \\vert a_1 - R_{11}e_1 \\right \\vert \\right \\vert_2}. \\] If we multiply this by \\(e_1^\\prime\\) from the right, we get \\[ R_{11} = \\pm \\left \\vert \\left \\vert a_1 \\right \\vert \\right \\vert_2, v_1 = \\frac{a_1 - \\left \\vert \\left \\vert a_1 \\right \\vert \\right \\vert e_1}{\\left \\vert \\left \\vert a_1 - \\left \\vert \\left \\vert a_1 \\right \\vert \\right \\vert_2 e_1 \\right \\vert \\right \\vert_2}. \\] \\[ H_1 = I - \\frac{a_1 - \\left \\vert \\left \\vert a_1 \\right \\vert \\right \\vert_2e_1)((a_1 - \\left \\vert \\left \\vert a_1 \\right \\vert \\right \\vert_2 e_1)}{\\left \\vert \\left \\vert a_1 - \\left \\vert \\left \\vert a_1 \\right \\vert \\right \\vert_2 e_1 \\right \\vert \\right \\vert_2^2} \\] Definition 8 (Givens Rotations) A Givens Rotation is a matrix \\(G^{(i,j)}\\) with entries \\((g_{ij})\\) such that \\(g_{ii} = g_{jj} = \\lambda\\) (the \\(i\\)th and \\(j\\)th elements of the diagonal are \\(\\lambda\\)). \\(g_{kk} = 1\\) for all \\(k \\notin \\{i,j\\}\\). (all other diagonal elements are \\(1\\)) \\(g_{ij} = -g_{ji} = \\sigma\\) \\(g_{ij} = 0\\) for all other pairs of \\(i,j\\). In words: \\(G^{(i,j)}\\) is the identity matrix with the \\(i\\)th and \\(j\\)th diagonal elements made \\(\\lambda\\), and the entries at \\((i,j)\\) and \\((j,i)\\) are \\(\\sigma\\). Example 8 (2x2 Givens rotation to create upper triangular matrix.) The general \\(G^{(1,2)}\\) is \\(\\begin{bmatrix} \\lambda &amp; \\sigma \\\\ -\\sigma &amp; \\lambda \\end{bmatrix}\\). Let us consider a general \\(M \\in \\mathbb{R}^{2\\times m}\\): \\[ M = \\begin{bmatrix} M_{11} &amp; M_{12} &amp; \\dots &amp; M_{1m} \\\\ M_{21} &amp; M_{22} &amp; \\dots &amp; M_{2m} \\end{bmatrix}. \\] We want this Givens matrix to be orthogonal. By homework exercise 45, we know this is the case when \\(\\lambda^2 + \\delta^2 = 1\\). We also want \\(G^{(1,2)}M\\) to be an upper triangular matrix. Since \\[ G^{(i,j)} M = \\begin{bmatrix} \\lambda M_{11} + \\sigma M_{12} &amp; \\dots &amp; \\lambda M_{1m} + \\sigma M_{2m} \\\\ -\\sigma M_{11} + \\lambda M_{12} &amp; \\dots &amp; -\\sigma M_{1m} + \\lambda M_{2m} \\\\ \\end{bmatrix}, \\] we need \\(\\lambda M_{12} = \\sigma M_{11}\\). So, solving these two equations, we find that \\(\\lambda = \\frac{M_{11}}{\\sqrt{M_{11}^2 + M_{21}^2}}\\) and \\(\\sigma = \\frac{M_{21}}{\\sqrt{M_{11}^2 + M_{21}^2}}\\). Hence, given the matrix \\(M\\), we can find a Givens rotation which when multiplied by \\(M\\) returns an upper triangular matrix. This also means that given a vector \\(a \\in \\mathbb{R}^n\\), we can find a Givens rotation such that \\(G^{(i,j)} a\\) gives back \\(a\\) except for one entry, which has been changed to \\(0\\). 1.2.4 “Large” Data Problem Finally, we will take a look at how to solve a “large” data problem using QR decomposition. To do so, let \\(A \\in \\mathbb{R}^{n\\times m}\\) be a matrix with \\(n\\) “big”. By “big”, we mean so large that \\(A\\) won’t fit in memory, but the first \\(m+1\\) rows of \\(A\\) will. Gentleman published a few papers in 1973/1974 describing a method for incremental QR decomposition. The key ideas are as follows: Remember that the solution to the linear model is \\((A&#39;A)^{-1}A&#39;b = R^{-1}(Q&#39;b)\\). The QR decomposition of \\(\\begin{bmatrix}A &amp; b\\end{bmatrix}\\) gives \\(\\begin{bmatrix} R &amp; Q&#39;b \\\\ 0 &amp; S \\end{bmatrix}\\) (ex. 35 and 36) Now, if we do QR decomposition on the first \\(m+1\\) rows of \\(A\\) we get \\(\\begin{bmatrix} \\tilde{R}_{m+1} &amp; \\tilde{Q}_{m+1}&#39;b_{m+1} \\\\ 0 &amp; S_{m+1} \\end{bmatrix}\\). Now, add the next row of \\(A\\) to get \\(\\begin{bmatrix} \\tilde{R}_{m+1} &amp; \\tilde{Q}_{m+1}&#39;b_{m+1} \\\\ 0 &amp; S_{m+1} \\\\ a_{m+2} &amp; b_{m+2} \\end{bmatrix}\\). Hit this with the right Givens rotation to change entry \\((m+1, 1)\\) to \\(0\\). This will give us something of the form \\(\\begin{bmatrix} \\tilde{R}_{m+2} &amp; \\tilde{Q}_{m+2}&#39;b_{m+2} \\\\ 0 &amp; S_{m+2} \\end{bmatrix}\\). Here, \\(\\tilde{R}_{m+2}\\) is still an upper triangular matrix with less than \\(m\\) rows. Repeat the procedure until we’ve added all rows of \\(A\\)/elements of \\(b\\). Here we use the notion \\(Q_{i*}\\) to mean the \\(i\\)’th row, and \\(Q_{*j}\\) to mean the \\(j\\)’th column of \\(Q\\).↩ "],
["singular-value-decomposition-svd.html", "1.3 Singular Value Decomposition (SVD)", " 1.3 Singular Value Decomposition (SVD) Outline: Motivating problems SVD and solutions Existence and properties Random projections (a modern application of SVD) 1.3.1 Motivating Problems Example 9 Let \\(A \\in \\mathbb{R}^{n \\times m}\\) with \\(\\text{rank}(A) &lt; m \\le n\\), and let \\(B \\in \\text{range}(A)\\). Find \\(x\\) s.t. \\[ x \\in \\text{argmin}_{y \\in \\mathbb{R}^m} \\left\\{\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 : Ay = b \\right \\} \\] Example 10 Let \\(A \\in \\mathbb{R}^{n \\times m}\\). Find \\[ \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 = \\sup_{v \\in \\mathbb{R}^m\\setminus\\{0\\}} \\frac{\\left \\vert \\left \\vert Av \\right \\vert \\right \\vert_2}{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2}. \\] Example 11 Let \\(A \\in \\mathbb{R}^{n\\times m}\\). Find \\(x\\) s.t. \\[ x = \\text{argmin}_{rank(Y) \\le k}\\left \\vert \\left \\vert A-Y \\right \\vert \\right \\vert_F. \\] Note: \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F\\) is the Frobenius norm and is defined as \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 \\right)^{1/2}\\). So it is sort of a Euclideaon norm extended to matrices. 1.3.2 SVD 1.3.2.1 SVD definition Theorem 3 Suppose \\(A \\in \\mathbb{R}^{n\\times m}\\) and \\(n\\ge m\\). Then there exists \\(U \\in \\mathbb{R}^{n \\times n}\\) and \\(V \\in \\mathbb{R}^{m\\times m}\\) that are both orthogonal, and a diagonal matrix \\(\\Sigma \\in \\mathbb{R}^{n \\times m}\\) with diagonal elements \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_m \\ge 0\\) such that \\[ A = U \\Sigma V&#39;\\] Note: The diagonal elements of \\(\\Sigma\\) are called the singular values of \\(A\\). The columns of \\(U\\) are called the left singular vectors The columns of \\(V\\) are called the right singular vectors Corollary 1 \\(\\text{rank}(A)\\) is the number of non-zero singular values of \\(A\\). 1.3.2.2 Solutions to motivating problems Solution (Solution to example 9). Since \\(b \\in \\text{range}(A)\\), there exists a \\(y\\) such that \\(Ay = b\\). Now, by theorem 3 there exist \\(U\\),\\(V\\), and \\(\\Sigma\\) such that \\(A=U\\Sigma V^\\prime\\). Since \\(U\\) is orthogonal, \\(U^{-1} = U^\\prime\\). So, \\[\\begin{aligned} U\\Sigma V^\\prime y &amp;= b \\Leftrightarrow \\\\ \\Sigma V^\\prime y &amp;= U^\\prime b. \\end{aligned}\\] Let \\(z = V^\\prime y\\) and \\(c = U^\\prime b\\). By corollary 1 we now that there are exactly \\(\\text{rank}(A) = r\\) non-zero singular values. So \\[\\begin{aligned} \\begin{bmatrix} \\sigma_1 &amp; &amp; &amp; &amp; &amp; \\\\ &amp; \\ddots &amp; &amp; &amp; &amp; \\\\ &amp; &amp; \\sigma_r &amp; &amp; &amp; \\\\ &amp; &amp; &amp; 0 &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} = \\begin{bmatrix} c_1 \\\\ 0 \\end{bmatrix}. \\end{aligned}\\] So, \\(z_1 = \\begin{bmatrix} \\sigma_1^{-1} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r^{-1} \\end{bmatrix} c_1\\). We want to minimize \\(\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert V^\\prime z \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 = \\sqrt{\\left \\vert \\left \\vert z_1 \\right \\vert \\right \\vert_2^2 + \\left \\vert \\left \\vert z_2 \\right \\vert \\right \\vert_2^2}\\). This is done by setting \\(z_2=0\\), which we can do since \\(z_2\\) does not affect the equation above. (It is multiplied by all the \\(0\\) rows of \\(\\Sigma\\).) So, the minimum norm solution is \\[\\begin{aligned} x &amp;= V \\begin{bmatrix} \\sigma_1^{-1} &amp; &amp; &amp; \\\\ &amp; \\ddots &amp; &amp; \\huge{c_1} \\\\ &amp; &amp; \\sigma_{r}^{-1} &amp; \\\\ &amp; {\\huge 0} &amp; &amp; \\end{bmatrix} \\\\ &amp;= V \\cdot V^\\prime y. \\end{aligned}\\] For the solution to example 10, we’ll need the following result: Lemma 12 Let \\(D\\) be a non-zero diagonal matrix of size \\(n \\times m\\), \\(n\\ge m\\). Then, \\[\\left \\vert \\left \\vert D \\right \\vert \\right \\vert_2 = \\max_i |D_{ii}|\\]. Proof. Note that \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 = \\max_{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1} \\left \\vert \\left \\vert Av \\right \\vert \\right \\vert_2\\). Also, if we let \\(D_{ii} = \\max_{j} |D_{jj}|\\) and \\(v\\) be a vector with norm \\(1\\), \\[ \\left \\vert \\left \\vert Dv \\right \\vert \\right \\vert_2^2 = \\sum_{j=1}^m (D_{jj}^2 v_j)^2 \\leq D_{ii}^2 \\sum_{j=1}^m v_j^2 = D_{ii}^2 \\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2^2 = D_{ii}^2. \\] Now, let \\(z \\in \\mathbb{R}^m\\) such that \\(z_{ij} = \\left\\{ \\begin{matrix} 0, &amp; j \\neq i \\\\ \\frac{D_{ii}}{|D_{ii}|}, &amp; j = i \\end{matrix} \\right .\\). Then \\(\\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 = \\sqrt{\\frac{D_{ii}^2}{|D_{ii}|^2}} = 1\\), and \\[ \\left \\vert \\left \\vert Dz \\right \\vert \\right \\vert_2^2 = \\sum_{j=1}^m (D_{jj} z_j)^2 = D_{ii}^2 \\frac{D_{ii}^2}{|D_{ii}|^2} = D_{ii}^2. \\] Since \\(\\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 = 1\\), \\[ D_{ii}^2 \\left \\vert \\left \\vert Dz \\right \\vert \\right \\vert_2^2 \\le (\\max_{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1} |D_{ii}|)^2 \\le D_{ii}^2\\], so \\(\\left \\vert \\left \\vert D \\right \\vert \\right \\vert_2 = \\max_{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1} |D_{ii}| = |D_{ii}| = max_{j} |D_{jj}|\\). Solution (Solution to example 10). First, use the SVD of \\(A\\) to write \\(A = U \\Sigma V^\\prime\\). Now, since the 2-norm is invariant under multiplication of orthogonal matrices, we have that \\[\\begin{aligned} \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 &amp;= \\max_{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert = 2} \\left \\vert \\left \\vert U\\Sigma V^\\prime v \\right \\vert \\right \\vert_2 \\\\ &amp;= \\max_{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert = 2} \\left \\vert \\left \\vert \\Sigma V^\\prime v \\right \\vert \\right \\vert_2. \\end{aligned}\\] If we let \\(z = V^\\prime v\\), we see that \\(\\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1\\), since \\(V^\\prime\\) is an orthogonal matrix. Hence, \\[ \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 = \\max_{\\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2 = 1} \\left \\vert \\left \\vert \\Sigma z \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert \\Sigma \\right \\vert \\right \\vert_2 = \\max_{i} |\\sigma_i| = \\sigma_1, \\] where we used the lemma above to obtain the second to last equality. Solution (Solution to example 11). Recall, \\(A = \\sum_{i=1}^n \\sigma_i u_i v_i^\\prime = U \\Sigma V^\\prime\\). We want to find \\(\\text{argmin}_{\\text{rank}(Y) \\le k} \\left \\vert \\left \\vert A-Y \\right \\vert \\right \\vert_F\\). Case 1: If \\(\\text{rank}(A) \\le k\\), then \\(Y = A\\) is the solution. Case 2: \\(\\text{rank}(A) &gt; k\\). Since the Frobenius norm is orthogonally invariant, we can obtain that \\[\\begin{aligned} \\left \\vert \\left \\vert A-Y \\right \\vert \\right \\vert_F^2 &amp;= \\left \\vert \\left \\vert U\\Sigma V^\\prime - Y \\right \\vert \\right \\vert_F^2 \\\\ &amp;= \\left \\vert \\left \\vert \\Sigma - U^\\prime Y V \\right \\vert \\right \\vert_F^2. \\end{aligned}\\] If we let \\(X = U^\\prime Y V\\), we get that \\[\\begin{aligned} \\left \\vert \\left \\vert A-Y \\right \\vert \\right \\vert_F^2 &amp;= \\sum_{i=1}^n \\sum_{j=1}^m (\\sigma_{ij} - x_{ij})^2 \\\\ &amp;= \\sum_{i=1}^n \\sum_{j=1,i\\ne j}^m ( - x_{ij})^2 + \\sum_{i=1}^n (\\sigma_i - x_{ij})^2, \\end{aligned}\\] since \\(\\sigma_{ij}=0\\) for all \\(i \\ne j\\), and at most \\(k\\) of the \\(\\sigma_{ii}\\) are non-zero. To minimize the expression above, we choose \\(Y\\) such that \\(x_{ij}=0\\) for \\(i\\ne j\\), and \\(x_{ii} = \\sigma_i\\). Hence, \\[ X = \\begin{bmatrix} \\sigma_1 &amp; &amp; &amp; &amp; &amp; \\\\ &amp; \\ddots &amp; &amp; &amp; &amp; \\\\ &amp; &amp; \\sigma_k &amp; &amp; &amp; \\\\ &amp; &amp; &amp; 0 &amp; &amp; \\\\ &amp; &amp; &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; &amp; &amp; 0 \\end{bmatrix}, \\] and \\(Y = U X V^\\prime\\). 1.3.3 Existence and Properties Proof (SVD (3)). Recall that \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 = \\sup_{v \\ne 0} \\frac{\\left \\vert \\left \\vert Av \\right \\vert \\right \\vert_2}{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2} = \\max_{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1} \\left \\vert \\left \\vert Av \\right \\vert \\right \\vert_2\\). I.e. there exists a \\(v_1\\) such that \\(\\left \\vert \\left \\vert v_1 \\right \\vert \\right \\vert_2 = 1\\) and \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert Av_1 \\right \\vert \\right \\vert_2\\). Let \\(u_1\\) be a vector such that \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 u_1 = Av_1\\). This implies that \\(\\left \\vert \\left \\vert u_1 \\right \\vert \\right \\vert_2 = 1\\), since \\(\\left \\vert \\left \\vert \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2u_1 \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 \\left \\vert \\left \\vert u_1 \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert Av_1 \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2\\). Let \\(\\sigma_1 = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2\\). So, \\(\\sigma_1 u_1 = A v_1\\). Using the Gram-Schmidt procedure, we can create a matrix \\(V_1 \\in \\mathbb{R}^{n\\times (n-1)}\\) and \\(\\tilde{U}_1 = \\begin{bmatrix} u_1 V_1 \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}\\) being an orthogonal matrix. Similarly, we can create \\(\\tilde{V}_1 \\in \\mathbb{R}^{m\\times m}\\) such that \\(\\tilde{V}_1 = \\begin{bmatrix} v_1 V_1 \\end{bmatrix} \\in \\mathbb{R}^{m \\times m}\\) is orthogonal. Now, \\(A \\tilde{V}_1 = \\begin{bmatrix} Av_1 &amp; AV_1 \\end{bmatrix} = \\begin{bmatrix} \\sigma_1 u_1 &amp; AV_1 \\end{bmatrix}\\). Hence, \\(\\tilde{U}_1^\\prime A \\tilde{V}_1 = \\begin{bmatrix} u_1 \\\\ U_1^\\prime \\end{bmatrix} \\begin{bmatrix} \\sigma_1 u_1 &amp; A V_1 \\end{bmatrix} = \\begin{bmatrix} \\sigma_1 &amp; w^\\prime \\\\ 0 &amp; \\tilde{A} \\end{bmatrix}\\), since \\(u_1\\) is orthogonal to all columns of \\(U_1^\\prime\\). We need to show that \\(w=0\\). Since the 2-norm is orthogonally invariant, it holds for any \\(z \\ne 0\\), \\[\\begin{aligned} \\sigma_1^2 &amp;= \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2^2 \\\\ &amp;= \\left \\vert \\left \\vert \\tilde{U}_1^\\prime A \\tilde{V}_1 \\right \\vert \\right \\vert_2^2 \\\\ &amp;= \\left \\vert \\left \\vert \\begin{bmatrix} \\sigma_1 &amp; w^\\prime \\\\ 0 &amp; \\tilde{A} \\end{bmatrix} \\right \\vert \\right \\vert_2^2 \\\\ &amp;\\ge \\frac{\\left \\vert \\left \\vert \\begin{bmatrix} \\sigma_1 &amp; w^\\prime \\\\ 0 &amp; \\tilde{A} \\end{bmatrix} z \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert z \\right \\vert \\right \\vert_2^2}. \\end{aligned}\\] Let \\(z = \\begin{bmatrix} \\sigma_1 \\\\ w \\end{bmatrix}\\). Then, \\[ \\sigma_1^2 \\ge \\frac{\\left \\vert \\left \\vert \\begin{bmatrix} \\sigma_1 &amp; w^\\prime \\\\ 0 &amp; \\tilde{A} \\end{bmatrix} \\begin{bmatrix} \\sigma_1 \\\\ w \\end{bmatrix} \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert \\begin{bmatrix} \\sigma_1 \\\\ w \\end{bmatrix} \\right \\vert \\right \\vert_2^2} = \\frac{(\\sigma_1^2 + \\left \\vert \\left \\vert w \\right \\vert \\right \\vert_2^2)^2 + \\left \\vert \\left \\vert \\tilde{A}w \\right \\vert \\right \\vert_2^2}{\\sigma_1^2 + \\left \\vert \\left \\vert w \\right \\vert \\right \\vert_2^2}\\ge \\sigma_1^2 + \\left \\vert \\left \\vert w \\right \\vert \\right \\vert_2^2 \\ge \\sigma_1^2, \\] i.e. \\(\\left \\vert \\left \\vert w \\right \\vert \\right \\vert_2^2 = 0\\). So, \\[ \\tilde{U}_1 A V_1 = \\begin{bmatrix} \\sigma_1 &amp; 0 \\\\ 0 &amp; \\tilde{A} \\end{bmatrix}. \\] Repeat the same procedure to get \\(U, V\\) such that \\(A = U^\\prime \\Sigma V\\). Corollary 2 \\(\\text{rank}(A)\\) is exactly the number of non-zero singular values of \\(A\\). Proof. \\(\\text{rank}(A) = \\text{rank}(U \\Sigma V^\\prime ) = \\text{rank}(\\Sigma) =\\) number of non-zero diagonal elements. Corollary 3 Let \\(A,E \\in \\mathbb{R}^{n\\times m}\\), \\(\\sigma_{\\text{max}}\\) (\\(\\sigma_{\\text{min}}\\)) denote the largest (smallest) singular value of A. Then \\[ \\sigma_{\\text{max}}(A+E) \\le \\sigma_{\\text{max}}(A) + \\left \\vert \\left \\vert E \\right \\vert \\right \\vert_2 \\] and \\[ \\sigma_{\\text{min}}(A+E) \\ge \\sigma_{\\text{min}}(A) - \\left \\vert \\left \\vert E \\right \\vert \\right \\vert_2. \\] Corollary 4 (Hoffman-Wielandt Inequality) Let \\(A,E \\in \\mathbb{R}^{n \\times m}\\), \\(\\sigma_k(\\dot)\\) denote the k’th largest singular value. Let \\(p =\\min(m,n) \\le \\left \\vert \\left \\vert E \\right \\vert \\right \\vert_F^2\\). Then \\[ \\sum_{k=1}^p (\\sigma_k(A+E) - \\sigma_k(A))^2 \\le \\left \\vert \\left \\vert E \\right \\vert \\right \\vert_F^2. \\] Corollary 5 Let \\(r = \\text{rank}(A)\\). \\(\\text{range}(A) = \\text{span}(u_1, \\ldots, u_r)\\) row space of A \\(= \\text{span}(v_1, \\ldots, v_r)\\) \\(\\text{null}(A) = \\text{span}(v_{r+1}, \\ldots, v_{m})\\) \\(\\text{null}(A^\\prime) = \\text{span}(u_{r+1}, \\ldots, u_{m})\\) Moreover, \\(U_r = \\begin{bmatrix} u_1 &amp; \\cdots &amp; u_r \\end{bmatrix}\\), \\(U_r U_r^\\prime = P_{\\text{range}(A)}\\) \\(U_{n-r} = \\begin{bmatrix} u_{r+1} &amp; \\cdots &amp; u_n \\end{bmatrix}\\), \\(U_{n-r} U_{n-r}^\\prime = P_{\\text{null}(A)}\\) \\(V_r = \\begin{bmatrix} v_1 &amp; \\cdots &amp; v_r \\end{bmatrix}\\), \\(V_r V_r^\\prime = P_{\\text{row}(A)}\\) \\(V_{n-r} = \\begin{bmatrix} v_{r+1} &amp; \\cdots &amp; v_n \\end{bmatrix}\\), \\(V_{n-r} V_{n-r}^\\prime = P_{\\text{null}(A)}\\) where \\(P_{\\mathcal{B}}\\) is the projection onto the space \\(\\mathcal{B}\\). Definition 9 (Pseudo-inverse) For a matrix \\(A \\in \\mathbb{R}^{n\\times m}\\), we call \\(A^+ \\in \\mathbb{R}^{m\\times n}\\) a pseudo-inverse to \\(A\\) if \\(AA^+A = A\\) \\(A^+AA^+ = A^+\\) \\((AA^+)^\\prime = AA^+\\) \\((A^+A)^\\prime = A^+A\\) Theorem 4 For any matrix \\(A \\in \\mathbb{R}^{n \\times m}\\), there exists a unique pseudo-inverse \\(A^+\\). Proof. Suppose \\(B\\) and \\(C\\) are both pseudo-inverses of \\(A\\). Then \\[\\begin{aligned} BA &amp;= B(ACA) \\\\ &amp;= (BA)(CA) \\\\ &amp;= (BA)^\\prime (CA)^\\prime \\\\ &amp;= A^\\prime B^\\prime A^\\prime C^\\prime \\\\ &amp;= (ABA)^\\prime C^\\prime \\\\ &amp;= A^\\prime C^\\prime \\\\ &amp;= (CA)^\\prime \\\\ &amp;= CA. \\end{aligned}\\] Similarly, \\[\\begin{aligned} AB &amp;= (ACA)B \\\\ &amp;= (AC)(AB) \\\\ &amp;= (AC)^\\prime (AB)^\\prime \\\\ &amp;= C^\\prime A^\\prime B^\\prime A^\\prime \\\\ &amp;= C^\\prime (ABA)^\\prime \\\\ &amp;= C^\\prime A^\\prime \\\\ &amp;= (AC)^\\prime \\\\ &amp;= AC. \\end{aligned}\\] So, \\(B = BAB = CAB = CAC = C\\). This shows that if there exists a pseudo-inverse, it is unique. Now, let \\(D \\in \\mathbb{R}^{n\\times m}\\) be a diagonal matrix with \\(d_{ii}\\) its diagonal elements. Then the diagonal matrix \\(E \\in \\mathbb{R}^{m\\times n}\\) with diagonal elements \\(d_{ii}^{-1}\\) for \\(d_{ii} \\neq 0\\) and \\(0\\) otherwise is the pseudo-inverse of \\(D\\). So, \\[[DE]_{ij} = \\sum_{k=1}^m D_{ik} E_{kj} = \\left\\{\\begin{array}{rl} 0, &amp; i \\neq j \\\\ 1, &amp; i = j, d_{ii} \\neq 0 \\\\ 0, &amp; i = j, d_{ii} = 0 \\end{array} \\right .\\] and \\[[ED]_{ij} = \\sum_{k=1}^n D_{ik} E_{kj} = \\left\\{\\begin{array}{rl} 0, &amp; i \\neq j \\\\ 1, &amp; i = j, d_{ii} \\neq 0 \\\\ 0, &amp; i = j, d_{ii} = 0 \\end{array} \\right .\\] Since \\(ED\\) and \\(DE\\) are both diagonal matrices, \\((ED)^{\\prime} = ED\\) and \\((DE)^{\\prime} = DE\\). Furthermore, \\[[DED]_{ij} = \\sum_{k=1}^n D_{ik}[ED]_{kj} = \\left\\{\\begin{array}{rl} 0, &amp; i \\neq j \\\\ D_{ij}, &amp; i = j \\end{array} \\right .\\] and \\[[EDE]_{ij} = \\sum_{k=1}^n E_{ik}[DE]_{kj} = \\left\\{\\begin{array}{rl} 0, &amp; i \\neq j \\\\ E_{ij}, &amp; i = j \\end{array} \\right .\\] So \\(E\\) satisfies the four conditions of a pseudo-inverse. I.e. for any diagonal matrix, there exists a pseudo-inverse. Now, we know that there exist \\(U,\\Sigma, V\\) such that \\(A = U\\Sigma V^\\prime\\). Let \\(B = V \\Sigma^+ U^\\prime\\). Recall that \\(U^\\prime U = I\\) and \\(V^\\prime V = I\\). \\(ABA = U\\Sigma V^\\prime V \\Sigma^+ U^\\prime U\\Sigma V^\\prime = U \\Sigma \\Sigma^+ \\Sigma V^\\prime = U \\Sigma V^\\prime = A\\) \\(BAB = V \\Sigma^+ U^\\prime U \\Sigma V^\\prime V \\Sigma^+ U = V \\Sigma^+ \\Sigma \\Sigma^+ U^\\prime = V \\Sigma^+ U^\\prime = B\\) \\[\\begin{aligned} (AB)^\\prime = B^\\prime A^\\prime &amp;= (V \\Sigma^+ U^\\prime)^\\prime (U \\Sigma V^\\prime)^\\prime \\\\ &amp;= U (\\Sigma^+)^\\prime V^\\prime V \\Sigma^\\prime U^\\prime \\\\ &amp;= U (\\Sigma \\Sigma^+)^\\prime U^\\prime \\\\ &amp;= U \\Sigma \\Sigma^+ U^\\prime \\\\ &amp;= U \\Sigma V^\\prime V \\Sigma^+ U^\\prime \\\\ &amp;= AB \\end{aligned}\\] \\[\\begin{aligned} (BA)^\\prime = A^\\prime B^\\prime &amp;= (U \\Sigma V^\\prime)^\\prime (V \\Sigma^+ U^\\prime)^\\prime \\\\ &amp;= V \\Sigma^\\prime U^\\prime U (\\Sigma^+)^\\prime V^\\prime \\\\ &amp;= V (\\Sigma^+ \\Sigma)^\\prime V^\\prime \\\\ &amp;= V \\Sigma^+ \\Sigma V^\\prime \\\\ &amp;= V \\Sigma^+ U^\\prime U \\Sigma V^\\prime \\\\ &amp;= BA. \\end{aligned}\\] So, \\(B\\) is a pseudo-inverse of \\(A\\). Hence, any matrix has a unique pseudo-inverse. 1.3.4 Random Projections Let \\(A \\in \\mathbb{R}^{n\\times m}\\), \\(m &gt;&gt; n\\). When this is the case, it is very hard to compute the SVD. So instead we try to find a matrix \\(C\\) with \\(\\text{range}(C) \\approx \\text{range}(A)\\). In other words, \\(A \\approx P_{\\text{range}(C)}A\\). One way to find such a \\(C\\) is to simply sample columns from \\(A\\). Theorem 5 Suppose \\(A \\in \\mathbb{R}^{n \\times m}\\), \\(C\\) is as in the algorithm inexactRankK (see here), and \\(H\\) is the \\(k\\) left singular vectors of \\(C\\). Then \\[\\left \\vert \\left \\vert A-HH^\\prime A \\right \\vert \\right \\vert_F^2 \\le \\left \\vert \\left \\vert A-A_k \\right \\vert \\right \\vert_F^2 + 2\\sqrt{k} \\left \\vert \\left \\vert AA^\\prime - CC^\\prime \\right \\vert \\right \\vert_F,\\] where \\(A_k\\) is the rank \\(k\\) approximation of \\(A\\). Proof. Recall that \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2 = \\text{tr}(A^\\prime A)\\), and \\(H^\\prime H = I\\), since the columns of \\(H\\) are singular vectors, i.e. are orthogonal to each other. \\[\\begin{aligned} \\left \\vert \\left \\vert A - HH^\\prime A \\right \\vert \\right \\vert_F^2 &amp;= \\text{tr}((A-HH^\\prime A)^\\prime (A-HH^\\prime A)) \\\\ &amp;= \\text{tr}((A^\\prime - A^\\prime H H^\\prime)(A-HH^\\prime A)) \\\\ &amp;= \\text{tr}(A^\\prime A - A^\\prime H H^\\prime A - A^\\prime HH^\\prime A + A^\\prime H H^\\prime HH^\\prime A) \\\\ &amp;= \\text{tr}(A^\\prime A - A^\\prime H H^\\prime A) \\\\ &amp;= \\text{tr}(A^\\prime A) - \\text{tr}(A^\\prime H H^\\prime A) \\\\ &amp;= \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2 - \\left \\vert \\left \\vert A^\\prime H \\right \\vert \\right \\vert. \\end{aligned}\\] Since \\[ \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2 = \\left \\vert \\left \\vert A-A_k \\right \\vert \\right \\vert_F^2 + \\sum_{j=1}^k \\sigma_j^2(A), \\] we have that \\[\\begin{aligned} \\left \\vert \\left \\vert A-H H^\\prime A \\right \\vert \\right \\vert_F^2 &amp;= \\left \\vert \\left \\vert A-A_k \\right \\vert \\right \\vert_F^2 + \\sum_{j=1}^k \\sigma_j^2(A) - \\left \\vert \\left \\vert A^\\prime H \\right \\vert \\right \\vert_F^2 \\\\ &amp;= \\left \\vert \\left \\vert A-A_k \\right \\vert \\right \\vert_F^2 + \\sum_{j=1}^k \\left[\\sigma_j^2(A) - \\sigma_j^2(C)\\right] + \\sum_{j=1}^k \\sigma_j^2{C} - \\left \\vert \\left \\vert A^\\prime H \\right \\vert \\right \\vert_F^2. \\end{aligned}\\] Note that, using the Cauchy-Schwarts inequality, we get \\[\\begin{aligned} \\sum_{j=1}^k \\left[\\sigma_j^2(A) - \\sigma_j^2(C)\\right] &amp;= \\left | \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} \\sigma_1^2(A) - \\sigma_1^2(C) &amp; \\cdots &amp; \\sigma_k^2(A) - \\sigma_k^2(C) \\end{bmatrix} \\right | \\\\ &amp;\\le \\sqrt{\\left \\vert \\left \\vert \\begin{bmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix} \\right \\vert \\right \\vert \\left \\vert \\left \\vert \\begin{bmatrix} \\sigma_1^2(A) - \\sigma_1^2(C) &amp; \\cdots &amp; \\sigma_k^2(A) - \\sigma_k^2(C) \\end{bmatrix} \\right \\vert \\right \\vert} \\\\ &amp;= \\sqrt{k} \\sqrt{\\sum_{j=1}^k \\left(\\sigma_j^2(A) - \\sigma_j^2(C)\\right)^2} \\\\ &amp;\\le \\sqrt{k} \\sqrt{\\sum_{j=1}^k \\left(\\sigma_j(AA^\\prime) - \\sigma_j(CC^\\prime)\\right)^2} \\\\ &amp;= \\sqrt{k} \\sqrt{\\sum_{j=1}^k \\left(\\sigma_j(CC^\\prime + AA^\\prime - CC^\\prime) - \\sigma_j(CC^\\prime)\\right)^2} \\\\ &amp;\\le \\sqrt{k} \\left \\vert \\left \\vert AA^\\prime - CC^\\prime \\right \\vert \\right \\vert_F. \\end{aligned}\\] If \\(H_1, \\ldots, H_k\\) denote columns of \\(H\\), then \\(A^\\prime H = \\begin{bmatrix} A^\\prime H_1 &amp; \\cdots &amp; A^\\prime H_k \\end{bmatrix}\\). Since \\[ \\left \\vert \\left \\vert A^\\prime H \\right \\vert \\right \\vert_F^2 = \\sum_{j=1}^k \\left \\vert \\left \\vert A^\\prime H_j \\right \\vert \\right \\vert_2^2, \\] we can use Cauchy-Schwartz as above to obtain that \\[\\begin{aligned} \\sum_{j=1}^k \\sigma_j^2(C) - \\left \\vert \\left \\vert A^\\prime H \\right \\vert \\right \\vert_F^2 &amp;\\le \\sqrt{k} \\sqrt{\\sum_{j=1}^k \\left[\\sigma_j{CC^\\prime} - H_j^\\prime A A^\\prime H_j\\right]^2} \\\\ &amp;= \\sqrt{k} \\sqrt{\\sum_{j=1}^k \\left[H_j^\\prime CC^\\prime H_j - H_j^\\prime A A^\\prime H_j\\right]^2} \\\\ &amp;\\le \\sqrt{k} \\left \\vert \\left \\vert AA^\\prime - CC^\\prime \\right \\vert \\right \\vert_F, \\end{aligned}\\] where we in the last step use the Hoffman-Wielandt inequality (corollary 4). Combining all of this, we get \\[ \\left \\vert \\left \\vert A - HH^\\prime A \\right \\vert \\right \\vert_F^2 \\le \\left \\vert \\left \\vert A-A_k \\right \\vert \\right \\vert_F^2 + 2\\sqrt{k} \\left \\vert \\left \\vert AA^\\prime - CC^\\prime \\right \\vert \\right \\vert_F. \\] "],
["iterative-methods.html", "1.4 Iterative Methods", " 1.4 Iterative Methods 1.4.1 Overview First, some references: Golub and Van Loan Saad (2000): Iterative methods for sparse linear systems link Shewchuk (1994): Introduction to Conjugated Gradients without the Agonizing Pain link Gower and Richtarik (2015): Randomized Iterative Methods for Linear Systems link 1.4.2 Outline We will be going throuh the following: Why iterative methods? Splitting Methods Randomized Kaezmarz Method Gradient Descent Conjugated Gradients GMRES (optional) 1.4.3 Motivation These methods have had a great impact historically speaking. They take advantage of any sparsity found in a matrix, making them “easier” computational. When working with huge systems (such as weather prediction, computational chemistry, genomics, etc), this is the way to deal with it. 1.4.4 Splitting Methods Let \\(A\\) be a matrix, and \\(b\\) a vector. Let \\(x^c\\) denote the current iterate \\(x^+\\) denote the next iterate \\(r^c\\) denote the current residual \\(r^+\\) denote the next residual where \\(r^c = Ax^c - b\\) and \\(r^+ = Ax^+ - b\\). The goal is to find methods to minimize the residual by iteratively defining \\(x^+\\) based on \\(x^c\\). A splitting method takes a matrix \\(A\\) and splits it. One way of doing so is by splitting \\(A\\) into three parts \\(A = D - E - F\\), where \\(D\\) is the diagonal \\(E\\) is the negative lower triangular part of \\(A\\) excluding the diagonal \\(F\\) is the negative upper triangular part of \\(A\\) excluding the diagonal Based on this split, there are a few different ways to iteratively update \\(r^+\\). Definition 10 (The Jacobi Method) The Jacobi Method finds \\(x^+\\) from \\(x^c\\) using the following rule: \\[x_i^+ = \\frac{b_i - \\sum_{k\\neq i}A_{ik}x_k^c}{A_{ii}}.\\] If we do this for all \\(i\\), we can write this in matrix form: \\[x^+ = D^{-1}(b + (E+F)x^c).\\] For the Jacobi Method, we can see that \\(r_i^+ = 0:\\) \\[\\begin{aligned} x_i^+ &amp;= \\frac{b_i - \\sum_{k\\neq i}A_{ik}x_k^c}{A_{ii}} \\iff \\\\ A_{ii}x_i^+ &amp;= b_i - \\sum_{k\\neq i}A_{ik}x_k^c \\iff \\\\ 0 &amp;= b_i - \\sum_{k\\neq i}A_{ik}x_k^c - A_{ii}x_i^+ = b_i - \\sum_{k}A_{ik}x_k^c = r_i^+. \\end{aligned}\\] Definition 11 (The Gauss-Seidel Method) The Gauss-Seidel Method finds \\(x^+\\) from \\(x^c\\) using the following rule: \\[x_i^+ = \\frac{1}{A_ii}\\left(b_i - \\sum_{k=1}^{i-1} A_{ik}x_k^+ - \\sum_{k=i+1}^d A_{ik} x_k^c\\right),\\] which in matrix formulation is \\[x^+ = D^{-1}(b + Ex^+ + Fx^c)\\], or equivalently \\[x^+ = (D-E)^{-1}(b + Fx^c).\\] As for the Jacobi method, it can be seen that \\(r_i^+ = 0\\). Definition 12 (Successive Over Relaxation (SOR)) When we update \\(x^c\\) to \\(x^+\\) using a rule of the form \\[ (D-\\omega E)x^+ = (\\omega F + (1-\\omega)D)x^c + \\omega b,\\] it is called a Successive Over Relaxation method. Note: if we pick \\(\\omega = 1\\), we get back the Gauss-Seidel method. Definition 13 (Backward SOR) The following is called the Backward SOR: \\[ (D-\\omega E)x^+ = (\\omega F + (1-\\omega)D)x^c + \\omega b,\\] Definition 14 (Symmetric SOR) The Symmetric SOR is a method where we first find \\(z\\) based on the rule \\[(D-\\omega F)z = (\\omega F + (1-\\omega)D)x^c+\\omega b\\] before using the rule \\[(D- \\omega F)x^+ = (\\omega F + (1-\\omega)D)z + \\omega b\\] to find \\(x^+\\). 1.4.5 Convergence All of the schemes mentioned above are of the form \\(x^+ = G x^c + f\\): Jacobi: \\(G = D^{-1}(E+F)\\), \\(f = D^{-1}b\\). Gauss-Seidel: \\(G = (D-E)^{-1}F\\), \\(f = (D-E)^{-1} b\\). SOR: \\(G = (D - \\omega E)^{-1}(\\omega F + (1- \\omega)D)\\), \\(f = \\omega (D-\\omega E)^{-1} b\\). Now, if we assume that \\(x^*\\) is a vector such that \\(Ax^* = b\\), then \\((I-G)x^* = f\\): (for the following, keep in mind that \\(A = D - E - F\\)) Jacobi: \\[\\begin{aligned} f &amp;= D^{-1}b \\\\ &amp;= D^{-1}Ax^* \\\\ &amp;= D^{-1}(D-E-F)x^* \\\\ &amp;= (DD^{-1} - D^{-1}(E+F))x^*\\\\ &amp;= (I - G)x^*. \\end{aligned}\\] Gauss-Seidel: \\[\\begin{aligned} f &amp;= (D-E)^{-1}b \\\\ &amp;= (D-E)^{-1}Ax^* \\\\ &amp;= (D-E)^{-1}(D-E-F)x^* \\\\ &amp;= ((D-E)^{-1}(D-E)-(D-E)^{-1}F)x^* \\\\ &amp;= (I - G)x^*. \\end{aligned}\\] SOR: \\[\\begin{aligned} f &amp;= \\omega(D-\\omega E)^{-1}b\\\\ &amp;= \\omega(D-\\omega E)^{-1}Ax^* \\\\ &amp;= \\omega(D-\\omega E)^{-1}(D-E-F)x^* \\\\ &amp;= (D-\\omega E)^{-1}(\\omega D-\\omega E-\\omega F)x^* \\\\ &amp;= (D-\\omega E)^{-1}(D - D + \\omega D-\\omega E-\\omega F)x^* \\\\ &amp;= (D-\\omega E)^{-1}(D -\\omega E - [(1-\\omega)D + \\omega F])x^* \\\\ &amp;= ((D-\\omega E)^{-1}(D -\\omega E) - (D-\\omega E)^{-1}[D(1-\\omega) + \\omega F])x^* \\\\ &amp;= (I - G)x^*. \\end{aligned}\\] Lemma 13 For the Jacobi, Gauss-Seidel, and SOR methods, if there exists a \\(x^*\\) s.t. \\(Ax^* = b\\), then \\(x^+ - x^* = G(x^c - x^*)\\). Proof. We just saw that \\(Ax^* = b\\) implies \\((I-G)x^* = f\\). We also saw that \\(x^+ = Gx^c + f\\). Hence \\[\\begin{aligned} x^+ &amp;= G x^c + f \\\\ &amp;= G x^c + (I-G)x^* \\\\ &amp;= G x^c + x^* - Gx^* \\iff \\\\ x^+ - x^* &amp;= G(x^c - x^*). \\end{aligned}\\] Theorem 6 Suppose there exists a \\(x^*\\) s.t. \\(A x^* = b\\). Let \\(x_0\\) be arbitrary and define a sequence \\(\\{x_k\\}_{k \\in \\mathbb{N}}\\) by \\[ x_k = Gx_{k-1} + f. \\] If \\(\\rho(G) &lt; 1\\), then \\(x^*\\) is unique, and \\(x_k \\rightarrow x^*\\) as \\(k \\rightarrow \\infty\\). In the theorem above, \\(\\rho(G)\\) is the spectral radius of the matrix \\(G\\). This is defined as the largest eigenvalue of the matrix \\(G\\). Definition 15 (Jordan Canonical Form) Let \\(A \\in \\mathbb{R}^{n \\times n}\\). There exists an \\(X\\) which is invertible, and a block-diagonal matrix \\(J\\), whose blocks are of the form \\(\\lambda I + E\\), where \\(\\lambda\\) is an eigenvalue of \\(A\\), and \\[ E = \\left\\{ \\begin{array}{rl} 1, &amp; j = i+1 \\\\ 0, &amp; \\text{ otherwise} \\end{array} \\right . \\] such that \\(A = XJX^{-1}\\). 1.4.6 Randomized Kaczmarz Method Consider \\(Ax = b\\). Let the rows of \\(A\\) be denoted as \\(a_1^\\prime, a_2^\\prime, \\dots, a_n^\\prime\\), and \\(b = (b_1, \\dots, b_n)^\\prime\\). Now, the goal is to choose the next iteration of \\(x\\) such that \\(r^+ = b_i - a_i^\\prime x^+ = 0\\). Let’s say we try to set \\(x^+ = x^c + v\\) for some appropriate \\(v\\). What should \\(v\\) be then? We want \\(b_i = a_i^\\prime (x^c + v)\\), which would imply that \\(a_i^\\prime v = b_i - a_i^\\prime x^c\\). This has many possible solutions. So let’s look for a particular one, namely one that is proportional to \\(a_i\\): \\(v=\\alpha a_i\\). Then \\(\\alpha \\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2 = b_i - a_i^\\prime x^c\\) which implies \\(\\alpha = \\frac{b_i - a_i^\\prime x^c}{\\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2}\\). The above approach gives us Kaczmarz Method: the next iterate of \\(x\\) is \\(x^+ = x^c + \\frac{b_i - a_i^\\prime x^c}{\\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2}a_i\\). Now, the Randomized Kaczmarz Method is the one where we randomly choose which column to use for the next iteration based on the probability distribution over all columns of \\(A\\) given by \\(P(i = l) = \\frac{\\left \\vert \\left \\vert a_l \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2}\\). The following theorem guarantees us that this approach actually works, i.e. we converge towards the solution of \\(Ax = b\\). Theorem 7 (Randomized Kaczmarz Method) Suppose \\(A \\in \\mathbb{R}^{n\\times m}\\) is invertible and \\(x^* = A^{-1}b\\). Given a sequence of i.i.d. random variables \\(i_1, i_2, \\dots\\) (\\(P(i_k = l) = \\frac{\\left \\vert \\left \\vert a_l \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2}\\)), and \\(x_0\\) an arbitrary initial value, define \\[ x_k = x_{k-1} + \\frac{b_{i_{k-1}}-a_{i_{k-1}}^\\prime x_{k-1}}{\\left \\vert \\left \\vert a_{i_{k-1}} \\right \\vert \\right \\vert_2^2}a_{i_{k-1}}\\] for all \\(k &gt; 1\\). Then \\[E\\left[\\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2 \\right] \\leq \\left(1-\\frac{\\sigma_{min}(A)^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2}\\right)^k \\left \\vert \\left \\vert x_0 - x^* \\right \\vert \\right \\vert_2^2.\\] Note that the above result (by Chebyshev) implies \\[P\\left[\\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2 &gt; \\epsilon \\right] \\leq \\frac{1}{\\epsilon^2}E\\left[\\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2\\right]\\] which in turn implies \\(x_k \\rightarrow_p x^*\\) as \\(k \\rightarrow \\infty\\). Proof. First of all, note that since \\(i_k\\) is independent of \\(i_{l}\\) for all \\(l &lt; k\\), \\(i_k\\) is independent of all \\(x_l\\) for all \\(l &lt; k\\). Observation 1: \\(x_{k+1} - x_k\\) is orthogonal to \\(x_{k+1} - x^*\\). Therefore \\[\\left \\vert \\left \\vert x_{k+1} - x^* \\right \\vert \\right \\vert_2^2 = \\left \\vert \\left \\vert x_{k} - x^* \\right \\vert \\right \\vert_2^2 - \\left \\vert \\left \\vert x_{k+1}-x_k \\right \\vert \\right \\vert_2^2.\\] Since \\(b_{i_k} = a_{i_k}^\\prime x^*\\), we have that \\[\\begin{aligned} x_{k+1} - x_k &amp;= \\frac{b_{i_k}-a_{i_k}^\\prime x_k}{\\left \\vert \\left \\vert a_{i_k} \\right \\vert \\right \\vert_2^2}a_{i_k} \\\\ &amp;= \\frac{a_{i_k}^\\prime x^*-a_{i_k}^\\prime x_k}{\\left \\vert \\left \\vert a_{i_k} \\right \\vert \\right \\vert_2^2}a_{i_k} \\\\ &amp;= \\frac{a_{i_k}^\\prime (x^*- x_k)}{\\left \\vert \\left \\vert a_{i_k} \\right \\vert \\right \\vert_2^2}a_{i_k}. \\end{aligned}\\] So \\[\\begin{aligned} \\left \\vert \\left \\vert x_{k+1} - x_k \\right \\vert \\right \\vert_2^2 &amp;= \\left \\vert \\left \\vert a_{i_k} \\right \\vert \\right \\vert_2^2 \\frac{[a_{i_k}^\\prime (x_k - x^*)]^2}{\\left \\vert \\left \\vert a_{i_k} \\right \\vert \\right \\vert_2^4} \\\\ &amp;= \\frac{[a_{i_k}^\\prime (x_k - x^*)]^2}{\\left \\vert \\left \\vert a_{i_k} \\right \\vert \\right \\vert_2^2}. \\end{aligned}\\] Observation 2: For any vector \\(y \\in \\mathbb{R}^n\\) it holds that \\[E\\left(\\frac{(a_{i_0}^\\prime y)^2}{\\left \\vert \\left \\vert a_{i_0} \\right \\vert \\right \\vert_2^2}\\right) \\geq \\frac{\\sigma_{min}^2(A)}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2} \\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2^2.\\] “Proof:” Recall that \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2^2 = \\max_{y \\neq 0} \\frac{\\left \\vert \\left \\vert Ay \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2^2} = \\sigma_{max}(A)^2 &gt;= \\sigma_{min}(A)^2\\). With this in mind \\[\\begin{aligned} \\sum_{i=1}^n (a_i^\\prime y)^2 &amp;= \\left \\vert \\left \\vert Ay \\right \\vert \\right \\vert_2^2 \\ge \\sigma_{min}(A)^2 \\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2^2. \\end{aligned}\\] Divide through by \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2\\) to get \\(\\sum_{i=1}^n \\frac{1}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2}(a_i^\\prime y)^2 \\ge \\frac{\\sigma_{min}(A)^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2} \\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2^2\\), before we simply multiply each term in the sum by \\(1 = \\frac{\\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2}\\). So \\[\\begin{aligned} \\frac{\\sigma_{min}(A)^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2} \\left \\vert \\left \\vert y \\right \\vert \\right \\vert_2^2 &amp;\\le \\sum_{i=1}^n \\frac{\\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2}\\frac{(a_i^\\prime y)^2}{\\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2} \\\\ &amp;= \\sum_{i=1}^n P(i_0 = i) \\frac{(a_i^\\prime y)^2}{\\left \\vert \\left \\vert a_i \\right \\vert \\right \\vert_2^2} \\\\ &amp;= E\\left(\\frac{(a_{i_0}^\\prime y)^2}{\\left \\vert \\left \\vert a_{i_0} \\right \\vert \\right \\vert_2^2}\\right). \\end{aligned}\\] Now, using these two observations together: \\[\\begin{aligned} E\\left[\\left \\vert \\left \\vert x_{k+1} - x^* \\right \\vert \\right \\vert_2^2 | x_k \\right ] &amp;= E\\left[\\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2 | x_k\\right] - E\\left[ \\left \\vert \\left \\vert x_{k+1} - x_k \\right \\vert \\right \\vert_2^2 | x_k \\right] \\\\ &amp;= \\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2 - E\\left[ \\frac{(a_{i_k}^\\prime (x_k - x^*))^2}{\\left \\vert \\left \\vert a_{i_k} \\right \\vert \\right \\vert_2^2} \\right] \\\\ &amp;\\le \\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2 - \\frac{\\sigma_{min}(A)^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2} \\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2 \\\\ &amp;= (1 - \\frac{\\sigma_{min}(A)^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2})\\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2. \\end{aligned}\\] Finally, taking expectation on both sides gives us that \\(E\\left[\\left \\vert \\left \\vert x_{k+1} - x^* \\right \\vert \\right \\vert_2^2\\right] \\le \\left(1 - \\frac{\\sigma_{min}(A)^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2}\\right)E\\left[\\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2^2\\right]\\). This upper bound holds for any \\(k\\), so we can repeatedly use this to get \\[ E\\left[\\left \\vert \\left \\vert x_{k+1} - x^* \\right \\vert \\right \\vert_2^2\\right] \\le \\left(1 - \\frac{\\sigma_{min}(A)^2}{\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2}\\right)^k \\left \\vert \\left \\vert x_0 - x^* \\right \\vert \\right \\vert_2^2 \\] since \\(E[\\left \\vert \\left \\vert x_0 - x^* \\right \\vert \\right \\vert_2^2] = \\left \\vert \\left \\vert x_0 - x^* \\right \\vert \\right \\vert_2^2\\). 1.4.7 Gradient Descent The Gradient Descent iterative method updates the current \\(x^c\\) to \\(x^+\\) by \\[\\begin{align} x^+ &amp;= x^c + \\alpha \\sum_{i=1}^n a_i (b_i - a_i x^c) \\\\ &amp;= x^c + \\alpha A^\\prime (b - Ax^c) \\end{align}\\] The question now is: how do we choose \\(\\alpha\\)? Strategy 1 Choose the \\(\\alpha\\) that minimizes \\(\\left \\vert \\left \\vert r^+ \\right \\vert \\right \\vert_2^2 = \\left \\vert \\left \\vert Ax^+ - b \\right \\vert \\right \\vert_2^2\\). Lemma 14 \\[\\text{argmin}_\\alpha \\left \\vert \\left \\vert r^+ \\right \\vert \\right \\vert_2^2 = \\frac{\\left \\vert \\left \\vert A^\\prime r^c \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert AA^\\prime r^c \\right \\vert \\right \\vert_2^2}\\] Proof. First, note that by homework exercise 67, \\(A^\\prime r^c \\neq 0\\), since if this were the case, the problem would be solved already. \\[ \\left \\vert \\left \\vert r^+ \\right \\vert \\right \\vert_2^2 = \\left \\vert \\left \\vert r^c \\right \\vert \\right \\vert_2^2 - 2\\alpha \\left \\vert \\left \\vert A^\\prime r^c \\right \\vert \\right \\vert_2^2 + \\alpha^2 \\left \\vert \\left \\vert AA^\\prime r^c \\right \\vert \\right \\vert_2^2, \\] since \\(r^+ = Ax^+ - b = A(x^c + \\alpha A^\\prime (b - Ax^c)) - b = Ax^c - b - \\alpha A A^\\prime ( Ax^c - b) = r^c - \\alpha A A^\\prime r^c\\), and \\(\\left \\vert \\left \\vert r^+ \\right \\vert \\right \\vert_2^2 = (r^+)^\\prime r^+\\). Differentiate this equation with respect to \\(\\alpha\\), equate to \\(0\\), and solve for \\(\\alpha\\) will yield the result. Theorem 8 Suppose \\(A\\) is invertible. Given \\(x_0 \\mathbb{R}^n\\), let \\(x_k\\) be defined as \\[x_k = x_{k-1} + \\alpha_{k-1}A^\\prime (b-Ax_{k-1}),\\] where \\[ \\alpha_k = \\frac{\\left \\vert \\left \\vert A^\\prime r_k \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert AA^\\prime r_k \\right \\vert \\right \\vert_2^2}.\\] Then \\[ \\left \\vert \\left \\vert r_k \\right \\vert \\right \\vert_2^2 \\le \\left(1 - \\frac{4\\sigma_1^2 \\sigma_n^2}{\\sigma_1^2 + \\sigma_n^2}\\right)^k \\left \\vert \\left \\vert r_0 \\right \\vert \\right \\vert_2^2. \\] Proof. \\[\\begin{aligned} \\left \\vert \\left \\vert r_{k+1} \\right \\vert \\right \\vert_2^2 &amp;= \\left \\vert \\left \\vert r_k \\right \\vert \\right \\vert_2^2 - \\alpha_k 2\\left \\vert \\left \\vert A^\\prime r_k \\right \\vert \\right \\vert_2^2 + \\alpha_k^2 \\left \\vert \\left \\vert AA^\\prime r_k \\right \\vert \\right \\vert_2^2 \\\\ &amp;= \\left \\vert \\left \\vert r_k \\right \\vert \\right \\vert_2^2 - \\frac{\\left \\vert \\left \\vert A^\\prime r_k \\right \\vert \\right \\vert_2^4}{\\left \\vert \\left \\vert A A^\\prime r_k \\right \\vert \\right \\vert_2^2} \\\\ &amp;= \\left \\vert \\left \\vert r_k \\right \\vert \\right \\vert_2^2 \\left(1 - \\frac{\\left \\vert \\left \\vert A^\\prime r_k \\right \\vert \\right \\vert_2^4}{\\left \\vert \\left \\vert r_k \\right \\vert \\right \\vert_2^2 \\left \\vert \\left \\vert AA^\\prime r_k \\right \\vert \\right \\vert_2^2}\\right). \\end{aligned}\\] If we let \\(A = U\\Sigma V^\\prime\\) be the SVD of \\(A\\), then \\[\\begin{aligned} 1 - \\frac{\\left \\vert \\left \\vert A^\\prime r_k \\right \\vert \\right \\vert_2^4}{\\left \\vert \\left \\vert r_k \\right \\vert \\right \\vert_2^2 \\left \\vert \\left \\vert AA^\\prime r_k \\right \\vert \\right \\vert_2^2} &amp;= 1 - \\frac{\\left \\vert \\left \\vert \\Sigma U^\\prime r_k \\right \\vert \\right \\vert_2^4}{\\left \\vert \\left \\vert U^\\prime r_k \\right \\vert \\right \\vert_2^2 \\left \\vert \\left \\vert \\Sigma \\Sigma^\\prime U^\\prime r_k \\right \\vert \\right \\vert_2^2} \\\\ &amp;= 1 - \\frac{\\left \\vert \\left \\vert w \\right \\vert \\right \\vert_2^4}{\\left \\vert \\left \\vert \\Sigma^{-1}w \\right \\vert \\right \\vert_2^2 \\left \\vert \\left \\vert \\Sigma w \\right \\vert \\right \\vert_2^2}. \\end{aligned}\\] We want to find an upper bound on \\(\\left \\vert \\left \\vert \\Sigma^{-1}u \\right \\vert \\right \\vert_2^2 \\left \\vert \\left \\vert \\Sigma u \\right \\vert \\right \\vert_2^2\\) for any unit vector \\(w\\). \\[\\begin{aligned} \\left \\vert \\left \\vert \\Sigma^{-1}u \\right \\vert \\right \\vert_2^2 \\left \\vert \\left \\vert \\Sigma u \\right \\vert \\right \\vert_2^2 &amp;= \\left( \\sum_{i=1}^k \\frac{u_i^2}{\\sigma_i^2} \\right) \\left( \\sum_{i=1}^n \\sigma_i^2 u_i^2\\right) \\\\ \\le \\frac{(\\sigma_1^2 + \\sigma_n^2)^2}{4\\sigma_1^2 \\sigma_n^2}, \\end{aligned}\\] where the last inequality is the Kontorovich inequality (see homework 69). Strategy 2 Choose \\(\\alpha = \\text{argmin}_\\alpha \\left \\vert \\left \\vert x_{k+1} - x^* \\right \\vert \\right \\vert_2^2\\). Strategy 3 Note that \\(\\left \\vert \\left \\vert r^+ \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert (I - \\alpha A A^\\prime)r^c \\right \\vert \\right \\vert_2 \\le \\left \\vert \\left \\vert I-\\alpha A A^\\prime \\right \\vert \\right \\vert_2 \\left \\vert \\left \\vert r^c \\right \\vert \\right \\vert_2\\). Choose \\(\\alpha = \\text{argmin}_\\alpha \\left \\vert \\left \\vert I - \\alpha A A^\\prime \\right \\vert \\right \\vert_2\\). Strategy 4 Note that \\(\\left \\vert \\left \\vert x_{k+1} - x^* \\right \\vert \\right \\vert_2 \\le \\left \\vert \\left \\vert I-\\alpha A A^\\prime \\right \\vert \\right \\vert_2 \\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2\\). Choose \\(\\alpha = \\text{argmin}_\\alpha \\left \\vert \\left \\vert I-\\alpha A A^\\prime \\right \\vert \\right \\vert_2 \\left \\vert \\left \\vert x_k - x^* \\right \\vert \\right \\vert_2\\). 1.4.8 Conjugate Gradient 1.4.8.1 Orthogonal Search Directions Let \\(A \\in \\mathbb{R}^{d \\times d}\\) be a symmetric positive definite matrix, \\(\\{x_k\\}\\) be a sequence where \\(x_0\\) is arbitrary, and \\[x_{k+1} = x_k - \\alpha_k s_k.\\] We call \\(\\alpha_k\\) the step length, and \\(s_k\\) the search direction. The question is then: given an orthonormal set of vectors \\(\\{s_k\\}\\) and an arbitrary \\(x_0\\), how do we choose \\(\\alpha_k\\) such that we eventually find \\(x^*\\) with \\(Ax^* = b\\)? From \\(\\{s_k\\}\\), take all unique vectors: \\(s_0, s_1, ..., s_{d_1}\\) (note: we there are at most \\(d\\) since these are orthonormal vectors from \\(\\mathbb{R}^d\\)). Since we assumed that \\(A\\) is positive definite, it is invertible. Let \\(x^* = A^{-1}x\\) and take \\(\\alpha_k = \\left&lt;x_0 - x^*, s_k\\right&gt;\\). This gives us that \\[\\begin{align} x_0 - x^* &amp;= \\sum_{i=0}^{d-1} \\alpha_i s_i \\iff \\\\ x_0 - \\sum_{i=0}^{d-1} \\alpha_i s_i &amp;= x^* \\end{align}\\] But since \\(x_k - x_{k+1} = \\alpha_k s_k\\), \\[\\begin{align} x^* &amp;= x_0 - \\sum_{i=0}^{d-1} \\alpha_i s_i \\\\ &amp;= x_0 - \\sum_{i=0}^{d-1} (x_i - x_{i+1}) \\\\ &amp;= x_d. \\end{align}\\] So, this choice of \\(\\alpha_k\\) ensures that we will eventually find the solution. Since \\(A\\) is symmetric and positive definite, \\[\\begin{align} \\alpha_k &amp;= (x_0 - x^*)^\\prime s_k \\\\ &amp;= (Ax_0 - Ax^*)^\\prime A^{-1} s_k \\\\ &amp;= (Ax_0 - b)^\\prime A^{-1} s_k \\\\ &amp;= \\left&lt;Ax_0 - b, s_k\\right&gt;_{A^{-1}}. \\end{align}\\] This shows us that our choice of \\(\\alpha_k\\) is actually the inner product of \\(Ax_0 - b\\) and \\(s_k\\) with respect to the matrix \\(A^{-1}\\). But calculating \\(A^{-1}\\) requires a lot of effort. Now, what if we use the inner product \\(\\left&lt;\\cdot,\\cdot\\right&gt;_A\\) instead? I.e. \\(\\alpha_k = \\left&lt;x_0-x^*,s_k\\right&gt;_A = (Ax_0 - Ax^*)^\\prime s_k = r_0^\\prime s_k\\). This we can actually calculate. The problem now is that the \\(s_k\\)’s are not orthonormal in this new metric. So, we need a way to create a set that is: 1.4.8.2 Conjugation Definition 16 (Conjugated Set) A set of vectors \\(\\{s_0, \\ldots, s_{d-1}\\} \\subset \\mathbb{R}^d\\) are (\\(A-\\))conjugated if they are orthogonal w.r.t. the metric \\(&lt;\\cdot,\\cdot&gt;_A\\) (i.e. \\(s_i^\\prime A s_j = 0\\) for all \\(i \\neq j\\)). We can use Gram-Schmidt to do this: given a set of linearly independent vectors \\(\\{a_0, ..., a_{d-1}\\}\\), let \\(s_0 = \\frac{a_0}{\\left \\vert \\left \\vert a_0 \\right \\vert \\right \\vert_A}\\) and \\[ s_j = \\frac{a_j - \\sum_{i=0}^{j-1} &lt;a_j, s_i&gt;_A \\cdot s_i}{\\left \\vert \\left \\vert a_j - \\sum_{i=0}^{j-1} &lt;a_j, s_i&gt;_A \\cdot s_i \\right \\vert \\right \\vert_A} \\] for all \\(j &gt; 0\\). Given a set \\(\\{s_0, ..., s_{d-1}\\}\\) that is \\(A-\\)conjugated and an arbitrary \\(x_0\\), pick \\[\\alpha_k = \\frac{&lt;x_0-x^*, s_k&gt;_A}{\\left \\vert \\left \\vert s_k \\right \\vert \\right \\vert_A} = \\frac{r_0^\\prime s_k}{\\left \\vert \\left \\vert s_k \\right \\vert \\right \\vert_A}.\\] Lemma 15 Let \\(A \\in \\mathbb{R}^{d\\times d}\\) be a symmetric, positive definite matrix (i.e. all eigenvalues are positive), \\(b\\in \\mathbb{R}^d\\) be arbitrary, and \\(x^* \\in \\mathbb{R}^d\\) such that \\(Ax^* = b\\). Let \\(\\{s_0, ..., s_{d-1}\\}\\) be an \\(A-\\)conjugated and \\(A-\\)normalized set, and \\(x_0 \\in \\mathbb{R}^d\\) be arbitrary. Define the sequence \\(\\{x_k\\}\\) by \\(x_{k+1} = x_k - \\alpha_k s_k\\), where \\(\\alpha_k = r_k^\\prime s_k\\). Then the following hold: \\(\\alpha_k = r_0^\\prime s_k\\), \\(0 = r_{k+1}^\\prime s_j\\) for all \\(j=0,1,...,k\\). \\(x_d = x^*\\). Proof. By definition, \\(x_k = x_0 - \\sum_{i=0}^{k-1} \\alpha_i s_i\\). This implies that \\(Ax_k = r_k = r_0 - \\sum_{i=0}^{k-1} \\alpha_i A s_i\\). WTS (1): \\[\\begin{aligned} \\alpha_k &amp;= s_k^\\prime r_k \\\\ &amp;= s_k^\\prime r_0 - \\sum_{i=0}^{k-1}\\alpha_i s_k^\\prime A s_i \\\\ &amp;= s_k^\\prime r_0 - \\sum_{i=0}^{k-1}\\alpha_i &lt;s_k,s_i&gt;_A \\\\ &amp;= s_k^\\prime r_0 \\end{aligned}\\] WTS (2): Let \\(l &lt; k\\). Recall that \\(A\\) is symmetrical, i.e. \\(A^\\prime = A\\), and \\(s_i^\\prime A s_l = 0\\) for all \\(i \\neq l\\). Now, \\[\\begin{aligned} r_{k+1}^\\prime s_l &amp;= \\left(r_0 - \\sum_{i=0}^k \\alpha_i A s_i\\right)^\\prime s_l \\\\ &amp;= r_0^\\prime s_l - \\sum_{i=0}^k \\alpha_i s_i^\\prime A^\\prime s_l \\\\ &amp;= \\alpha_l - \\alpha_l s_l^\\prime A s_l = 0. \\end{aligned}\\] WTS (3): Suppose \\(x_d \\neq x^*\\). Then \\(r_d = Ax_d - b \\neq 0\\). But (2) implies that \\(r_d^\\prime s_l = 0\\) for all \\(l &lt; d\\). I.e. \\(r_d\\) is linearly independent of \\(\\text{span}\\{s_0, ..., s_{d-1}\\} \\subset \\mathbb{R}^d\\), so \\(r_d\\) must be \\(0\\), which contradicts the assumption that \\(x_d \\neq x^*\\). 1.4.8.3 What is the choice of vectors to conjugate? (ref:conjugated_gradients) Conjugate gradient \\(A-\\)conjugates residuals \\(r_0, ..., r_{d-1}\\). Step 0: given arbitrary \\(x_0 \\in \\mathbb{R}^d\\), compute \\(s_0 = \\frac{r_0}{\\left \\vert \\left \\vert r_0 \\right \\vert \\right \\vert_A}\\). Update the iterate to \\(x_1 = x_0 - \\alpha_0 s_0\\), where \\(\\alpha_0 = r_0^\\prime s_0\\), and update the residual to \\(r_1 = r_0 - \\alpha_0 A s_0\\). Step 1: \\(U_{12} = &lt;r_1, s_0&gt;_A = r_1^\\prime A s_0 = r_1^\\prime \\frac{r_0-r_1}{\\alpha_0} = -\\frac{\\left \\vert \\left \\vert r_1 \\right \\vert \\right \\vert^2}{\\alpha_0}\\), since \\(r_0 \\perp r_1\\). Unnormalized \\(A-\\)conjugate direction: \\(P_1 = r_1 - U_{12} \\cdot s_0\\). Normalize and compute search direction: \\(U_{22} = \\left \\vert \\left \\vert P_1 \\right \\vert \\right \\vert_A\\) and \\(s_1 = \\frac{P_1}{U_{22}}\\). Update: \\(x_2 = x_1 - \\alpha_1 s_1\\) and \\(r_2 = r_1 - \\alpha_1 A s_1\\), where \\(\\alpha_1 = r_1^\\prime s_1 = \\left(r_1^\\prime r_1 - U_{12} r_1^\\prime s_0\\right)/U_{22} = \\frac{\\left \\vert \\left \\vert r_1 \\right \\vert \\right \\vert_2^2}{U_{22}}\\) (since \\(r_1^\\prime s_0 = 0\\)). Check that lemma still holds: step length: \\(\\alpha_1 = r_1^\\prime s_1 = (r_0 - \\alpha_0 A s_0)^\\prime s_1 = r_0^\\prime s_1\\) orthogonality: \\(r_2^\\prime s_j = r_0 - \\alpha_0 A s_0 - \\alpha_1 A s_1)^\\prime s_j = r_0^\\prime s_j - \\alpha_j = 0\\). A few key observations: \\(r_0 \\in \\text{span}\\{s_0\\}, r_1 \\in \\text{span}\\{s_0,s_1\\}\\). since \\(r_2 \\perp s_0, s_1\\), \\(r_2^\\prime r_j = 0\\) for \\(j=0,1\\). In general, let’s say we’re at step \\(L\\). This means we have already calculated \\(s_0, ..., s_{L-1}\\), \\(r_0,...,r_L\\), and \\(x_0,...,x_L\\). To get the next iterate, we would do the following: Gram-Schmidt: Compute the inner products for \\(x_L\\) with \\(s_j\\) for all \\(j &lt; L\\). Recall that \\(r_{j+1} = r_j - \\alpha_j r_L^\\prime A s_j\\). For \\(j &lt; L-1\\): \\(&lt;r_L, s_j&gt;_A = r_L^\\prime A s_j = r_L^\\prime \\left(\\frac{r_{j+1} - r_j}{-\\alpha_j}\\right) = 0\\), since \\(r_L \\perp s_j\\) for all \\(j &lt; L-1\\), and \\(r_j\\) is simply a linear combination of these. &lt;&gt; For \\(j = L-1\\): \\(&lt;r_L, s_{L-1}&gt;_A = r_A^\\prime \\left(\\frac{r_L - r_{L-1}}{-\\alpha_{L-1}}\\right) = \\frac{\\left \\vert \\left \\vert r_L \\right \\vert \\right \\vert_2^2}{-\\alpha_{L-1}}\\). Find unnormalized search direction: \\(P_L = r_L - \\sum_{j=0}^{L-1} &lt;r_L, s_j&gt;_A s_j = r_L + \\frac{\\left \\vert \\left \\vert r_L \\right \\vert \\right \\vert_2^2}{\\alpha_{L-1}}s_{L-1}\\) Calculate normalization constant: \\(\\left \\vert \\left \\vert P_L \\right \\vert \\right \\vert_A\\) Calculate final normalization direction: \\(s_L = \\frac{P_L}{\\left \\vert \\left \\vert P_L \\right \\vert \\right \\vert_A}\\). Update iterates: \\(\\alpha_L = &lt;s_L, r_L&gt; = \\frac{\\left \\vert \\left \\vert r_L \\right \\vert \\right \\vert_2^2}{\\left \\vert \\left \\vert P_L \\right \\vert \\right \\vert_A}\\). \\(x_{L+1} = x_L - \\alpha_L s_L\\). \\(r_{L+1} = r_L - \\alpha_L A s_L\\). When working with exact arithmetic, CG is guaranteed to converge in a finite number of steps. However, when implementing in algorithms, there’s no such guarentee. It will converge in the limit, and the convergence rate is of the order \\(\\frac{\\sqrt{k} - 1}{\\sqrt{k} + 1}\\), where \\(k\\) is the condition number. If \\(k\\) is very large, the convergence happens very slowly. This is why there are a lot of work being done in socalled “precondition methods”, that is methods that aim at transforming the matrix \\(A\\) such that the condition number is smaller, but features are not lost. So far, we have talked abouth iterative methods without much motivation or justification. However, as we will see later, iterative methods are very useful when dealing with non-linear systems of equations. "],
["nonlinear-systems-of-equations.html", "1.5 Nonlinear Systems of Equations", " 1.5 Nonlinear Systems of Equations Outline: I. Review of Jacobians II. Motivating example: clustered data III. Non-linear systems of equations IV. Picard’s method V. Newton’s method VI. Inexact Newton’s Method VII. Linear Search VIII. Semi-smooth Newton’s Method 1.5.1 Review of Jacobians Definition 17 Let \\(f: \\mathbb{R}^m \\to \\mathbb{R}\\). The gradient of \\(f\\) at \\(x \\in \\mathbb{R}^m\\) is a vector \\(g\\) (if it exists) such that \\[ \\lim_{h \\to 0} \\frac{\\left| f(x+h) - f(x) - &lt;g,h&gt;\\right |}{\\left \\vert \\left \\vert h \\right \\vert \\right \\vert} = 0. \\] Example 12 Let \\(f(z) = z_1 ... z_m\\). Guess that the gradient is \\(g\\) with coordinate vectors \\(g_1(x) = x_2...x_m; ...; g_k(x) = x_1,...,x_{k-1}, x_{k+1}, ..., x_m; ...; g_m(x) = x_1,...,x_{m-1}\\). Check that \\(g\\) is the gradient: \\[ \\frac{\\left|(x_1+h_1)\\cdots(x_m+h_m) - x_1\\cdots x_m - \\sum_{k=1}^m h_k \\prod_{i \\neq k} x_i \\right|}{\\sqrt{\\sum_{k=1}^m h_k^2}} = \\frac{\\left| \\sum_{k=1}^m \\sum_{l\\neq k} O(h_k h_l) \\right |}{\\sqrt{\\sum_{k=1}^m h_k^2}} \\to 0. \\] Definition 18 Let \\(f: \\mathbb{R}^m \\to \\mathbb{R}^n\\). The Jacobian at a point \\(x\\) in \\(\\mathbb{R}^m\\) (if it exists) is a function satisfying \\[ \\lim_{h\\to 0} \\frac{\\left \\vert \\left \\vert f(x+h) - f(x) - Jh \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert h \\right \\vert \\right \\vert} = 0. \\] Note that the norm in the numerator is a norm in \\(\\mathbb{R}^n\\) and the norm in the denominator is a norm in \\(\\mathbb{R}^m\\). Example 13 Let \\(f: \\mathbb{R}^m \\to \\mathbb{R}^{m-1}\\) be given by the coordinate functions \\(f_k(z) = z_k z_{k-1}\\). Guess for the Jacobian: \\[ J_{ij} = \\left\\{ \\begin{array}{rl} 0, &amp; j\\neq 1, j \\neq i+1 \\\\ x_{i+1}, &amp; j = 1, \\\\ x_i, &amp; j = i+1 \\end{array} \\right. \\] 1.5.2 Motivating Problem Consider a scenario where we enroll 100 patients for a study. Each patient will undergo two different treatments for a disease. The treatment is given at visits every two weeks for a one year period. At the first visit, we record which treatment is given, gender \\(\\{0,1\\}\\), and concentration of the given treatment in the patients blood. For every follow up visit, we record the treatment status \\(\\{0,1,2,3\\}\\). We would like to model the treatment status based on the other mentioned variables. To do so, we assume that the patients are independent, but the disease status is correlated in time (status at closer time points are more likely to be similar). To model this data, we let \\(Y_i \\in \\{0,1,2,3\\}^{26}\\) denote the disease statuses collected at the 26 visits for patient \\(i\\), and \\(X_{ij} \\in \\{0,1\\} \\times \\{0,1\\} \\times (0,1)\\) is the covariate matrix for the \\(i\\)’th patient at time point \\(j\\). For a vector of coefficients \\(\\beta \\in \\mathbb{R}^p\\), a correlation value \\(\\rho \\in [0,1]\\), and a function \\(\\mu: \\mathbb{R}^p \\times (\\{0,1\\} \\times \\{0,1\\} \\times (0,1)) \\to \\{0,1,2,3\\}\\), we define a variance function \\[ V_{lj}(\\rho, \\beta, X_{i\\cdot}) = \\rho^{\\vert l-j\\vert}(\\mu(\\beta, X_{il}) + 1)(\\mu(\\beta, X_{ij}) + 1). \\] Let \\(M(\\beta, X_{i\\cdot}) = \\begin{pmatrix} \\mu(\\beta, X_{i1}) &amp; \\cdots &amp; \\mu(\\beta, X_{i26}) \\end{pmatrix}^\\prime\\). Now, an estimator of \\(\\beta\\) can be found by solving the equation \\[ 0 = \\sum_{i=1}^N J(\\beta, X_{i\\cdot})^\\prime V^{-1}(\\rho, \\beta, X_{i\\cdot})(Y_i - M(\\beta, X_{i\\cdot})). \\] 1.5.3 Non-linear Equations 1.5.3.1 Problem Formulation The problems we will be looking at here can be formulated as \\(0 = F(x)\\), where \\(F:\\mathbb{R}^m \\to \\mathbb{R}^n\\), or (if the righthand side is “nice enough”) \\(x = G(x)\\), where \\(G: \\mathbb{R}^m \\to \\mathbb{R}^m\\). 1.5.3.2 Uniqueness In order to be able to discuss uniqueness of solutions to non-linear equations, we need the implicit function theorem. Theorem 9 (Linear Case of the Implicit Function Theorme) Let \\(M \\in \\mathbb{R}^{n\\times m}\\), where \\(m&gt;n\\) and \\(\\text{rank}(H) = n\\). Without loss of generality, let \\(M = \\begin{bmatrix} A &amp; B \\end{bmatrix}\\), where \\(A \\in \\mathbb{R}^{n\\times n}\\) is invertible. Then, for all \\(x \\in \\mathbb{R}^{m-n}\\), \\(z = \\begin{bmatrix} -A^{-1}Bx &amp; x \\end{bmatrix}^\\prime\\) satisfies \\(Mz = 0\\). Theorem 10 (Implicit Function Theorem) Let \\(F:\\mathbb{R}^m \\to \\mathbb{R}^n\\) be continuously differentiable. Then \\(F\\) has a Jacobian, \\(J:\\mathbb{R}^m \\to \\mathbb{R}^{n\\times m}\\). Suppose \\(z^*\\) satisfies \\(F(z^*) = 0\\). Without loss of generality, suppose \\(J(z^*) = \\begin{bmatrix} J_1(z^*) &amp; J_2(z^*) \\end{bmatrix}\\), where \\(J_1(z^*) \\in \\mathbb{R}^{n\\times n}\\) is invertible. Then there exists an open neighborhood \\(U \\subseteq \\mathbb{R}^{n\\times n}\\), and a continuous function \\(g: U \\to \\mathbb{R}^n\\) such that for all \\(x \\in U\\), \\(z= \\begin{bmatrix} g(x) &amp; x \\end{bmatrix}^\\prime\\) satisfies that \\(F(z) = 0\\). "],
["homework-assignments.html", "Chapter 2 Homework Assignments ", " Chapter 2 Homework Assignments "],
["homework-1.html", "2.1 Homework 1", " 2.1 Homework 1 Exercise 2 Can all nonnegative real numbers be represented in such a manner (i.e. as a fp number) for an arbitrary base \\(\\beta \\in \\{2,3,...\\}\\)? Solution. No. For any given \\(\\beta\\) and a largest exponent \\(e_{max}\\), any decimal larger than \\(\\beta\\cdot\\beta^{e_{max}}\\) is larger than the largest number possibly representated. Exercise 3 Suppose \\(e = -1\\), what are the range of numbers that can be represented for an arbitrary base \\(\\beta \\in \\{2,3,...\\}\\)? Solution. The smallest number that can be represented for an arbitrary base must be \\((0+0\\cdot \\beta^{-1} + ... + 0\\cdot\\beta^{-(p-1)})\\cdot\\beta^{-1}\\). Since \\(0 \\leq d_i &lt; \\beta, \\forall i\\), the largest value must be attained when \\(d_i = \\beta - 1\\) for all \\(i\\). I.e. the largest value must be \\[\\begin{align*} MAX &amp;= (\\beta-1 + (\\beta-1)\\beta^{-1} + ... + (\\beta-1)\\beta^{-(p-1)})\\cdot\\beta^{-1}\\\\ &amp;= (1+\\beta^{-1}+...+\\beta^{-(p-1)})(\\beta-1)\\cdot\\beta^{-1} \\\\ &amp;= (1+\\beta^{-1}+...+\\beta^{-(p-1)})\\cdot(1-\\beta^{-1}) \\\\ &amp;= (1+\\beta^{-1}+...+\\beta^{-(p-1)})\\cdot(1-\\beta^{-1}) \\end{align*}\\] Exercise 4 Characterize the numbers that have a unique representation in a base \\(\\beta \\in \\{2,3,...\\}\\). Solution. Let \\[f = \\left(d_1\\cdot\\beta^{-1} + \\ldots + d_{p-1}\\cdot\\beta^{-(p-1)}\\right)\\cdot\\beta^{e},\\] i.e. \\(f\\) is not normarlized. Then, \\[ f = \\left(d_1+d_2\\beta^{-1} + \\ldots + d_{p-1}\\cdot\\beta^{-p} + 0\\cdot \\beta^{-(p-1)}\\right)\\cdot\\beta^{e-1}. \\] So, non-normalized fp numbers are NOT unique. Now, let \\(f\\) be a normalized fp number. I.e. \\[ f = \\left(d_0 + d_1\\cdot\\beta^{-1} + \\ldots + d_{p-1}\\cdot\\beta^{-(p-1)} \\right)\\cdot\\beta^e, \\] where \\(d_0 \\neq 0\\). If we let \\(e_n &lt; e\\), then \\[ f &gt; \\left(d_0 + d_1\\cdot\\beta^{-1} + \\ldots + d_{p-1}\\cdot\\beta^{-(p-1)} \\right)\\cdot\\beta^{e_n}, \\] and if \\(e_n &gt; e\\), then \\[ f &lt; \\left(d_0 + d_1\\cdot\\beta^{-1} + \\ldots + d_{p-1}\\cdot\\beta^{-(p-1)} \\right)\\cdot\\beta^{e_n} \\] If we let \\[d_i^\\prime \\neq d_i\\] for some number of \\(i\\)’s, then \\[ f \\neq \\left(d_0^\\prime + d_1\\prime\\cdot\\beta^{-1} + \\ldots + d_{p-1}\\prime\\cdot\\beta^{-(p-1)} \\right)\\cdot\\beta^e. \\] Hence, normalized FP numbers are unique. Exercise 5 Write a function that takes a decimal number, base, and precision, and returns the closest normalized FP representation. I.e. a vector of digits and the exponent. Solution. The function provided in class is actually the solution (?). This is guarenteed to give a normalized FP representation. Using this algorithm gives \\(d_0 = \\lfloor \\frac{N}{\\beta^{\\lfloor log_{\\beta}\\left(N\\right) \\rfloor}}\\rfloor\\). It holds that \\(\\lfloor log_{\\beta}\\left(N\\right) \\rfloor \\le log_\\beta\\left(N\\right)\\), which implies that \\(\\beta^{\\lfloor log_{\\beta}\\left(N\\right) \\rfloor} \\le \\beta^{log_\\beta\\left(N\\right)} = N\\) (remember, \\(\\beta \\geq 2\\)). Hence, \\(d_0 &gt; 0\\). get_normalized_FP = function(number::Float64, base::Int64, prec::Int64) #number = 4; base = float(10); prec = 2 si=sign(number) base = float(base) e = floor(Int64,log(base,abs(number))) d = zeros(Int64,prec) num = abs(number)/(base^e) for j = 1:prec d[j] = floor(Int64,num) num = (num - d[j])*base end return &quot;The sign is $si, the exponent is $e, and the vector with d is $d&quot; end ## #5 (generic function with 1 method) Exercise 6 List all normalized fp numbers that can be representated given base, precision, \\(e_{min}\\), and \\(e_{max}\\). all_normalized_fp = function(base::Int64, prec::Int64, emin::Int64, emax::Int64) ## Number of possible values for each e: N = (base-1)*base^(prec-1)#*(emax-emin+1) out=zeros(Int64, N, prec, emax-emin+1) es = emin:emax for e=1:length(es) for b0=1:(base-1) for i=1:(base^(prec-1)) out[(b0-1)*(base^(prec-1))+i,1,e] = b0 for j=1:(prec-1) out[(b0-1)*(base^(prec-1))+i,prec-j+1,e] = floor((i-1)/base^(j-1))%base end end end end return(out) end ## #7 (generic function with 1 method) "],
["homework-2.html", "2.2 Homework 2", " 2.2 Homework 2 Exercise 7 Lookup the 64 bit standard to find allowed exponents. Solution. According to Wikipedia, the allowed exponents for the 64 bit standard are \\(-1022,\\ldots, 1023\\). Exercise 8 What is the smallest non-normalized positive value for the 64 bit standard? Solution. The smallest non-normalized positive value is \\[ \\left(0 + 0\\cdot 2^{-1} + \\ldots + 0\\cdot 2^{-51} + 1\\cdot 2^{-52} \\right )\\cdot 2^{-1022} = 2^{-1074} \\approx 4.94\\cdot 10^{-324} \\] Exercise 9 What is the smallest normalized positive value? Solution. The smallest normalized positive value is \\[ \\left(1 + 0\\cdot 2^{-1} + \\ldots + 0\\cdot 2^{-52} \\right )\\cdot 2^{-1022} = 2^{-1022} \\approx 2.23 \\cdot 10^{-308} \\] Exercise 10 What is the largest normalized positive value? Solution. The largest normalized finite value is \\[ \\left(1 + 1\\cdot 2^{-1} + \\ldots + 1\\cdot 2^{-52} \\right )\\cdot 2^{1023} \\approx 1.80\\cdot 10^{308}. \\] Exercise 11 Is there a general formula for determining the largest positive value for a given base \\(\\beta\\), precision \\(p\\), and largest exponent \\(e_{max}\\)? Solution. The largest positive, finite value is \\[ \\left(\\sum_{i=0}^{p-1} (\\beta-1)\\beta^{-i}\\right)\\cdot \\beta^{e_{max}} = ... = \\frac{\\beta^p - 1}{\\beta^{p-1}}\\beta^{e_max}. \\] Exercise 12 Verify the smallest non-normalized, positive number that can be represented. Solution. See the Julia chunk below. nextfloat(Float64(0)) == 2^(-1074) ## true Exercise 13 Verify the smallest normalized, positive number that can be represented. nextfloat(Float64(0))*2^(52) ## 2.2250738585072014e-308 Exercise 14 Verify the largest, finite number that can be represented. prevfloat(Float64(Inf)) ## 1.7976931348623157e308 Exercise 15 Proof lemma (1). Solution. Let \\(z = (d_0 + d_1 \\beta^{-1} + \\dots)\\beta^e\\) be a number. Let \\(fl(z) = (d_0^\\prime + d_1^\\prime\\beta^{-1} + \\dots + d_{p-1}^\\prime\\beta^{-(p-1)})\\beta^e\\) be its fp representation with precision \\(p\\). We know that \\((d_0 + d_1 \\beta^{-1} + \\dots + d_{p-1}\\beta^{-(p-1)})\\beta^e \\le z \\le (d_0 + d_1 \\beta^{-1} + \\dots + (d_{p-1}+1)\\beta^{-(p-1)})\\beta^e\\). We know that \\(fl(z)\\) is equal to the one of these two bounds that is closest to \\(z\\). Hence, \\(|fl(z) - z|\\) must be at most half the distance between these two. Subtract the upper bound from the lower bound, and you get \\(\\beta^{-(p-1)+e}\\), i.e. \\[ |fl(z) - z| \\le \\frac{\\beta^{e-p+1}}{2}. \\] Exercise 16 What happens with lemmas (bounds of absolute and relative error) if we consider negative numbers? Solution. They still hold. Let \\(z^\\prime = -z\\). Then \\(fl(z^\\prime) = -fl(z)\\). Hence, \\[ \\left| fl(z^\\prime) - z^\\prime \\right | = \\left | -fl(z) + z \\right | = \\left | fl(z) - z \\right |, \\] hence the bounds still hold. Exercise 17 Show that \\(\\left|\\left| A \\right|\\right|_1 =\\) max of the \\(l^1\\) norms of the columns of \\(A\\). Solution. By definition, \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_1 = \\max_{x\\neq 0}\\frac{\\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert_1}{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_1} = \\max_{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_1 = 1} \\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert_1\\). Let \\(A \\in \\mathbb{R}^{m\\times n}\\) and \\(x \\in \\mathbb{R}^n\\) s.t. \\(\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_1 = 1\\). Recall that \\[Ax = \\sum_{j=1}^n x_j A_{*,j},\\] where \\(A_{*,j}\\) is the \\(j^\\prime\\)th column of \\(A\\). So \\[ \\begin{aligned} \\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert_1 &amp;= \\sum_{i=1}^m \\left | \\sum_{j=1}^n x_j A_{i,j} \\right| \\\\ &amp;\\leq \\sum_{i=1}^m \\sum_{j=1}^n | x_j | \\cdot | A_{i,j} | \\\\ &amp;= \\sum_{j=1}^n | x_j | \\left\\{\\sum_{i=1}^m | A_{i,j} | \\right \\} \\\\ &amp;= \\sum_{j=1}^n | x_j | \\left \\vert \\left \\vert A_{*,j} \\right \\vert \\right \\vert \\\\ &amp;\\leq \\sum_{j=1}^n | x_j | \\max_{j \\in \\{1,\\ldots, n\\}} \\left \\vert \\left \\vert A_{*,j} \\right \\vert \\right \\vert \\\\ &amp;= \\max_{j \\in \\{1,\\ldots, n\\}} \\left \\vert \\left \\vert A_{*,j} \\right \\vert \\right \\vert \\end{aligned} \\] I.e. \\(\\max_{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_1 = 1} \\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert_1 \\le \\max_{j \\in \\{1,\\ldots, n\\}} \\left \\vert \\left \\vert A_{*,j} \\right \\vert \\right \\vert\\). Now, let \\(1_i = (x_j)_{j=1}^n\\) be defined as \\[x_j = \\left\\{ \\begin{array}{rl} 1, &amp; \\text{ if } j = i \\\\ 0, &amp; \\text{ otherwise}\\end{array} \\right .\\] Then \\(\\left \\vert \\left \\vert 1_i \\right \\vert \\right \\vert_1 = 1\\). Since \\(\\max_{j \\in \\{1, \\ldots, n\\}}\\left \\vert \\left \\vert A_{*,j} \\right \\vert \\right \\vert = \\left \\vert \\left \\vert A\\cdot \\mathbf{1}_i \\right \\vert \\right \\vert\\) for \\(i = \\text{argmax}\\left \\vert \\left \\vert A_{*,j} \\right \\vert \\right \\vert_1\\), we have that \\(\\max_{j \\in \\{1,\\ldots,n\\}} \\left \\vert \\left \\vert A_{*,j} \\right \\vert \\right \\vert_1 \\le \\max_{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_1 = 1} \\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert_1\\). Exercise 18 Let \\(A \\in \\mathbb{R}^{n \\times m}\\). Show that \\(\\left|\\left| A \\right|\\right|_\\infty =\\) max of the \\(l^1\\) norms of the rows of \\(A\\). Solution. Let \\(j \\in \\{1, \\ldots, n\\}\\) such that \\(|a_{j1}| + \\dots + |a_{jm}| \\ge |a_{i1}| + \\dots + |a_{im}|\\) for all \\(i \\in \\{1, \\ldots , n\\}\\), and let \\(\\tilde{x} \\in \\mathbb{R}^{m}\\) such that \\(\\tilde{x}_i = \\text{sign}(a_{ji})\\). Note that for all \\(x \\in \\mathbb{R}^{m}\\) with \\(\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_\\infty = 1\\) it holds that \\(|x_i| \\le 1\\). So, \\[\\begin{aligned} |a_{i1}x_1 + \\dots + a_{im}x_m| &amp;\\le |a_{i1}| + \\dots + |a_{im}| \\\\ &amp;\\le |a_{j1}| + \\dots + |a_{jm}| \\\\ &amp;= |a_{j1} \\tilde{x}_1 + \\dots + a_{jm}\\tilde{x}_1|, \\end{aligned}\\] I.e. for any \\(x \\in \\mathbb{R}^m\\) with \\(\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_\\infty = 1\\), \\[\\begin{aligned} |a_{j1} \\tilde{x}_1 + \\dots + a_{jm}\\tilde{x}_1| &amp;\\ge \\max_{1 \\le i \\le n}\\{|a_{i1} x_1 + \\dots + a_{im}x_m|\\} \\\\ &amp; = \\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert_\\infty. \\end{aligned}\\] Since \\(\\left \\vert \\left \\vert \\tilde{x} \\right \\vert \\right \\vert_\\infty = 1\\), we have that \\[\\begin{aligned} \\max_{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert = 1} \\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert_\\infty &amp;= |a_{j1} \\tilde{x}_1 + \\dots + a_{jm}\\tilde{x}_1| \\\\ &amp;= |a_{j1}| + \\dots + |a_{jm}| \\\\ &amp;= \\max_{1\\le j \\le n} \\sum_{i=1}^n |a_{ji}|, \\end{aligned}\\] which is exactly the maximum over the \\(\\mathcal{l}^1\\)-norms of the rows in \\(A\\). Exercise 19 Assume the Fundamental Axiom. Show the following: \\[\\left|\\left| fl(A)-A \\right|\\right|_p \\leq u \\left|\\left|A\\right|\\right|_p\\] Solution. \\[\\begin{align*} \\left | \\left | fl(A) - A \\right | \\right |_p &amp;= \\left|\\left| \\left [ fl(a_{ij}) - a_{ij} \\right ] \\right|\\right|_p \\\\ &amp;\\leq \\left|\\left| \\left[u\\cdot a_{ij}\\right]\\right|\\right|_p \\\\ &amp;= \\left|\\left| u\\cdot A\\right|\\right|_p = u\\left|\\left|A\\right|\\right| \\end{align*}\\] "],
["homework-3.html", "2.3 Homework 3", " 2.3 Homework 3 Exercise 20 Prove lemma 5. Solution. Recall that a matrix \\(A\\) is invertible if and only if \\(Ax = 0\\) implies that \\(x = 0\\). So to check that \\(I-F\\) is invertible, we check this: \\[ (I-F)x = x-Fx = 0 \\Rightarrow x = Fx \\Rightarrow \\left \\vert \\left \\vert x \\right \\vert \\right \\vert_p \\leq \\left \\vert \\left \\vert F \\right \\vert \\right \\vert_p \\left \\vert \\left \\vert x \\right \\vert \\right \\vert_p. \\] Since \\(\\left \\vert \\left \\vert F \\right \\vert \\right \\vert_p &lt; 1\\) by assumption, the only solution to the inequality above is \\(x = 0\\). So, \\(I-F\\) is invertible. Note that \\(\\sum_{k=0}^N F^k (I - F) = I - F^{N+1}\\). Since \\(\\left \\vert \\left \\vert F^k \\right \\vert \\right \\vert_p &lt; \\left \\vert \\left \\vert F \\right \\vert \\right \\vert_p^k &lt; 1\\), we know that \\(F^N \\rightarrow 0\\) as \\(N \\rightarrow \\infty\\). So, \\(\\sum_{k=0}^\\infty F^k (I-F) = I\\), and using that \\(I-F\\) is invertible, we get \\((I-F)^{-1} = \\sum_{k=0}^\\infty F^k\\). Finally, \\[\\left \\vert \\left \\vert (I-F)^{-1} \\right \\vert \\right \\vert_p \\leq \\sum_{k=1}^\\infty \\left \\vert \\left \\vert F \\right \\vert \\right \\vert_p^k = \\frac{1}{1- \\left \\vert \\left \\vert F \\right \\vert \\right \\vert_p}\\]. Exercise 21 Consider Theorem and Lemmas under “Square Linear Systems” (1.1.4). What happens if we use \\(l^{1}\\)-norm instead? Solution. Since we never use any properties of the infinity norm to prove these theorems and lemmas, we could replace it with the \\(\\mathcal{l}^1\\)-norm. Exercise 22 Generate examples that show the bound in theorem 1 is too conservative. using LinearAlgebra p = 53 # precision for float-64 ## 53 u = 2.0^(-p+1) ## 2.220446049250313e-16 A = [ 0. 0 0.000001; 200000 0 0; 0 1 20000; ] ## 3×3 Array{Float64,2}: ## 0.0 0.0 1.0e-6 ## 200000.0 0.0 0.0 ## 0.0 1.0 20000.0 x = [1.; 1; 1;] ## 3-element Array{Float64,1}: ## 1.0 ## 1.0 ## 1.0 b = A*x ## 3-element Array{Float64,1}: ## 1.0e-6 ## 200000.0 ## 20001.0 kappa = norm(A, Inf) * norm(inv(A), Inf) ## 4.0e15 bound = 2.0*u*kappa/(1.0 - u*kappa) ## 15.885635265004744 println(&quot;Diff: $(norm(x - inv(A)*b, Inf)/norm(x,Inf))&quot;) println(&quot;Bound: $(bound)&quot;) Exercise 23 Generate examples that show the bound is nearly achieved using LinearAlgebra A = [ 0. 0 1.; 1 0 0; 0 1 0; ] ## 3×3 Array{Float64,2}: ## 0.0 0.0 1.0 ## 1.0 0.0 0.0 ## 0.0 1.0 0.0 x = [1.; 1; 1;] ## 3-element Array{Float64,1}: ## 1.0 ## 1.0 ## 1.0 b = A*x ## 3-element Array{Float64,1}: ## 1.0 ## 1.0 ## 1.0 kappa = norm(A, Inf) * norm(inv(A), Inf) ## 1.0 bound = 2.0*2.0^(-p+1)*kappa/(1.0 - 2.0^(-p+1)*kappa) ## 4.440892098500627e-16 println(&quot;Diff: $(norm(x - inv(A)*b, Inf)/norm(x,Inf))&quot;) println(&quot;Bound: $(bound)&quot;) Exercise 24 For motivating problems 1-5, when is \\(x\\) unique? Solution. - Motivating Problem 1: * when \\(A\\) is invertible - Motivating Problem 2: Always. * Since \\(F(y) = \\left \\vert \\left \\vert Ay - b \\right \\vert \\right \\vert_2 = (Ay-b)^\\prime (Ay-b)\\) is convex (twice differentiated is positive definite because \\(\\nabla^2 f = A^\\prime A\\) and \\(A\\) has full rank). * (since objective function is \\(A^\\prime A\\), and it is positive definite (because A is full rank), then the function is convex, and you always have a unique solution) - Motivating Problem 3: Always. - Motivating Problem 4: Always. * We know how to characterize all \\(z\\) that satisfy objective: \\(Q\\begin{bmatrix} R &amp; S \\\\ 0 &amp; 0 \\end{bmatrix} \\Pi^\\prime = A\\). Motivating Problem 5: always Exercise 25 For motivating problem 5, what happens if \\(p\\geq m\\)? Explore the case where \\(m &gt;&gt; n\\). Solution. * For m &gt;&gt; n, you have an undertermined system with more unknowns than equations. Since the null space of A has a dimension larger than zero, for any particular solution xp for the system, xp+h with \\(h \\in \\text{null}(A)\\) is also a solution, and there are infinitely many choices for h. The constraint system might help narrow down the solutions from the null space. * For p &gt;= m: if C becomes inconsistent, we cannot narrow down the solutions in the null space. If C is consistent and rank(C)=m, then we can pick a unique solution. If the rank of C is less than m, then C constrains some of the possible solutions for y. Exercise 26 What do you get if you multiply a matrix by a permutation matrix from the left? From the right? A permutation with itself? Solution. From the left: permute rows. From the right: permute columns. Permutation squared gives you the identity. Exercise 27 Suppose \\(R \\in \\mathbb{R}^{m\\times m}\\) is an upper triangular matrix with \\(R_{ii} \\neq 0\\) for all \\(i = 1,\\ldots, n\\). Is \\(R\\) invertible? Solution. Since \\(R\\) is an upper triangular matrix, \\(\\det(R) = \\prod_{i=1}^{m} R_{ii} &gt; 0\\). Hence, \\(R\\) is invertible. Exercise 28 Assume \\(R\\) is an invertible upper triangular matrix. Implement a solution to invert \\(R\\). Solution. First, we will show that the inverse of \\(R\\) is also an upper triangular matrix. So, let \\(B = R^{-1}\\). The \\(RB = I\\). We will show this using induction. Let \\(i = n - 0, j &lt; i\\). Then, since \\(r_{ij} = 0\\) for all \\(i &gt; j\\), \\[\\begin{aligned} 0 &amp;= \\sum_{k=1}^n r_{n,k}b_{k,j} \\\\ &amp;= r_{n,n}b_{n,j}. \\end{aligned}\\] Since \\(R\\) is invertible, \\(r_{n,n} \\neq 0\\), hence \\(b_{n,j} = 0\\) for all \\(j &lt; n\\). Now assume that \\(b_{i,j} = 0\\) for all \\(i=n-0,n-1, \\ldots, n-m, j &lt; i\\). We then want to show it holds for \\(i = n - (m+1) = n-m-1\\). Let \\(j &lt; n-(m+1)\\). Then \\[\\begin{aligned} 0 &amp;= \\sum_{k=1}^n r_{n-(m+1), k} b_{k, j} \\\\ &amp;= \\sum_{k=n-(m+1)}^{n} r_{n-(m+1), k} b_{k, j} \\\\ &amp;= r_{n-(m+1), n-(m+1)} b_{n-(m+1),j}, \\end{aligned}\\] where the first equality is due to the fact that \\(r_{ij} = 0\\) for all \\(i &gt; j\\), and the last equality holds since \\(b_{k,j} = 0\\) for all \\(j &lt; k\\) when \\(k \\ge n-m\\) (per the induction hypothesis). Since \\(r_{ii} \\neq 0\\) for all \\(i\\), \\(b_{n-(m+1),j} = 0\\). So, \\(b_{ij} = 0\\) for all \\(i &gt; j\\). Now, lets look at the case where \\(i=j\\), i.e. diagonal elements of the inverse matrix. Then \\[\\begin{aligned} 1 = \\sum_{k=1}^n r_{ik} b_{ki} = \\sum_{k=i}^{n} r_{ik} b_{ki} = r_{ii}b_{ii}. \\end{aligned}\\] The second equality holds since \\(r_{ik} = 0\\) for all \\(i &gt; k\\), the last equality holds because \\(b_{ki} = 0\\) for all \\(k &gt; i\\). We see that \\(b_{ii} = r_{ii}^{-1}\\). Finally, consider the case where \\(i &lt; j\\). Then, using that \\(r_{ik} = 0\\) for all \\(i&gt;k\\) and \\(b_{kj} = 0\\) for all \\(k&gt;j\\), we see that \\[ 0 = \\sum_{k=1}^n r_{ik}b_{kj} = \\sum_{k=i}^j r_{ik} b_{kj}, \\] which implies \\[ b_{ij} = \\frac{-\\sum_{k=i+1}^j r_{ik} b_{kj}}{a_{ii}}. \\] So, in other words, given \\(i,j\\), we can find \\(b_{ij}\\) if we know \\(b_{kj}\\) for all \\(k&gt;i\\) (i.e. all entries in the same column below the entry we are considering). Since we already know all entries below and on the diagonal, this is true for all columns. Hence, we can construct \\(B\\) this way. See the Julia chunk below for an implementation of this. function invertUpperTri(A) ## Get dimensions of A n, m = size(A) ## Setup empty array to hold result B = zeros(n,m) ## Fill out diagonal with inverse diagonal from A for k = 1:n B[k, k] = 1/A[k,k] end ## Starting in the lower right corner, fill out the rest of the matrix. for i = (n-1):-1:1 for j = (i+1):n B[i,j] = -sum(A[i,(i+1):j].*B[(i+1):j,j])/A[i,i] end end return(B) end ## invertUpperTri (generic function with 1 method) ## Create an upper triangular matrix to check A = rand(4,4); A[2:4,1] = [0 0 0]; A[3:4,2] = [0 0]; A[4,3] = 0; ## Check function B = invertUpperTri(A); B - inv(A) # should be 0 matrix ## 4×4 Array{Float64,2}: ## 0.0 0.0 -2.84217e-14 2.84217e-14 ## 0.0 0.0 8.88178e-16 0.0 ## 0.0 0.0 0.0 -1.77636e-15 ## 0.0 0.0 0.0 0.0 A*B # should be identity ## 4×4 Array{Float64,2}: ## 1.0 -4.10934e-17 -3.33957e-16 -6.49396e-17 ## 0.0 1.0 3.45654e-16 -3.96299e-16 ## 0.0 0.0 1.0 -1.84862e-16 ## 0.0 0.0 0.0 1.0 "],
["homework-4.html", "2.4 Homework 4", " 2.4 Homework 4 Exercise 29 In the solution to 2, why do \\(0\\) rows on the left-hand side of (1) correspond to \\(0\\) entries of the \\(c\\) vector on the right-hand side. Solution. By definition of matrix multiplication, if the \\(i\\)th row is a zero row of \\(A\\), then \\(c_{ij} = [AB]_{ij} = \\sum{j=1}^n a_{ij} b_{ij} = 0\\), no matter what \\(B\\) is. Exercise 30 Let \\(Q\\) be an orthogonal matrix. Show that \\(\\left \\vert \\left \\vert Qx \\right \\vert \\right \\vert_2 = \\left \\vert \\left \\vert x \\right \\vert \\right \\vert_2\\). Solution. Since \\(\\left \\vert \\left \\vert x \\right \\vert \\right \\vert_2^2 = x^\\prime x\\) and \\(Q^\\prime Q = 1\\) (\\(Q\\) is orthogonal), \\[ \\left \\vert \\left \\vert Qx \\right \\vert \\right \\vert^2_2 = (Qx)^\\prime Qx = x^\\prime Q^\\prime Q x = x^\\prime x = \\left \\vert \\left \\vert x \\right \\vert \\right \\vert^2_2. \\] Exercise 31 Let f be a vector-valued function over \\(\\mathbb{R}^d\\). When is \\[\\text{argmin}_x \\left \\vert \\left \\vert f(x) \\right \\vert \\right \\vert_2 = \\text{argmin}_x \\left \\vert \\left \\vert f(x) \\right \\vert \\right \\vert_2^2.\\] Solution. Most of the time…? Exercise 32 Prove that \\(P^T P + I\\) from solution to example 4 is invertible. Solution. For all non-zero \\(x\\), \\[ x^\\prime(P^TP + I)x = x^\\prime P^\\prime P x + x^\\prime x = \\left \\vert \\left \\vert Px \\right \\vert \\right \\vert_2 + \\left \\vert \\left \\vert x \\right \\vert \\right \\vert_2 &gt; 0. \\] This means \\(P^\\prime P + I\\) is positive definite, which in turne implies that it is invertible. Exercise 33 Write out the solution to example 6. Also consider the case where \\(p \\ge m\\). Solution. First we do QR decomposition on \\(C^\\prime\\): \\[\\begin{aligned} C^\\prime = Q\\begin{bmatrix} R \\\\ 0 \\end{bmatrix} \\end{aligned}\\] Then we have: \\[\\begin{aligned} AQ &amp;= \\begin{bmatrix} A_1 &amp; A_2 \\end{bmatrix} \\\\ Q^\\prime y &amp;= \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} \\\\ \\end{aligned}\\] And we can update the objective: \\[\\begin{aligned} O &amp;= \\min \\|Ay-b\\|_2 : Cy = d\\\\ &amp;= \\min \\|AQQ^\\prime y -b\\|_2 : Cy = d\\\\ &amp;= \\min \\left\\| \\begin{bmatrix} A_1 &amp; A_2 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} - b \\right\\| _2 : Cy = d\\\\ &amp;= \\min \\left\\| A_1y_1 + A_2 y_2 - b \\right\\|_2 : Cy = d\\\\ \\end{aligned}\\] We can also update the constraint: \\[\\begin{aligned} O &amp;= \\min \\left\\| A_1y_1 + A_2 y_2 - b \\right\\|_2 : \\begin{bmatrix} R^\\prime &amp; 0 \\end{bmatrix} Q^\\prime y = d \\\\ &amp;= \\min \\left\\| A_1y_1 + A_2 y_2 - b \\right\\|_2 : \\begin{bmatrix} R^\\prime &amp; 0 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = d \\\\ &amp;= \\min \\left\\| A_1y_1 + A_2 y_2 - b \\right\\|_2 : R^\\prime y_1 = d \\\\ &amp;= \\min \\left\\| A_1y_1 + A_2 y_2 - b \\right\\|_2 : y_1 = (R^\\prime)^{-1}d \\\\ &amp;= \\min \\left\\| A_1(R^\\prime)^{-1}d + A_2 y_2 - b \\right\\|_2\\\\ &amp;= \\min \\left\\| A_2 y_2 - (b - A_1(R^\\prime)^{-1}d)\\right\\|_2\\\\ \\end{aligned}\\] So \\(y_1\\) can be calculated using a consistent linear system solver, and now the objective is the same as that of a least squares solver, which can be used to calculate \\(y_2\\). We can finally recover \\(y\\): \\[\\begin{aligned} y &amp;= Q \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} \\\\ \\end{aligned}\\] Exercise 34 For all motivating problems, implement solutions. For the following, assume \\(A \\in \\mathbb{R}^{n\\times m}\\) with \\(\\text{rank}(A) = m\\), and \\(b \\in \\mathbb{R}^n\\). Let \\(C = \\begin{bmatrix} A &amp; b \\end{bmatrix}\\). Exercise 35 What does the last column of \\(R\\) (from the QR decomposition of \\(C\\)) represent? Solution. Generally, \\(Q^\\prime b\\), which is the projection of b onto the column space of A. If \\(b\\) is in the column space of \\(A\\), then it is specifically \\(Rx\\) where \\(x\\) is the solution to \\(Ax = b\\). Exercise 36 What does the last entry of last column of \\(R\\) (from the QR decomposition of \\(C\\)) represent? Solution. The square of the last entry of the last column of \\(R\\) is the sum of squares of the residuals. Exercise 37 How can this be used in computation? Exercise 38 We can use it for incremental QR for large datasets. "],
["homework-5.html", "2.5 Homework 5", " 2.5 Homework 5 Exercise 39 Implement the Gram-Schmidt procedure for matrices \\(A \\in \\mathbb{R}^{n \\times m}\\) assuming \\(A\\) has full column rank. Create examples to show that the function works (well enough). Solution. See here. Exercise 40 Find examples where Gram-Schmidt fails, i.e. where either \\(Q R \\neq A\\) or \\(Q^TQ \\neq I\\). Solution. Let \\(A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 10^{-8} &amp; 0 &amp; 0 \\\\ 0 &amp; 10^{-8} &amp; 0 \\end{bmatrix}\\). Then \\(Q^\\prime Q \\neq I\\). Exercise 41 Look up the modified Gram-Schmidt Procedure and implement it (again assuming \\(A\\) has full column rank). Solution. See here. Exercise 42 (Pivoting (OPTIONAL)) References: Businger, Galub: Linear Least Squares by Householder Transformation (1965) Engler: The Behavior of QR-factorization algorithm with column pivoting (1997) Implement modified Gram-Schmidt with column pivoting. Find example where the modified Gram-Schmidt fails, but the modified Gram-Schmidt with column pivoting does not. Exercise 43 Show that Householder reflections are orthogonal matrices. Solution. Show that \\(H^\\prime H = I\\). By definition of a Householder matrix (7), \\(H = I - 2vv^\\prime\\) for a \\(v\\) with \\(\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = 1\\). Note that \\(\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2 = \\sqrt{v^\\prime \\cdot v}\\). So, \\[\\begin{aligned} H^\\prime H &amp;= (I - 2vv^\\prime)^\\prime (I - 2vv^\\prime) \\\\ &amp;= (I^\\prime - 2(vv^\\prime)^\\prime)(I - 2 vv^\\prime)\\\\ &amp;= (I - 2vv^\\prime)(I - 2 vv^\\prime) \\\\ &amp;= I - 2vv^\\prime - 2vv^\\prime + 4 vv^\\prime v v^\\prime \\\\ &amp;= I - 2vv^\\prime - 2vv^\\prime + 4 v I v^\\prime \\\\ &amp;= I. \\end{aligned}\\] So by definition (5), \\(H\\) is an orthogonal matrix. Exercise 44 Show that if \\(H_1, \\dots, H_r\\) are Householder reflections, then \\(H_r \\cdots H_1\\) (i.e. the product) is orthogonal. Solution. \\[(H_r \\cdots H_1)^\\prime (H_r \\cdots H_1) = H_1^\\prime \\cdots H_r^\\prime H_r \\cdots H_1 = I,\\] since all \\(H_i\\) are Householder reflections, which implies they are orthogonal, which implies \\(H_i^\\prime H_i = I\\). Exercise 45 Show that a Givens rotation is an orthonormal matrix when \\(\\sigma^2 + \\lambda^2 = 1\\). Solution. Let \\(G^{(a,b)}\\) be a Givens rotation. I.e. the elements \\(g_{i,j}\\) are \\[ g_{i,j} = \\left\\{ \\begin{array}{rl} 1, &amp; i = j \\notin \\{a,b\\} \\\\ \\lambda, &amp; i = j \\in \\{a,b\\} \\\\ \\sigma, &amp; i = a, j = b \\\\ -\\sigma, &amp; i = b, j = a \\\\ 0, &amp; \\text{ otherwise} \\end{array} \\right. \\] Transposing this matrix gives us a new matrix \\(K\\) where \\[ k_{i,j} = \\left\\{ \\begin{array}{rl} 1, &amp; i = j \\notin \\{a,b\\} \\\\ \\lambda, &amp; i = j \\in \\{a,b\\} \\\\ -\\sigma, &amp; i = a, j = b \\\\ \\sigma, &amp; i = b, j = a \\\\ 0, &amp; \\text{ otherwise} \\end{array} \\right. \\] Now, let \\(L = K \\cdot G\\). Then the elements of \\(L\\) are \\(l_{i,j} = \\sum_{s=1}^n k_{i,s} g_{s,j}\\). If \\(i \\neq a,\\ j \\neq b\\), and \\(i \\neq j\\), then \\(l_{i,j} = k_{i,i} g_{i,j} = 0\\) (since \\(g_{i,j} = 0\\)). If \\(i \\notin \\{a, b\\}\\), then \\(l_{i,i} = k_{i,i} g_{i,i} = 1\\) (since \\(i \\notin \\{a,b\\}\\)). If \\(j \\notin \\{a,b\\}\\), then \\(l_{a,j} = k_{a,b}g_{b,j} + k_{a,a}g_{a,j} = 0\\) (since \\(j \\notin \\{a,b\\}\\) implies \\(g_{b,j} = 0\\) and \\(g_{a,j} = 0\\)). Furthermore, \\(l_{a,b} = k_{a,b}g_{b,b} + k_{a,a}g_{a,b} = -\\sigma \\lambda + \\lambda \\sigma = 0\\), \\(l_{a,a} = k_{a,b} g_{b,a} + k_{a,a} g_{a,a} = (-\\sigma)(-\\sigma) + \\lambda \\lambda = \\sigma^2 + \\lambda^2\\), \\(l_{b,b} = k_{a,b} g_{b,a} + k_{a,a} g_{a,a} = (-\\sigma)(-\\sigma) + \\lambda \\lambda = \\sigma^2 + \\lambda^2\\). So, \\[ l_{i,j} = \\left\\{ \\begin{array}{rl} 1, &amp; i = j \\notin \\{a,b\\} \\\\ \\sigma^2 + \\lambda^2, &amp; i = j \\in \\{a,b\\} \\\\ 0, &amp; \\text{ otherwise} \\end{array} \\right. \\] Similar calculations can be performed for \\(G \\cdot K\\). So \\(G^{(a,b)}\\) is orthogonal if and only if \\(\\sigma^2 + \\lambda^2 = 1\\). "],
["homework-6.html", "2.6 Homework 6", " 2.6 Homework 6 Exercise 46 Determine the computational complexity of the QR decomposition using Gram-Schmidt Modified Gram-Schmidt Householder Givens rotations for any arbitrary, dense \\(n \\times m\\) matrix. (dense = don’t know how many entries are \\(0\\).) Solution. a) Gram-Schmidt requires \\(O(m^2n)\\) computations. b) Same as Gram-Schmidt. c) \\(O(m^2n)\\). d) \\(O(m^2n)\\). Exercise 47 Compare the computational complexity of Householder and Givens for a sparse matrix (i.e. a matrix where a substantial number of entries are \\(0\\)). Solution. Householder still have the same complexity even for sparse matrices, whereas for Givens we do not have to complete as many multiplications with Givens rotations. (Everytime we encounter a 0 in the lower triangular matrix that can be paired with a zero above it, we can pair them up, and skip the computation.) Exercise 48 Implement QR decomposition using Householder. Write it as a function that takes a matrix \\(A \\in \\mathbb{R}^{n\\times m}\\) with \\(\\text{rank}(A) = m\\) as its input, and gives back \\(Q\\) and \\(R\\). Solution. See here. Exercise 49 Implement QR decomposition using Givens. (As above.) Solution. See here Exercise 50 What happens in theorem 3 if \\(m &gt; n\\). Solution. If \\(m &gt; n\\), the diagonal matrix of the SVD of \\(A\\) will be of the form \\(\\begin{bmatrix} \\Sigma &amp; 0 \\end{bmatrix}\\), where \\(\\Sigma = \\begin{bmatrix} \\sigma_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_n \\end{bmatrix}\\). Exercise 51 If \\(u,v\\) are respectively left and right singular vectors corresponding to the singular value \\(\\sigma\\), show that \\(Av = \\sigma u\\), and \\(A&#39;u = \\sigma v\\). Solution. Using the SVD of \\(A\\), we see that \\(AV = U\\Sigma\\), which implies \\[ A \\begin{bmatrix} v_1 &amp; \\cdots &amp; v_m \\end{bmatrix} = \\begin{bmatrix} \\sigma_1 u_1 &amp; \\cdots &amp; \\sigma_m u_m \\end{bmatrix}. \\] where \\(v_1, \\dots, v_m\\) are the right singular vectors of \\(A\\), and \\(u_1, \\dots, u_m\\) are the left singular vectors of \\(A\\). So, \\(Av_i = \\sigma_i u_i\\) for all \\(i\\). Similarly, \\(A^\\prime = V \\Sigma U^\\prime\\) implies \\(A^\\prime U = V \\Sigma\\). As above, this gives us \\(A^\\prime u_i = \\sigma_i v_i\\). "],
["homework-7.html", "2.7 Homework 7", " 2.7 Homework 7 Exercise 52 Show that \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2 = \\sum_{i=1}^n\\sum_{j=1}^n a_{ij}^2\\) is orthogonally invariant, i.e. if \\(Q,P\\) are orthogonal matrices, then \\(\\left \\vert \\left \\vert QA \\right \\vert \\right \\vert_F^2 = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2\\) and \\(\\left \\vert \\left \\vert AP \\right \\vert \\right \\vert_F^2 = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2\\). Solution. Recall, that \\(\\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2 = \\text{trace}(A^\\prime A)\\). So, \\[ \\left \\vert \\left \\vert QA \\right \\vert \\right \\vert_F^2 = \\text{trace}(A^\\prime Q^\\prime Q A) = \\text{trace}(A^\\prime A) = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2, \\] and \\[ \\left \\vert \\left \\vert AP \\right \\vert \\right \\vert_F^2 = \\text{trace}(P^\\prime A^\\prime A P) = \\text{trace}(A^\\prime A P P^\\prime) = \\text{trace}(A^\\prime A) = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_F^2. \\] Exercise 53 Proof corollary 3. Solution. Recall that \\(\\sigma_{\\max} (A) = \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2\\). Hence, \\(\\sigma_{\\max}(A+E) = \\left \\vert \\left \\vert A+E \\right \\vert \\right \\vert_2 \\leq \\left \\vert \\left \\vert A \\right \\vert \\right \\vert_2 + \\left \\vert \\left \\vert E \\right \\vert \\right \\vert_2 = \\sigma_{\\max}(A) + \\left \\vert \\left \\vert E \\right \\vert \\right \\vert_2\\). For the second inequality, recall that \\(\\sigma_{\\min}(A) = \\min_{x \\neq 0} \\frac{\\left \\vert \\left \\vert Ax \\right \\vert \\right \\vert}{\\left \\vert \\left \\vert x \\right \\vert \\right \\vert}\\). First, consider the case where \\(\\sigma_{\\min}(A) = 0\\). Then the result holds. Second, consider the case where \\(\\sigma_{\\min}(A) &gt; 0\\). Then \\[\\begin{aligned} \\sigma_{\\min}(A+E) &amp;= \\min_{v \\neq 0} \\frac{\\left \\vert \\left \\vert (A+E)v \\right \\vert \\right \\vert_2}{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2} \\ge \\min_{v \\ne 0} \\frac{\\left \\vert \\left \\vert Av \\right \\vert \\right \\vert_2}{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2} - \\max_{v \\ne 0} \\frac{\\left \\vert \\left \\vert Ev \\right \\vert \\right \\vert_2}{\\left \\vert \\left \\vert v \\right \\vert \\right \\vert_2} \\\\ &amp;= \\sigma_{\\min}(A) - \\left \\vert \\left \\vert E \\right \\vert \\right \\vert_2, \\end{aligned}\\] where we use the inverse triangle equality. Exercise 54 Proof corollary 5. Solution (1). Let \\(b \\in \\text{range}(A)\\). Then there exists some \\(z\\) such that \\(Az = b\\). Use the SVD of A to get \\[ b = U\\Sigma V^\\prime z. \\] Since \\(\\text{rank}(A) = r\\), the last \\(n-r\\) rows of \\(\\Sigma\\) are all \\(0\\), so \\(U \\Sigma V^\\prime = U_r \\Sigma_r (V_r)^\\prime\\), where \\(U_r\\) is the first \\(r\\) columns of \\(U\\), \\(\\Sigma_r\\) the first \\(r\\) columns and \\(r\\) rows of \\(\\Sigma\\), and \\(V_r\\) the first \\(r\\) columns of \\(V\\). Hence, \\(\\Sigma_r (V_r)^\\prime z = y \\in \\mathbb{R}^r\\). So, \\(b = U_r y\\), which means it is in the span of the first \\(r\\) columns of \\(U\\). Hence, \\(\\text{range}(A) \\subset \\text{span}\\{u_1, \\dots, u_r\\}\\). Now, let \\(b \\in \\text{span}\\{u_1, \\dots, u_r\\}\\). Then \\(b = U_r y\\) for some \\(y \\in \\mathbb{R}^r\\). Recall that \\(U = A V \\Sigma^{-1}\\), hence \\(U_r = A V_r \\Sigma_r^{-1}\\). So, \\(b = A V \\Sigma_r^{-1} y\\). Since \\(V \\Sigma_r^{-1} y = z \\in \\mathbb{R}^m\\), \\(b\\) is in the range of \\(A\\). Hence, \\(\\text{span}\\{u_1, \\dots, u_r\\} \\subset \\text{range}(A)\\). Solution (2). Note that the row space of \\(A\\) is the range of \\(A^\\prime\\). Solve as above. Solution (3). Let \\(b \\in \\text{span}\\{v_{r+1}, \\dots, v_{m}\\}\\). Then \\(b = x_{r+1} v_{r+1} + \\cdots + x_m v_{r+1}\\) for some coefficients \\(x_j \\in \\mathbb{R}\\). Then \\[\\begin{aligned} Ab &amp;= U \\Sigma V^\\prime b \\\\ &amp;= U \\Sigma (x_{r+1} V^\\prime v_{r+1} + \\cdots + x_{m} V^\\prime v_m) \\\\ &amp;= U \\Sigma \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ x_{r+1} \\\\ \\vdots \\\\ x_{m} \\end{bmatrix}, \\end{aligned}\\] since all columns of \\(V\\) are orthogonal. Since the last \\(n-r\\) rows of \\(\\Sigma\\) are all \\(0\\) rows, \\(\\Sigma V^\\prime b = 0\\), so \\(Ab = 0\\), so \\(b \\in \\text{null}(A)\\). Hence, \\(\\text{span}\\{v_{r+1}, \\ldots, v_{m}\\} \\subset \\text{null}(A)\\). Now, Assume \\(b \\in\\) null(\\(A\\)), then \\(Ab = 0\\). Because \\(b \\in R^m\\) and \\(v_1, \\cdots, v_m\\) is a base for \\(R^m\\), there exists a sequence \\(\\{x_i\\}\\) s.t. \\(b = x_1v_1 + \\cdots + x_rv_r + x_{r+1}v_{r+1} + \\cdots + x_{m}v_m\\). Assume for contradiction that \\(b \\notin\\) span(\\(v_{r+1}, \\cdots, v_m\\)), i.e. that \\(x_1, \\cdots, x_r\\) are not all zero. Then \\[Ab= \\sum_{i=1}^{r} \\sigma_i u_i v_i^T b = \\sum_{i=1}^{r}[ \\sigma_i u_i v_i^T (\\sum_{j=1}^{m}x_jv_j)]= \\sum_{i=1}^{r}[ \\sigma_i u_i (\\sum_{j=1}^{m}x_jv_i^Tv_j)] = \\sum_{i=1}^{r} \\sigma_i u_i x_i \\neq 0\\] But this contradicts the assumption that \\(b \\in \\text{null}(A)\\). So, \\(b\\) must be in \\(\\text{span}\\{v_{r+1}, \\cdots, v_m\\}\\). Hence, \\(\\text{null}(A) \\subset \\text{span}\\{v_{r+1}, \\cdots, v_m\\}\\). "],
["homework-8.html", "2.8 Homework 8", " 2.8 Homework 8 Exercise 55 What is the solution to \\(\\min_{x} \\left \\vert \\left \\vert Ax-b \\right \\vert \\right \\vert_2^2\\) in terms of the Moore-Penrose inverse? (pseudo-inverse) Solution. To find the solution, we differentiate and set equal to \\(0\\). So, differentiate \\(g(x) = x^\\prime A^\\prime A x - x^\\prime A b - b^\\prime A x + b^\\prime b\\) and set to \\(0\\): \\[\\frac{dg}{dx} = x^\\prime (A^\\prime A + A^\\prime A) - b^\\prime A - (A^\\prime b)^\\prime = 2x^\\prime (A^\\prime A) - 2b^\\prime A = 0\\] which implies \\((A A^\\prime)x = b A^\\prime\\). Check that \\(x = A^+b\\) satisfies this. Exercise 56 Let \\(Q\\) be an orthogonal matrix with columns \\(q_1, \\ldots, q_n\\). Let \\(Z \\in \\mathbb{R}^{n \\times n}\\) be a symmetric matrix. Show that \\(\\sum_{i=1}^n (q_i^\\prime Z q_i)^2 \\leq \\left \\vert \\left \\vert Z \\right \\vert \\right \\vert_F^2\\). Solution. Since the frobenius norm is invariant to mulitplication by orthogonal matrices: \\(\\left \\vert \\left \\vert Z \\right \\vert \\right \\vert_F^2 = \\left \\vert \\left \\vert Q^\\prime Z Q \\right \\vert \\right \\vert_F^2\\). Since \\[\\begin{aligned} \\begin{bmatrix} q_1^\\prime &amp; \\cdots q_n^\\prime \\end{bmatrix} Z \\begin{bmatrix} q_1 \\\\ \\vdots q_n \\end{bmatrix} &amp;= \\begin{bmatrix} q_1^\\prime &amp; \\cdots q_n^\\prime \\end{bmatrix} \\begin{bmatrix} Z q_1 \\\\ \\vdots Z q_n \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} q_1^\\prime Z q_1 &amp; \\cdots &amp; q_1^\\prime Z q_n \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ q_n^\\prime Z q_1 &amp; \\cdots &amp; q_n^\\prime Z q_n \\end{bmatrix}, \\end{aligned}\\] we have that \\(\\left \\vert \\left \\vert Z \\right \\vert \\right \\vert_F^2 = \\sum_{i=1}^n \\sum_{j=1}^n (q_i^\\prime Z q_j)^2 \\ge \\sum_{i=1}^n (q_i^\\prime Z q_i)^2\\). "],
["homework-9.html", "2.9 Homework 9", " 2.9 Homework 9 Exercise 57 What do we need from \\(A\\) to ensure that the different iterative method schemes are well-defined? Solution. A must be square and diagonal entries must be non-zero. Exercise 58 Does lemma 13 hold for the symmetric SOR? Exercise 59 Compute \\(J^k\\) where \\(J = I\\lambda + E\\) (the Jordan Canonical Form from definition 15) Show that if \\(G = XJX^{-1}\\) and \\(\\rho(G) &lt; 1\\), then \\(J^k \\rightarrow 0\\) as \\(k\\rightarrow \\infty\\). "],
["homework-10.html", "2.10 Homework 10", " 2.10 Homework 10 Exercise 60 Implement Jacobi, Gauss-Seidel without using the \\ operator. Solution. See Jacobi and Gauss-Seidel. Exercise 61 Implement SOR, SSOR without using the \\ operator. Solution. See SOR. Exercise 62 Randomly generate problems and at each iteration, record residual norm and absolute error compare rate of convergence against spectral radious put all this information into a narrative using graphics Exercise 63 Prove that observation 1 from the proof of 7 is true. Exercise 64 Implement Random Kaczmarz for random permutations. Exercise 65 Give a detailed comparison of cycle, randomized, and random permutation Kaczmarz. "],
["homework-11.html", "2.11 Homework 11", " 2.11 Homework 11 Exercise 66 Show that \\(\\alpha \\sum_{i=1}^n a_i (b_i - a_i x^c) = \\alpha A^\\prime (b - Ax^c)\\). Exercise 67 Show that if \\(A^\\prime r^c = 0\\), then \\(x^c = x^*\\), where \\(Ax^* = b\\). Exercise 68 Why is it enough to find an upper bound on \\(\\left \\vert \\left \\vert \\Sigma^{-1}u \\right \\vert \\right \\vert_2^2 \\left \\vert \\left \\vert \\Sigma u \\right \\vert \\right \\vert_2^2\\) for any unit vector \\(w\\) in the proof of theorem 8? Exercise 69 (Kontorovich’s Inequality) Let \\(0 &lt; u_n \\le u_{n-1} \\le \\cdots \\le u_1\\). Then \\[\\left(\\sum_{i=1}^n p_i u_i\\right)\\left(\\sum_{i=1}^n \\frac{p_i}{u_i}\\right) \\leq \\frac{(u_1 + u_n)^2}{4u_1u_n}\\] Exercise 70 For strategy 2, answer the following questions: What is \\(\\alpha\\)? Is it practical? With this \\(\\alpha\\), will we converge? If so, what is the rate of convergence? Exercise 71 For strategy 3, what is \\(\\alpha\\)? Exercise 72 For strategy 4, show the inequality holds. Exercise 73 For strategy 4, what is \\(\\alpha\\)? Exercise 74 Implement a single update of the gradient descent method for a user supplied alpha, assuming that A is invertible. Exercise 75 Now, implement four algorithms one for each of the four strategies of alpha discussed in class. Exercise 76 Generate several test problems for your four algorithms, and compare their performance. Exercise 77 In a narrative, explain which algorithm you would use and when you would use this algorithm. Understanding Gradient Descent’s Dependence on the Condition Number Exercise 78 Write a function to generate 2 equations with 2 unknowns such that the coefficient matrix is dense and symmetric with user specified nonzero eigenvalues. Exercise 79 How are the eigenvalues related to the singular values? Exercise 80 How are the eigenvectors related to the left and right singular vectors? Exercise 81 Write a function to draw the level sets of an arbitrary residual function \\(f\\left(x\\right) = \\left \\vert \\left \\vert Ax - b \\right \\vert \\right \\vert_2^2\\). Exercise 82 Run Gradient Descent using the step size (alpha) from Strategy 2 to solve a sequence of problems where the difference between the singular values of your problem increases in size. Plot the points that Gradient Descent visits (on your level set plot) as it finds its way to the solution. Exercise 83 What do you observe? What is the impact of the singular values on gradient descent? Why does this make sense based on your graphs? "],
["homework-12.html", "2.12 Homework 12", " 2.12 Homework 12 Exercise 84 What conditions (if any) on \\(a_0, ..., a_{d-1}\\) guarantee that an \\(A-\\)normalized, \\(A-\\)conjugated set \\(\\{s_0, ..., s_{d-1}\\}\\) can be produced by Gram Schmidt? "],
["homework-13.html", "2.13 Homework 13", " 2.13 Homework 13 Exercise 85 Show that the procedure @ref(conjugated_gradients) is the same as the one implemented in code provided by Patel. Exercise 86 Prove that CG converges in finitely many steps (in exact arithmetic). Exercise 87 Show that the limit in 12 holds. Exercise 88 Characterize the relationship between the Jacobian and the gradient. Exercise 89 Using IFT: when will \\(F(x) = 0\\) have an isolated solution? (i.e. a solution where there is an open set containing that solution with no other solutions) Exercise 90 What happens if \\(F(z^*) = 0\\) and \\(\\text{rank}(J(z^*)) &lt; \\min(n,m)\\)? "]
]
