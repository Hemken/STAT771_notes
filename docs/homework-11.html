<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STAT 771: My notes</title>
  <meta name="description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="STAT 771: My notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STAT 771: My notes" />
  
  <meta name="twitter:description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison." />
  

<meta name="author" content="Ralph Møller Trane">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="homework-10.html">
<link rel="next" href="homework-12.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>1</b> Lecture Notes</a><ul>
<li class="chapter" data-level="" data-path="lecture-1-96.html"><a href="lecture-1-96.html"><i class="fa fa-check"></i>Lecture 1: 9/6</a></li>
<li class="chapter" data-level="1.1" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html"><i class="fa fa-check"></i><b>1.1</b> Positional numeral system</a><ul>
<li class="chapter" data-level="1.1.1" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#floating-point-format"><i class="fa fa-check"></i><b>1.1.1</b> Floating Point Format</a></li>
<li class="chapter" data-level="1.1.2" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#ieee-standards"><i class="fa fa-check"></i><b>1.1.2</b> IEEE Standards</a></li>
<li class="chapter" data-level="1.1.3" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#errors"><i class="fa fa-check"></i><b>1.1.3</b> Errors</a></li>
<li class="chapter" data-level="1.1.4" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#square-linear-systems"><i class="fa fa-check"></i><b>1.1.4</b> Square Linear Systems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="orthogonalization.html"><a href="orthogonalization.html"><i class="fa fa-check"></i><b>1.2</b> Orthogonalization</a><ul>
<li class="chapter" data-level="1.2.1" data-path="orthogonalization.html"><a href="orthogonalization.html#motivating-problems"><i class="fa fa-check"></i><b>1.2.1</b> Motivating problems</a></li>
<li class="chapter" data-level="" data-path="orthogonalization.html"><a href="orthogonalization.html#lecture-4-918"><i class="fa fa-check"></i>Lecture 4: 9/18</a></li>
<li class="chapter" data-level="1.2.2" data-path="orthogonalization.html"><a href="orthogonalization.html#qr-decomposition"><i class="fa fa-check"></i><b>1.2.2</b> QR Decomposition</a></li>
<li class="chapter" data-level="1.2.3" data-path="orthogonalization.html"><a href="orthogonalization.html#existence-of-qr-decomposition."><i class="fa fa-check"></i><b>1.2.3</b> Existence of QR-decomposition.</a></li>
<li class="chapter" data-level="" data-path="orthogonalization.html"><a href="orthogonalization.html#lecture-5-920"><i class="fa fa-check"></i>Lecture 5: 9/20</a></li>
<li class="chapter" data-level="1.2.4" data-path="orthogonalization.html"><a href="orthogonalization.html#large-data-problem"><i class="fa fa-check"></i><b>1.2.4</b> “Large” Data Problem</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html"><i class="fa fa-check"></i><b>1.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="1.3.1" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#motivating-problems-1"><i class="fa fa-check"></i><b>1.3.1</b> Motivating Problems</a></li>
<li class="chapter" data-level="1.3.2" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#svd"><i class="fa fa-check"></i><b>1.3.2</b> SVD</a></li>
<li class="chapter" data-level="1.3.3" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#existence-and-properties"><i class="fa fa-check"></i><b>1.3.3</b> Existence and Properties</a></li>
<li class="chapter" data-level="1.3.4" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#random-projections"><i class="fa fa-check"></i><b>1.3.4</b> Random Projections</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="iterative-methods.html"><a href="iterative-methods.html"><i class="fa fa-check"></i><b>1.4</b> Iterative Methods</a><ul>
<li class="chapter" data-level="1.4.1" data-path="iterative-methods.html"><a href="iterative-methods.html#overview"><i class="fa fa-check"></i><b>1.4.1</b> Overview</a></li>
<li class="chapter" data-level="1.4.2" data-path="iterative-methods.html"><a href="iterative-methods.html#outline"><i class="fa fa-check"></i><b>1.4.2</b> Outline</a></li>
<li class="chapter" data-level="1.4.3" data-path="iterative-methods.html"><a href="iterative-methods.html#motivation"><i class="fa fa-check"></i><b>1.4.3</b> Motivation</a></li>
<li class="chapter" data-level="1.4.4" data-path="iterative-methods.html"><a href="iterative-methods.html#splitting-methods"><i class="fa fa-check"></i><b>1.4.4</b> Splitting Methods</a></li>
<li class="chapter" data-level="1.4.5" data-path="iterative-methods.html"><a href="iterative-methods.html#convergence"><i class="fa fa-check"></i><b>1.4.5</b> Convergence</a></li>
<li class="chapter" data-level="1.4.6" data-path="iterative-methods.html"><a href="iterative-methods.html#randomized-kaczmarz-method"><i class="fa fa-check"></i><b>1.4.6</b> Randomized Kaczmarz Method</a></li>
<li class="chapter" data-level="1.4.7" data-path="iterative-methods.html"><a href="iterative-methods.html#gradient-descent"><i class="fa fa-check"></i><b>1.4.7</b> Gradient Descent</a></li>
<li class="chapter" data-level="1.4.8" data-path="iterative-methods.html"><a href="iterative-methods.html#conjugate-gradient"><i class="fa fa-check"></i><b>1.4.8</b> Conjugate Gradient</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html"><i class="fa fa-check"></i><b>1.5</b> Nonlinear Systems of Equations</a><ul>
<li class="chapter" data-level="1.5.1" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#review-of-jacobians"><i class="fa fa-check"></i><b>1.5.1</b> Review of Jacobians</a></li>
<li class="chapter" data-level="1.5.2" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#motivating-problem"><i class="fa fa-check"></i><b>1.5.2</b> Motivating Problem</a></li>
<li class="chapter" data-level="1.5.3" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#non-linear-equations"><i class="fa fa-check"></i><b>1.5.3</b> Non-linear Equations</a></li>
<li class="chapter" data-level="1.5.4" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#picards-method"><i class="fa fa-check"></i><b>1.5.4</b> Picard’s Method</a></li>
<li class="chapter" data-level="1.5.5" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#newtons-method"><i class="fa fa-check"></i><b>1.5.5</b> Newton’s Method</a></li>
<li class="chapter" data-level="1.5.6" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#inexact-newtons-method"><i class="fa fa-check"></i><b>1.5.6</b> Inexact Newton’s Method</a></li>
<li class="chapter" data-level="1.5.7" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#semi-smooth-newtons-method"><i class="fa fa-check"></i><b>1.5.7</b> Semi-Smooth Newton’s Method</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html"><i class="fa fa-check"></i><b>1.6</b> Non-linear Unconstrained Optimization</a><ul>
<li class="chapter" data-level="1.6.1" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#motivating-problems-2"><i class="fa fa-check"></i><b>1.6.1</b> Motivating Problems</a></li>
<li class="chapter" data-level="1.6.2" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#formalize-minimization"><i class="fa fa-check"></i><b>1.6.2</b> Formalize Minimization</a></li>
<li class="chapter" data-level="1.6.3" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#algorithmic-and-theoretical-preview"><i class="fa fa-check"></i><b>1.6.3</b> Algorithmic and Theoretical Preview</a></li>
<li class="chapter" data-level="1.6.4" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#line-search"><i class="fa fa-check"></i><b>1.6.4</b> Line Search</a></li>
<li class="chapter" data-level="1.6.5" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#trust-region"><i class="fa fa-check"></i><b>1.6.5</b> Trust Region</a></li>
<li class="chapter" data-level="1.6.6" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#acceptancerejection-criteria"><i class="fa fa-check"></i><b>1.6.6</b> Acceptance/Rejection Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="homework-assignments.html"><a href="homework-assignments.html"><i class="fa fa-check"></i><b>2</b> Homework Assignments</a><ul>
<li class="chapter" data-level="2.1" data-path="homework-1.html"><a href="homework-1.html"><i class="fa fa-check"></i><b>2.1</b> Homework 1</a></li>
<li class="chapter" data-level="2.2" data-path="homework-2.html"><a href="homework-2.html"><i class="fa fa-check"></i><b>2.2</b> Homework 2</a></li>
<li class="chapter" data-level="2.3" data-path="homework-3.html"><a href="homework-3.html"><i class="fa fa-check"></i><b>2.3</b> Homework 3</a></li>
<li class="chapter" data-level="2.4" data-path="homework-4.html"><a href="homework-4.html"><i class="fa fa-check"></i><b>2.4</b> Homework 4</a></li>
<li class="chapter" data-level="2.5" data-path="homework-5.html"><a href="homework-5.html"><i class="fa fa-check"></i><b>2.5</b> Homework 5</a></li>
<li class="chapter" data-level="2.6" data-path="homework-6.html"><a href="homework-6.html"><i class="fa fa-check"></i><b>2.6</b> Homework 6</a></li>
<li class="chapter" data-level="2.7" data-path="homework-7.html"><a href="homework-7.html"><i class="fa fa-check"></i><b>2.7</b> Homework 7</a></li>
<li class="chapter" data-level="2.8" data-path="homework-8.html"><a href="homework-8.html"><i class="fa fa-check"></i><b>2.8</b> Homework 8</a></li>
<li class="chapter" data-level="2.9" data-path="homework-9.html"><a href="homework-9.html"><i class="fa fa-check"></i><b>2.9</b> Homework 9</a></li>
<li class="chapter" data-level="2.10" data-path="homework-10.html"><a href="homework-10.html"><i class="fa fa-check"></i><b>2.10</b> Homework 10</a></li>
<li class="chapter" data-level="2.11" data-path="homework-11.html"><a href="homework-11.html"><i class="fa fa-check"></i><b>2.11</b> Homework 11</a></li>
<li class="chapter" data-level="2.12" data-path="homework-12.html"><a href="homework-12.html"><i class="fa fa-check"></i><b>2.12</b> Homework 12</a></li>
<li class="chapter" data-level="2.13" data-path="homework-13.html"><a href="homework-13.html"><i class="fa fa-check"></i><b>2.13</b> Homework 13</a></li>
<li class="chapter" data-level="2.14" data-path="homework-14.html"><a href="homework-14.html"><i class="fa fa-check"></i><b>2.14</b> Homework 14</a></li>
<li class="chapter" data-level="2.15" data-path="homework-15.html"><a href="homework-15.html"><i class="fa fa-check"></i><b>2.15</b> Homework 15</a></li>
<li class="chapter" data-level="2.16" data-path="homework-16.html"><a href="homework-16.html"><i class="fa fa-check"></i><b>2.16</b> Homework 16</a><ul>
<li class="chapter" data-level="2.16.1" data-path="homework-16.html"><a href="homework-16.html#newtons-method-1"><i class="fa fa-check"></i><b>2.16.1</b> Newton’s Method</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="homework-17.html"><a href="homework-17.html"><i class="fa fa-check"></i><b>2.17</b> Homework 17</a></li>
<li class="chapter" data-level="2.18" data-path="homework-18.html"><a href="homework-18.html"><i class="fa fa-check"></i><b>2.18</b> Homework 18</a></li>
<li class="chapter" data-level="2.19" data-path="homework-19.html"><a href="homework-19.html"><i class="fa fa-check"></i><b>2.19</b> Homework 19</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 771: My notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="homework-11" class="section level2">
<h2><span class="header-section-number">2.11</span> Homework 11</h2>

<div class="exercise">
<span id="exr:q1101" class="exercise"><strong>Exercise 66  </strong></span>Show that <span class="math inline">\(\alpha \sum_{i=1}^n a_i (b_i - a_i x^c) = \alpha A^\prime (b - Ax^c)\)</span>.
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span> Recall that <span class="math inline">\(a_i\)</span> is a column vector, and that <span class="math inline">\(a_i^\prime\)</span> is the row vector of matrix <span class="math inline">\(A\)</span></p>
<p><span class="math inline">\(A^T(b-Ax^c) = \begin{pmatrix} a_1, \cdots, a_n \end{pmatrix}(b - Ax^c) = \sum_{i=1}^{n}a_i(b_i - a_i^Tx^c)\)</span></p>
</div>


<div class="exercise">
<span id="exr:q1102" class="exercise"><strong>Exercise 67  </strong></span>Show that if <span class="math inline">\(A^\prime r^c = 0\)</span>, then <span class="math inline">\(x^c = x^*\)</span>, where <span class="math inline">\(Ax^* = b\)</span>.
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span>  <span class="math inline">\(x^+= x^c + \alpha A^T(b-Ax^c) = x^c + \alpha A^Tr^c = x^c\)</span></p>
</div>


<div class="exercise">
<span id="exr:q1103" class="exercise"><strong>Exercise 68  </strong></span>Why is it enough to find an upper bound on <span class="math inline">\({\left \vert \left \vert \Sigma^{-1}u \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma u \right \vert \right \vert}_2^2\)</span> for any unit vector <span class="math inline">\(u\)</span> in the proof of theorem <a href="iterative-methods.html#thm:grad-descent">8</a>?
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span>  Recall that <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert} = \sup_{{\left \vert \left \vert v \right \vert \right \vert}=1} {\left \vert \left \vert Av \right \vert \right \vert} = \sup_{{\left \vert \left \vert v \right \vert \right \vert} \neq 0} \frac{{\left \vert \left \vert Av \right \vert \right \vert}}{{\left \vert \left \vert v \right \vert \right \vert}}\)</span>. Hence,</p>
<p><span class="math display">\[\begin{aligned}
{\left \vert \left \vert \Sigma^{-1} u \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma u \right \vert \right \vert}_2^2 \le \frac{(\sigma_1^2 + \sigma_n^2)^2}{4\sigma_1^2 \cdot \sigma_n^2} \quad \forall u: {\left \vert \left \vert u \right \vert \right \vert} = 1 \\
\Rightarrow \frac{1}{{\left \vert \left \vert \Sigma^{-1} u \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma u \right \vert \right \vert}_2^2} \le \frac{4\sigma_1^2 \cdot \sigma_n^2}{(\sigma_1^2 + \sigma_n^2)^2} \quad \forall u: {\left \vert \left \vert u \right \vert \right \vert} = 1 \\ 
\Rightarrow \frac{{\left \vert \left \vert w \right \vert \right \vert}}{{\left \vert \left \vert \Sigma^{-1} w \right \vert \right \vert}_2^2}\frac{{\left \vert \left \vert w \right \vert \right \vert}}{{\left \vert \left \vert \Sigma w \right \vert \right \vert}_2^2} \le \frac{4\sigma_1^2 \cdot \sigma_n^2}{(\sigma_1^2 + \sigma_n^2)^2} \quad \forall w: {\left \vert \left \vert w \right \vert \right \vert} \neq 0.
\end{aligned}\]</span></p>
<p>So, find an upper bound on <span class="math inline">\({\left \vert \left \vert \Sigma^{-1} u \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma u \right \vert \right \vert}_2^2\)</span> that holds for all unit vectors <span class="math inline">\(u\)</span>, and we have a lower bound on <span class="math inline">\(\frac{{\left \vert \left \vert w \right \vert \right \vert}}{{\left \vert \left \vert \Sigma^{-1} w \right \vert \right \vert}_2^2}\frac{{\left \vert \left \vert w \right \vert \right \vert}}{{\left \vert \left \vert \Sigma w \right \vert \right \vert}_2^2}\)</span> that holds for all non-zero vectors <span class="math inline">\(w\)</span>.</p>
</div>


<div class="exercise">
<p><span id="exr:q1104" class="exercise"><strong>Exercise 69  (Kontorovich’s Inequality)  </strong></span>Let <span class="math inline">\(0 &lt; u_n \le u_{n-1} \le \cdots \le u_1\)</span>. Then</p>
<p><span class="math display">\[\left(\sum_{i=1}^n p_i u_i\right)\left(\sum_{i=1}^n \frac{p_i}{u_i}\right) \leq \frac{(u_1 + u_n)^2}{4u_1u_n}\]</span></p>
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> <a href="http://mathworld.wolfram.com/KantorovichInequality.html" class="uri">http://mathworld.wolfram.com/KantorovichInequality.html</a> <a href="http://services.aops.com/download.php?id=YXR0YWNobWVudHMvMS84LzM2YWVhMTg3Yzc0NDA5YTZmNDRhZDVjNmZlNmM4NzllMjRjNmE3LnBkZg" class="uri">http://services.aops.com/download.php?id=YXR0YWNobWVudHMvMS84LzM2YWVhMTg3Yzc0NDA5YTZmNDRhZDVjNmZlNmM4NzllMjRjNmE3LnBkZg</a>==&amp;rn=a2FudG9yb3ZpY2hfcHJvb2YucGRm
</div>


<div class="exercise">
<p><span id="exr:q1105" class="exercise"><strong>Exercise 70  </strong></span>For strategy 2, answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>What is <span class="math inline">\(\alpha\)</span>? Is it practical?</li>
<li>With this <span class="math inline">\(\alpha\)</span>, will we converge? If so, what is the rate of convergence?</li>
</ol>
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span>  According to strategy 2, we choose <span class="math inline">\(\alpha\)</span> such that it minimizes <span class="math inline">\({\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert}_2^2\)</span>. So, to minimize this, we differentiate with respect to <span class="math inline">\(\alpha\)</span>, set to <span class="math inline">\(0\)</span>, and solve for <span class="math inline">\(\alpha\)</span> – the result is <span class="math inline">\(\alpha = \frac{{\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2}{{\left \vert \left \vert A^\prime r_k \right \vert \right \vert}_2^2}\)</span>.</p>
</div>


<div class="exercise">
<span id="exr:q1106" class="exercise"><strong>Exercise 71  </strong></span>For strategy 3, what is <span class="math inline">\(\alpha\)</span>?
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span>  According to strategy 3, we choose <span class="math inline">\(\alpha\)</span> such that it minimizes <span class="math inline">\({\left \vert \left \vert I-\alpha AA^\prime \right \vert \right \vert}_2^2\)</span>. Again, differentiate with respect to <span class="math inline">\(\alpha\)</span>, set to <span class="math inline">\(0\)</span>, and solve for <span class="math inline">\(\alpha\)</span> – the result is <span class="math inline">\(\alpha = \frac{{\left \vert \left \vert A \right \vert \right \vert}_2}{{\left \vert \left \vert AA^\prime \right \vert \right \vert}_2}\)</span>.</p>
</div>


<div class="exercise">
<span id="exr:q1107" class="exercise"><strong>Exercise 72  </strong></span>For strategy 4, show the inequality holds.
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span>  Recall <span class="math inline">\(Ax^* = b\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
  {\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert} &amp;= {\left \vert \left \vert x_k + \alpha A^\prime(b-Ax_k) - x^* \right \vert \right \vert} \\
                       &amp;= {\left \vert \left \vert x_k - x^* - \alpha A^\prime (Ax^* - Ax_k) \right \vert \right \vert} \\
                       &amp;= {\left \vert \left \vert (I - \alpha A^\prime A)(x_k - x^*) \right \vert \right \vert} \le {\left \vert \left \vert I-\alpha A^\primeA \right \vert \right \vert}{\left \vert \left \vert x_k - x^* \right \vert \right \vert}
\end{aligned}\]</span></p>
</div>


<div class="exercise">
<span id="exr:q1108" class="exercise"><strong>Exercise 73  </strong></span>For strategy 4, what is <span class="math inline">\(\alpha\)</span>?
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> Same as for strategy 3.
</div>


<div class="exercise">
<span id="exr:q1109" class="exercise"><strong>Exercise 74  </strong></span>Implement a single update of the gradient descent method for a user supplied alpha, assuming that A is invertible.
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> See the Julia chunk below
</div>

<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">using LinearAlgebra

<span class="co"># Single update of the gradient descent method</span>

<span class="kw">function</span> singleGradDescent(A,x,b, alpha)
    <span class="st">&quot;&quot;&quot;</span>
    b is constant vector.
    A is the coefficient matrix.
    x is current iterate.
    alpha is step size.
    <span class="st">&quot;&quot;&quot;</span>
    x = x + alpha * A&#39;*(b-A*x)
    <span class="kw">return</span> x
<span class="kw">end</span></code></pre></div>

<div class="exercise">
<span id="exr:q1110" class="exercise"><strong>Exercise 75  </strong></span>Now, implement four algorithms one for each of the four strategies of alpha discussed in class.
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> See Julia chunk below
</div>

<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">
<span class="co">## Gradient descent for Strategy 1: alpha = ||A&#39;r_c||^2/||AA&#39;r_c||^2</span>
<span class="kw">function</span> gradientDescentStrat1(A,b,x,x0;epsilon=<span class="fl">1e-15</span>, maxIter=<span class="fl">1000</span>)
    <span class="co"># Get current residual</span>
    rc = A*x0-b
    <span class="co"># Get current error</span>
    ec = norm(x - x0)
    i = <span class="fl">1</span>

    <span class="kw">while</span> norm(A&#39;*rc) &gt;= epsilon &amp;&amp; i &lt; maxIter
        alpha = norm(A&#39;*rc)^<span class="fl">2</span>/norm(A*A&#39;*rc)^<span class="fl">2</span>
        x0 = SingleStepGS(alpha,A,x0,b)
        rc = A*x0-b
        ec = vcat(ec, norm(x0 - x))
        i += <span class="fl">1</span>
    <span class="kw">end</span>

    <span class="kw">return</span> x0, ec
<span class="kw">end</span>

<span class="co">## Gradient descent for Strategy 2: alpha = ||A&#39;(x_c - x^*)||^2/||AA&#39;(x_c-x^*)||^2</span>
<span class="kw">function</span> gradientDescentStrat2(A,b,x,x0;epsilon=<span class="fl">1e-15</span>, maxIter=<span class="fl">10000</span>)
    <span class="co"># Get current residual</span>
    rc = A*x0-b
    <span class="co"># Get current error</span>
    ec = norm(x0 - x)
    i = <span class="fl">1</span>

    <span class="kw">while</span> norm(rc) &gt;= epsilon &amp;&amp; i &lt; maxIter
        alpha = norm(A&#39;*ec[<span class="kw">end</span>])^<span class="fl">2</span>/norm(A*A&#39;*ec[<span class="kw">end</span>])^<span class="fl">2</span>
        x0 = SingleStepGS(alpha,A,x0,b)
        rc = A*x0-b
        ec = vcat(ec, norm(x0 - x))
        i += <span class="fl">1</span>
    <span class="kw">end</span>

    <span class="kw">return</span> x0, ec
<span class="kw">end</span>

<span class="co">## Gradient descent for Strategy 3: alpha = ||A||^2/||AA&#39;||^2</span>
<span class="kw">function</span> gradientDescentStrat3(A,b,x,x0;epsilon=<span class="fl">1e-15</span>, maxIter=<span class="fl">10000</span>)
    <span class="co"># Get current residual</span>
    rc = A*x0-b
    <span class="co"># Get current error</span>
    ec = norm(x0 - x)
    i = <span class="fl">1</span>
    alpha = norm(A)^<span class="fl">2</span>/norm(A*A&#39;)^<span class="fl">2</span>

    <span class="kw">while</span> norm(rc) &gt;= epsilon &amp;&amp; i &lt; maxIter
        x0 = SingleStepGS(alpha,A,x0,b)
        rc = A*x0-b
        ec = vcat(ec, norm(x0 - x))
        i += <span class="fl">1</span>
    <span class="kw">end</span>

    <span class="kw">return</span> x0, ec
<span class="kw">end</span></code></pre></div>

<div class="exercise">
<span id="exr:q1111" class="exercise"><strong>Exercise 76  </strong></span>Generate several test problems for your four algorithms, and compare their performance.
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> ???
</div>


<div class="exercise">
<span id="exr:q1112" class="exercise"><strong>Exercise 77  </strong></span>In a narrative, explain which algorithm you would use and when you would use this algorithm.
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> ???
</div>

<p><strong>Understanding Gradient Descent’s Dependence on the Condition Number</strong></p>

<div class="exercise">
<span id="exr:q1113" class="exercise"><strong>Exercise 78  </strong></span>Write a function to generate 2 equations with 2 unknowns such that the coefficient matrix is dense and symmetric with user specified nonzero eigenvalues.
</div>

<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia"><span class="kw">function</span> generateProblemEigenvalues(eigenvalues)
    n = length(eigenvalues)
    B = rand(n,n)
    Q = qr(B).Q
    Lambda = diagm(<span class="fl">0</span> =&gt; eigenvalues)
    A = Q*Lambda*Q&#39;
    x = randn(n)
    b = A*x
    <span class="kw">return</span> A, x, b
<span class="kw">end</span></code></pre></div>

<div class="exercise">
<span id="exr:q1114" class="exercise"><strong>Exercise 79  </strong></span>How are the eigenvalues related to the singular values?
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span>  In general, the eigenvalues of a matrix <span class="math inline">\(A\)</span> is the square root of the symmetric matrix <span class="math inline">\(A^\prime A\)</span>.</p>
<p>When <span class="math inline">\(A\)</span> is symmetric, they are the same.</p>
</div>


<div class="exercise">
<span id="exr:q1115" class="exercise"><strong>Exercise 80  </strong></span>How are the eigenvectors related to the left and right singular vectors?
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span>  The left singular vectors are the eigen vectors.</p>
</div>


<div class="exercise">
<span id="exr:q1116" class="exercise"><strong>Exercise 81  </strong></span>Write a function to draw the level sets of an arbitrary residual function <span class="math inline">\(f\left(x\right) = {\left \vert \left \vert Ax - b \right \vert \right \vert}_2^2\)</span>.
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> ???
</div>


<div class="exercise">
<span id="exr:q1117" class="exercise"><strong>Exercise 82  </strong></span>Run Gradient Descent using the step size (alpha) from Strategy 2 to solve a sequence of problems where the difference between the singular values of your problem increases in size. Plot the points that Gradient Descent visits (on your level set plot) as it finds its way to the solution.
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> ???
</div>


<div class="exercise">
<span id="exr:q1118" class="exercise"><strong>Exercise 83  </strong></span>What do you observe? What is the impact of the singular values on gradient descent? Why does this make sense based on your graphs?
</div>


<div class="solution">
 <span class="solution"><em>Solution. </em></span> ???
</div>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="homework-10.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="homework-12.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["STAT771_notes.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
