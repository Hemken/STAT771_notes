<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STAT 771: My notes</title>
  <meta name="description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="STAT 771: My notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STAT 771: My notes" />
  
  <meta name="twitter:description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison." />
  

<meta name="author" content="Ralph Møller Trane">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="homework-assignments.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>1</b> Lecture Notes</a><ul>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lecture-1-96"><i class="fa fa-check"></i>Lecture 1: 9/6</a></li>
<li class="chapter" data-level="1.1" data-path="lecture-notes.html"><a href="lecture-notes.html#positional-numeral-system"><i class="fa fa-check"></i><b>1.1</b> Positional numeral system</a></li>
<li class="chapter" data-level="1.2" data-path="lecture-notes.html"><a href="lecture-notes.html#floating-point-format"><i class="fa fa-check"></i><b>1.2</b> Floating Point Format</a><ul>
<li class="chapter" data-level="1.2.1" data-path="lecture-notes.html"><a href="lecture-notes.html#ieee-standards"><i class="fa fa-check"></i><b>1.2.1</b> IEEE Standards</a></li>
<li class="chapter" data-level="1.2.2" data-path="lecture-notes.html"><a href="lecture-notes.html#errors"><i class="fa fa-check"></i><b>1.2.2</b> Errors</a></li>
<li class="chapter" data-level="1.2.3" data-path="lecture-notes.html"><a href="lecture-notes.html#square-linear-systems"><i class="fa fa-check"></i><b>1.2.3</b> Square Linear Systems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="lecture-notes.html"><a href="lecture-notes.html#orthogonalization"><i class="fa fa-check"></i><b>1.3</b> Orthogonalization</a><ul>
<li class="chapter" data-level="1.3.1" data-path="lecture-notes.html"><a href="lecture-notes.html#motivating-problems"><i class="fa fa-check"></i><b>1.3.1</b> Motivating problems</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lecture-4-918"><i class="fa fa-check"></i>Lecture 4: 9/18</a></li>
<li class="chapter" data-level="1.3.2" data-path="lecture-notes.html"><a href="lecture-notes.html#qr-decomposition"><i class="fa fa-check"></i><b>1.3.2</b> QR Decomposition</a></li>
<li class="chapter" data-level="1.3.3" data-path="lecture-notes.html"><a href="lecture-notes.html#existence-of-qr-decomposition."><i class="fa fa-check"></i><b>1.3.3</b> Existence of QR-decomposition.</a></li>
<li class="chapter" data-level="" data-path="lecture-notes.html"><a href="lecture-notes.html#lecture-5-920"><i class="fa fa-check"></i>Lecture 5: 9/20</a></li>
<li class="chapter" data-level="1.3.4" data-path="lecture-notes.html"><a href="lecture-notes.html#large-data-problem"><i class="fa fa-check"></i><b>1.3.4</b> “Large” Data Problem</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="lecture-notes.html"><a href="lecture-notes.html#singular-value-decomposition-svd"><i class="fa fa-check"></i><b>1.4</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="1.4.1" data-path="lecture-notes.html"><a href="lecture-notes.html#motivating-problems-1"><i class="fa fa-check"></i><b>1.4.1</b> Motivating Problems</a></li>
<li class="chapter" data-level="1.4.2" data-path="lecture-notes.html"><a href="lecture-notes.html#svd"><i class="fa fa-check"></i><b>1.4.2</b> SVD</a></li>
<li class="chapter" data-level="1.4.3" data-path="lecture-notes.html"><a href="lecture-notes.html#existence-and-properties"><i class="fa fa-check"></i><b>1.4.3</b> Existence and Properties</a></li>
<li class="chapter" data-level="1.4.4" data-path="lecture-notes.html"><a href="lecture-notes.html#random-projections"><i class="fa fa-check"></i><b>1.4.4</b> Random Projections</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="homework-assignments.html"><a href="homework-assignments.html"><i class="fa fa-check"></i><b>2</b> Homework Assignments</a><ul>
<li class="chapter" data-level="2.1" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-1"><i class="fa fa-check"></i><b>2.1</b> Homework 1</a></li>
<li class="chapter" data-level="2.2" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-2"><i class="fa fa-check"></i><b>2.2</b> Homework 2</a></li>
<li class="chapter" data-level="2.3" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-3"><i class="fa fa-check"></i><b>2.3</b> Homework 3</a></li>
<li class="chapter" data-level="2.4" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-4"><i class="fa fa-check"></i><b>2.4</b> Homework 4</a></li>
<li class="chapter" data-level="2.5" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-5"><i class="fa fa-check"></i><b>2.5</b> Homework 5</a></li>
<li class="chapter" data-level="2.6" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-6"><i class="fa fa-check"></i><b>2.6</b> Homework 6</a></li>
<li class="chapter" data-level="2.7" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-7"><i class="fa fa-check"></i><b>2.7</b> Homework 7</a></li>
<li class="chapter" data-level="2.8" data-path="homework-assignments.html"><a href="homework-assignments.html#homework-8"><i class="fa fa-check"></i><b>2.8</b> Homework 8</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 771: My notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture-notes" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Lecture Notes</h1>
<div id="lecture-1-96" class="section level3 unnumbered">
<h3>Lecture 1: 9/6</h3>
<p>Goals for the first few lectures:</p>
<ol style="list-style-type: decimal">
<li>Develop basic understanding of floating point numbers (<em>fp</em> numbers)</li>
<li>Develop some basic notions of errors and their consequences</li>
</ol>
<p>References:</p>
<ol style="list-style-type: lower-roman">
<li>David Goldberg (1991)</li>
<li>John D. Cook (2009)</li>
<li>Hingham (2002)</li>
</ol>
</div>
<div id="positional-numeral-system" class="section level2">
<h2><span class="header-section-number">1.1</span> Positional numeral system</h2>
<p>We assume we have a decimal representation of numbers. I.e. that it exists. It is not within the scope of this class to prove this.</p>
<p>Now, this is NOT the optimal way for a computer to represent numbers. For various reasons, there are more desirable ways to store numbers. So we need a different way of representing the numbers.</p>
<p>Ingredients for different representation:</p>
<ol style="list-style-type: lower-roman">
<li>A base, refered to as <span class="math inline">\(\beta\)</span>. It holds that <span class="math inline">\(\beta \in {2,3,4,...}\)</span></li>
<li>A significand: a sequence of digits: <span class="math inline">\(d_0.d_1d_2d_3d_4...\)</span>, where <span class="math inline">\(d_j \in \{0, 1,...,\beta-1\}\)</span></li>
<li>An exponent: <span class="math inline">\(e \in \mathbb{Z}\)</span>.</li>
</ol>
<p>The representation <span class="math inline">\(d_0.d_1d_2... \times \beta^e\)</span> means <span class="math inline">\(\left(d_0 + d_1\cdot \beta^{-1} + ... + d_{p-1}\beta^{-(p-1)}\right)\cdot \beta^e\)</span>.</p>
</div>
<div id="floating-point-format" class="section level2">
<h2><span class="header-section-number">1.2</span> Floating Point Format</h2>

<div class="definition">
<span id="def:unnamed-chunk-1" class="definition"><strong>Definition 1.1  </strong></span>A fp is one that can be represented in a base <span class="math inline">\(\beta\)</span> with a fixed digit <span class="math inline">\(p\)</span> (precision), and whose exponent is between <span class="math inline">\(e_{min}\)</span> and <span class="math inline">\(e_{max}\)</span>.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-2" class="example"><strong>Example 1.1  </strong></span>Let <span class="math inline">\(\beta = 10, p = 3, e_{min} = -1, e_{max} = 1\)</span>. Want to represent <span class="math inline">\(0.1\)</span>. Several options:</p>
<ol style="list-style-type: lower-roman">
<li>Let d_0=0, d_1 = 0, d_2 = 1, e = 1.</li>
<li>Let d_0=0, d_1 = 1, d_2 = 0, e = 0.</li>
<li>d_0 = 1, d_1 = 0, d_2 = 0, e = -1.</li>
</ol>
<p>If we fill into the equation above, we get 0.1:</p>
<span class="math display">\[\begin{align*}
  i:  &amp; \left(0 + 0\cdot 10^{-1} + 1\cdot 10^{-2}\right)\cdot 10^{1} \\
  ii: &amp; \left(0 + 1\cdot 10^{-1} + 1\cdot 10^{-2}\right)\cdot 10^{0} \\
  iii:&amp; \left(1 + 0\cdot 10^{-1} + 1\cdot 10^{-2}\right)\cdot 10^{-1}
\end{align*}\]</span>
</div>


<div class="definition">
<span id="def:unnamed-chunk-3" class="definition"><strong>Definition 1.2  </strong></span>A fp number is said to be <em>normalized</em> if <span class="math inline">\(d_0 \neq 0\)</span>.
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-4" class="exercise"><strong>Exercise 1.1  </strong></span>What is the total number of values that can be represented in the normalized fp format with base <span class="math inline">\(\beta, p, e_{min}, e_{max}\)</span>?</p>
<p>We count the different values each of the elements of a <em>fp</em> can take:</p>
<ul>
<li><span class="math inline">\(d_0\)</span> can be from 1 to <span class="math inline">\(\beta-1\)</span>, so <span class="math inline">\(\beta-1\)</span> different values.</li>
<li><span class="math inline">\(d_1,...,d_{p-1}\)</span> each takes a value in <span class="math inline">\(\{0,1,...,\beta-1\}\)</span>. Hence, we can choose the digits <span class="math inline">\(d_1,...,d_{p-1}\)</span> in <span class="math inline">\(\beta^{p-1}\)</span> different ways.</li>
<li><span class="math inline">\(e\)</span> can take <span class="math inline">\(e_{max} - e_{min} + 1\)</span> different values (all integers from <span class="math inline">\(e_{min}\)</span> to <span class="math inline">\(e_{max}\)</span>, both included, hence the <span class="math inline">\(+1\)</span>).</li>
</ul>
So, in total, there are <span class="math inline">\((\beta-1)\cdot \beta^{p-1}\cdot (e_{max} - e_{min} + 1)\)</span> different values that can be represented in the normalized fp format with base <span class="math inline">\(\beta\)</span>, precision <span class="math inline">\(p\)</span>, and <span class="math inline">\(e_{min}, e_{max}\)</span> given.
</div>

<div id="ieee-standards" class="section level3">
<h3><span class="header-section-number">1.2.1</span> IEEE Standards</h3>
<p>IEEE have standards for how to deal with approximations and errors.</p>
<p>For our purposes, a bit is a single unit of storage on a computer, which can either be 0 or 1. Hence, we’ll be focusing on fp formats where <span class="math inline">\(\beta = 2\)</span>.</p>
<div id="the-16-bit-standard-half-precision-standard." class="section level4">
<h4><span class="header-section-number">1.2.1.1</span> The 16 bit standard (half precision standard).</h4>
<p>The 16 bits of storage are used in the following way, when following the 16 bit standard:</p>
<ul>
<li>1 bit for the sign
<ul>
<li>0 = positive</li>
<li>1 = negative</li>
</ul></li>
<li>5 bits for the exponent
<ul>
<li>00000 is reserved for 0</li>
<li>11111 is reserved for <span class="math inline">\(\infty\)</span></li>
<li>30 exponents left: <span class="math inline">\(2^5 - 2 = 30\)</span></li>
<li>the 16 bit standard dictates that the used exponents are <span class="math inline">\(-14,...,15\)</span>.
<ul>
<li><strong>Note</strong>: <span class="math inline">\(0\)</span> is also included in this list of 30 exponents. This is because the <span class="math inline">\(00000\)</span> representation is reserved for integers, while <span class="math inline">\(01111\)</span> is used with non-integers.</li>
</ul></li>
</ul></li>
<li>11 bit for the significand.
<ul>
<li>10 are actually stored – we always work with normalized FP numbers, i.e. <span class="math inline">\(\beta_0 = 1\)</span>.</li>
</ul></li>
</ul>
<p><strong>Question:</strong> What are smallest and largest positive numbers that can be represented?</p>
<p><strong>Answer:</strong> Smallest non-normalized number would be the one with the smallest possible exponent, and all digits of the significand are 0 except the very last one. So, the smallest non-normalized FP number in the 16 bit standard would be</p>
<p><span class="math display">\[
\left(0 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-9} + 1\cdot 2^{-10}\right)\cdot 2^{-14} = 2^{-24} \approx 5.96\cdot 10^{-8}
\]</span></p>
<p>The smallest normalized number is the one with all digits <span class="math inline">\(0\)</span> (except for the leading digit, of course, which has to be <span class="math inline">\(1\)</span> for it to be normalized), and <span class="math inline">\(e = -14\)</span>. So the smallest normalized FP number:</p>
<p><span class="math display">\[
\left(1 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-10}\right)\cdot 2^{-14} = 2^{-14} \approx 6.10\cdot 10^{-5}
\]</span> Finally, the largest (finite) FP number in the 16 bit standard is the one where the exponent is as large as possible (<span class="math inline">\(e = 15\)</span>), and all digits are <span class="math inline">\(1\)</span>. So</p>
<p><span class="math display">\[
\left(1 + 1\cdot 2^{-1} + ... + 1\cdot 2^{-10}\right)\cdot 2^{15} = 65504
\]</span></p>
</div>
<div id="lecture-2-911" class="section level4 unnumbered">
<h4>Lecture 2: 9/11</h4>
</div>
<div id="the-32-bit-standard-single-precision" class="section level4">
<h4><span class="header-section-number">1.2.1.2</span> The 32 bit standard (single precision)</h4>
<p>The 32 bits of storage are used in the following way, when following the 32 bit standard:</p>
<ul>
<li>1 bit for the sign
<ul>
<li>0 = positive</li>
<li>1 = negative</li>
</ul></li>
<li>8 bits for the exponent
<ul>
<li>00000000 is reserved for 0</li>
<li>11111111 is reserved for <span class="math inline">\(\infty\)</span></li>
<li>exponents left: <span class="math inline">\(2^8 - 2 = 254\)</span></li>
<li>the 32 bit standard dictates that the used exponents are <span class="math inline">\(-126,...,127\)</span>.
<ul>
<li><strong>Note</strong>: <span class="math inline">\(0\)</span> is also included in this list of the 254 exponents. This is because the <span class="math inline">\(00000000\)</span> representation is reserved for integers, while <span class="math inline">\(01111111\)</span> (I think this is the representation for <span class="math inline">\(0\)</span> here…) is used with non-integers.</li>
</ul></li>
</ul></li>
<li>24 bit for the significand.
<ul>
<li>23 are actually stored – we always work with normalized FP numbers, i.e. <span class="math inline">\(\beta_0 = 1\)</span>.</li>
</ul></li>
</ul>
<p><strong>Question:</strong> What are smallest and largest positive numbers that can be represented in the 32 bit standard?</p>
<p><strong>Answer:</strong> Smallest non-normalized number would be the one with the smallest possible exponent, and all digits of the significand are 0 except the very last one. So, the smallest non-normalized FP number in the 32 bit standard would be</p>
<p><span class="math display">\[
\left(0 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-22} + 1\cdot 2^{-23}\right)\cdot 2^{-126} = 2^{-149} \approx 1.40\cdot 10^{-45}
\]</span></p>
<p>The smallest normalized number is the one with all digits <span class="math inline">\(0\)</span> (except for the leading digit, of course, which has to be <span class="math inline">\(1\)</span> for it to be normalized), and <span class="math inline">\(e = -126\)</span>. So the smallest normalized FP number:</p>
<p><span class="math display">\[
\left(1 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-23}\right)\cdot 2^{-126} = 2^{-126} \approx 1.18\cdot 10^{-38}
\]</span> Finally, the largest (finite) FP number in the 32 bit standard is the one where the exponent is as large as possible (<span class="math inline">\(e = 127\)</span>), and all digits are <span class="math inline">\(1\)</span>. So</p>
<p><span class="math display">\[
\left(1 + 1\cdot 2^{-1} + ... + 1\cdot 2^{-126}\right)\cdot 2^{127} = 3.40\cdot 10^{38}
\]</span></p>
</div>
<div id="the-64-bit-standard-double-precision" class="section level4">
<h4><span class="header-section-number">1.2.1.3</span> The 64 bit standard (double precision)</h4>
<p>The 64 bits of storage are used in the following way, when following the 64 bit standard:</p>
<ul>
<li>1 bit for the sign
<ul>
<li>0 = positive</li>
<li>1 = negative</li>
</ul></li>
<li>11 bits for the exponent
<ul>
<li>00000000 is reserved for 0</li>
<li>11111111 is reserved for <span class="math inline">\(\infty\)</span></li>
<li>exponents left: <span class="math inline">\(2^11 - 2 = 2046\)</span></li>
<li>the 64 bit standard dictates that the used exponents are <span class="math inline">\(-1024,...,1023\)</span>.
<ul>
<li><strong>Note</strong>: <span class="math inline">\(0\)</span> is also included in this list of the 254 exponents. This is because the <span class="math inline">\(00000000\)</span> representation is reserved for integers, while <span class="math inline">\(01111111\)</span> (I think this is the representation for <span class="math inline">\(0\)</span> here…) is used with non-integers.</li>
</ul></li>
</ul></li>
<li>53 bit for the significand.
<ul>
<li>52 are actually stored – we always work with normalized FP numbers, i.e. <span class="math inline">\(\beta_0 = 1\)</span>.</li>
</ul></li>
</ul>
</div>
</div>
<div id="errors" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Errors</h3>
<div id="units-in-the-last-place-ulp" class="section level4">
<h4><span class="header-section-number">1.2.2.1</span> Units in the Last Place (ULP)</h4>
</div>
<div id="absolute-and-relative-error" class="section level4">
<h4><span class="header-section-number">1.2.2.2</span> Absolute and Relative Error</h4>
<p>Let <span class="math inline">\(fl: \mathbb{R}_{\geq 0} \rightarrow \mathcal{S}\)</span> be a function that takes a real value and return a FP number. Then we define the absolute and relative error as follows:</p>

<div class="definition">
<p><span id="def:unnamed-chunk-5" class="definition"><strong>Definition 1.3  </strong></span>Let <span class="math inline">\(z \in \mathbb{R}_{\geq 0}\)</span>. The <em>absolute error</em> is defined as</p>
<p><span class="math display">\[
\left | fl(z) - z \right | .
\]</span></p>
<p>The <em>relative error</em> is defined as</p>
<span class="math display">\[
\left | \frac{fl(z)-z}{z} \right |
\]</span>
</div>


<div class="lemma">
<span id="lem:absolute-error-bound" class="lemma"><strong>Lemma 1.1  </strong></span>If <span class="math inline">\(z\)</span> has exponent <span class="math inline">\(e\)</span>, then the maximum absolute error is <span class="math inline">\(\frac{\beta^{e-p+1}}{2}\)</span>.
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> 
</div>


<div class="lemma">
<span id="lem:unnamed-chunk-7" class="lemma"><strong>Lemma 1.2  </strong></span>If <span class="math inline">\(z\)</span> has exponent <span class="math inline">\(e\)</span>, then the maximum relative error is <span class="math inline">\(\frac{\beta^{1-p}}{2}\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> If <span class="math inline">\(z\)</span> has exponent <span class="math inline">\(e\)</span>, then <span class="math inline">\(\beta^{e}\leq z\)</span>. Using this with <a href="lecture-notes.html#lem:absolute-error-bound">1.1</a>, we get that</p>
<span class="math display">\[
\left | \frac{fl(z)-z}{z} \right | \leq \frac{\beta^{e-p+1}}{2\beta^e} = \frac{\beta^{1-p}}{2}.
\]</span>
</div>

<p><strong>Note:</strong> the upper bound of the relative error is called the <em>machine epsilon</em>. This can be obtained in Julia using the function <code>eps</code>.</p>
</div>
<div id="the-fundamental-axiom" class="section level4">
<h4><span class="header-section-number">1.2.2.3</span> The Fundamental Axiom</h4>
<p>… is that for any of the four arithmetic operations (<span class="math inline">\(+, -, \cdot, /\)</span>), we have the following error bound:</p>
<p><span class="math display">\[
fl(x \text{op} y) = (x \text{op} y)(1+\delta),
\]</span></p>
<p>with <span class="math inline">\(|\delta| \leq u\)</span>, where <span class="math inline">\(u\)</span> is commonly <span class="math inline">\(2\cdot \epsilon\)</span>. (<strong>NOTE: NEED TO CLARIFY IF THE ABOVE IS CORRECT!</strong>)</p>
<p>**Example:* Matrix storage. Let <span class="math inline">\(A \in \mathbb{R}^{m\times n}\)</span>. Then:</p>
<p><span class="math display">\[
\left| fl(A) - A \right | \leq u \left | A \right |
\]</span></p>
<p><strong>Example:</strong> Dot product. Let <span class="math inline">\(x,y \in \mathbb{R}^{n}\)</span>. Recall that the dot product of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is definted as <span class="math inline">\(x&#39;y = \sum_{i=1}^{n} x_i \cdot y_i\)</span>. This can be calculated in the following way:</p>
<div class="sourceCode"><pre class="sourceCode julia"><code class="sourceCode julia">fl = <span class="kw">function</span>(x,y)
  <span class="co"># Get length of x</span>
  n = length(x)
  <span class="co"># Check that length of y is equal to length of x. If not, throw error.</span>
  <span class="kw">if</span>(length(y) != n)
    <span class="kw">return</span> <span class="st">&quot;ERROR: y does not have same dimension as x&quot;</span>
  <span class="kw">end</span>

  <span class="co"># s will be the result of the dot product calculation</span>
  s = <span class="fl">0</span>

  <span class="kw">for</span> i = <span class="fl">1</span>:n
    s += x[i]*y[i]
  <span class="kw">end</span>

  <span class="kw">return</span>(s)

<span class="kw">end</span></code></pre></div>
<p>Next we want to prove the following lemma:</p>

<div class="lemma">
<p><span id="lem:unnamed-chunk-10" class="lemma"><strong>Lemma 1.3  </strong></span>Let <span class="math inline">\(x,y \in \mathbb{R}^n\)</span>, and <span class="math inline">\(n\cdot u \leq 0.01\)</span>. Then</p>
<span class="math display">\[
\left | fl(x&#39;y) - x&#39;y \right | \leq 1.01 \cdot n\cdot u \cdot \left|x\right|&#39;\left|y\right|
\]</span>
</div>

</div>
<div id="lecture-3-913" class="section level4 unnumbered">
<h4>Lecture 3: 9/13</h4>
<p>To prove the lemma above, we will need another lemma…</p>

<div class="lemma">
<p><span id="lem:lemForProof" class="lemma"><strong>Lemma 1.4  </strong></span>If <span class="math inline">\(|\delta_i| \leq u, \forall i=1,\ldots,n\)</span> s.t. <span class="math inline">\(n\cdot u &lt; 2\)</span>. Let <span class="math inline">\(1 + \eta = \prod_{i=1}^{n}(1 + \delta_i)\)</span>. Then</p>
<span class="math display">\[
|\eta | \leq \frac{n\cdot u}{1-\tfrac{n\cdot u}{2}}
\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Using the definition of <span class="math inline">\(\nu\)</span>, we can rewrite it to get</p>
<p><span class="math display">\[
| \eta | = \left | \prod_{i=1}^n (1 + \delta_i) - 1 \right |.
\]</span></p>
<p>By induction, we will show that the expression above is less than or equal to <span class="math inline">\((1 + u)^n - 1\)</span>. [TO BE COMPLETED!]</p>
<p>Since <span class="math inline">\(1+u \leq e^{u}\)</span> for all <span class="math inline">\(u \in {\mathbb{R}}\)</span>, we have that</p>
<span class="math display">\[\begin{align*}
  | \eta | &amp;\leq e^{n\cdot u} - 1 \\
          &amp;\leq n\cdot u + \frac{(n\cdot u)^2}{2!} + \frac{(n\cdot u)^3}{3!} + \ldots \text{(used the Taylor expansion)}\\
          &amp;\leq n\cdot u + \frac{(n\cdot u)^2}{2^1} + \frac{(n\cdot u)^3}{2^2} + \frac{(n\cdot u)^4}{2^3} + \ldots (\text{used that } x! &gt; 2^{x-1} \text{ for } x &gt; 1) \\
          &amp;= \sum_{k=0}^{\infty} n\cdot u \left(\frac{n\cdot u}{2}\right)^k \text{\small (identify this as a geometric series with } r = \tfrac{n\cdot u}{2} \text{, \small which is less than 1 by assumption)} \\
          &amp;= \frac{n\cdot u}{1-\tfrac{n\cdot u}{2}},
\end{align*}\]</span>
which is exactly what we wanted.
</div>

<p>With this in hand, we will prove the previously stated lemma.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(s_p\)</span> denote the value of <span class="math inline">\(s\)</span> after the <span class="math inline">\(p\)</span>’th iteration of the algorithm described above. Then, since we’re assuming the Fundamental Axiom, we have that <span class="math inline">\(s_1 = fl(x_1y_y) = x_1 y_1 (1 + \delta_1)\)</span>, where <span class="math inline">\(|\delta_1| \leq u\)</span>. We can similarly find <span class="math inline">\(s_p\)</span> as</p>
<span class="math display">\[\begin{align*}
s_p &amp;= fl(s_{p-1} + fl(x_p y_p)) \\
    &amp;= (s_{p-1} + fl(x_p y_p))(1 + \epsilon_p) (\text{where } |\epsilon_p| \leq u) \\
    &amp;= (s_{p-1} + x_py_p(1 + \delta_p))(1 + \epsilon_p) (\text{where } |\delta_p| \leq u).
\end{align*}\]</span>
<p>Let <span class="math inline">\(\epsilon_1 = 0\)</span>. <span class="math inline">\(s_p\)</span> is a recursive formula, and can be rewritten as follows:</p>
<p><span class="math display">\[
s_p = \sum_{i=1}^p x_iy_i (1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j).
\]</span></p>
<p>So,</p>
<span class="math display">\[\begin{align*}
  | s_n - x^\prime y |  &amp;= \left | \sum_{i=1}^n (x_i y_i)(1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - \sum_{i=1}^{n}x_iy_i\right | \\
                        &amp;= \left | \sum_{i=1}^n (x_i y_i)\left((1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - 1 \right) \right | \\
                        &amp;\leq \sum_{i=1}^n \left |x_i y_i \right |\left | (1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - 1 \right| .
\end{align*}\]</span>
<p>We now use <a href="lecture-notes.html#lem:lemForProof">1.4</a> to get:</p>
<span class="math display">\[\begin{align*}
\sum_{i=1}^n \left |x_i y_i \right |\left | (1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - 1 \right| &amp;\leq  \frac{nu}{1-\tfrac{nu}{2}} \sum_{i=1}^n \left |x_i y_i \right | \\
&amp;\leq \frac{nu}{0.995} \sum_{i=1}^n |x_i| |y_i | \\
&amp;\leq 1.01 \cdot nu \cdot |x|^\prime |y|
\end{align*}\]</span>
</div>

</div>
</div>
<div id="square-linear-systems" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Square Linear Systems</h3>
<p>In the following, let <span class="math inline">\(A \in {\mathbb{R}}^{n \times m}\)</span> be an invertible matrix, and assume <span class="math inline">\(Ax = b\)</span> for a <span class="math inline">\(b \neq 0\)</span>. This implies that <span class="math inline">\(x = A^{-1}b\)</span>.</p>

<div class="theorem">
<p><span id="thm:kappa-bound" class="theorem"><strong>Theorem 1.1  </strong></span>Let <span class="math inline">\(\kappa_\infty = {\left \vert \left \vert A \right \vert \right \vert}_\infty {\left \vert \left \vert A^{-1} \right \vert \right \vert}_\infty\)</span>. Assume we can store <span class="math inline">\(A\)</span> with precision <span class="math inline">\(E\)</span> (i.e. as <span class="math inline">\(A+E\)</span>), where <span class="math inline">\({\left \vert \left \vert E \right \vert \right \vert}_\infty \leq u {\left \vert \left \vert A \right \vert \right \vert}_\infty\)</span>, and <span class="math inline">\(b\)</span> with precision <span class="math inline">\(e\)</span> (i.e. as <span class="math inline">\(b+e\)</span>), where <span class="math inline">\({\left \vert \left \vert e \right \vert \right \vert}_\infty \leq u {\left \vert \left \vert b \right \vert \right \vert}_\infty\)</span>.</p>
<p>If <span class="math inline">\({\left \vert \left \vert A+E \right \vert \right \vert}\hat{x} = b+e\)</span> and <span class="math inline">\(u\cdot \kappa_\infty &lt; 1\)</span>, then</p>
<span class="math display">\[
  \frac{{\left \vert \left \vert x-\hat{x} \right \vert \right \vert}_\infty}{{\left \vert \left \vert x \right \vert \right \vert}} \leq \frac{2\cdot u \cdot \kappa_\infty}{1-u\cdot \kappa_\infty}
\]</span>
</div>


<div class="lemma">
<p><span id="lem:proofHW" class="lemma"><strong>Lemma 1.5  </strong></span>Let <span class="math inline">\(I\in {\mathbb{R}}^{n \times n}\)</span> be the identity matrix, and <span class="math inline">\(F \in {\mathbb{R}}^{n\times n}\)</span> s.t. <span class="math inline">\({\left \vert \left \vert F \right \vert \right \vert}_p &lt; 1\)</span> for some <span class="math inline">\(p \in [1,\infty]\)</span>. Then <span class="math inline">\(I-F\)</span> is invertible, and</p>
<span class="math display">\[
  {\left \vert \left \vert (I-F)^{-1} \right \vert \right \vert}_p \leq \frac{1}{1-{\left \vert \left \vert F \right \vert \right \vert}_p}
\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> <strong>HOMEWORK</strong>
</div>


<div class="lemma">
<p><span id="lem:unnamed-chunk-14" class="lemma"><strong>Lemma 1.6  </strong></span>Suppose <span class="math inline">\(\exists \epsilon &gt; 0\)</span> s.t. <span class="math inline">\({\left \vert \left \vert \Delta A \right \vert \right \vert} \leq \epsilon {\left \vert \left \vert A \right \vert \right \vert}\)</span> and <span class="math inline">\({\left \vert \left \vert \Delta b \right \vert \right \vert} \leq \epsilon {\left \vert \left \vert b \right \vert \right \vert}\)</span>, and <span class="math inline">\(y\)</span> s.t. <span class="math inline">\((A+\Delta A)y = b+\Delta b\)</span>.</p>
If <span class="math inline">\(\epsilon {\left \vert \left \vert A \right \vert \right \vert}{\left \vert \left \vert A^{-1} \right \vert \right \vert} = r &lt; 1\)</span>, then <span class="math inline">\(A+\Delta A\)</span> is invertible and <span class="math display">\[\frac{{\left \vert \left \vert y \right \vert \right \vert}}{{\left \vert \left \vert x \right \vert \right \vert}} \leq \frac{1+r}{1-r}.\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that <span class="math inline">\(A+\Delta A = A\left (I + A^{-1}\Delta A\right ) = A\left (I - \left(- A^{-1}\Delta A\right)\right)\)</span>. Since <span class="math inline">\({\left \vert \left \vert -A^{-1}\Delta A \right \vert \right \vert} = {\left \vert \left \vert A^{-1}\Delta A \right \vert \right \vert} \leq \epsilon {\left \vert \left \vert A^{-1} \right \vert \right \vert}\cdot {\left \vert \left \vert A \right \vert \right \vert} &lt; 1\)</span> (by assumptions), Lemma <a href="lecture-notes.html#lem:proofHW">1.5</a> gives us that <span class="math inline">\(I + A^{-1}\Delta A\)</span> is invertible. Since <span class="math inline">\(A\)</span> is also invertible (again, by assumption), <span class="math inline">\(A+\Delta A\)</span> is invertible (product of two invertible matrices is invertible).</p>
<p>Performing some linear algebra:</p>
<span class="math display">\[\begin{align*}
  (A + \Delta A) &amp;=  b + \Delta b \Leftrightarrow \\
  A(I + A^{-1} \Delta A) y &amp;= b + \Delta b \Leftrightarrow \\
  (I + A^{-1} \Delta A) y &amp;= A^{-1}b + A^{-1}\Delta b \Leftrightarrow \\
  y &amp;= (I + A^{-1} \Delta A)^{-1} A^{-1}b + A^{-1}\Delta b.
\end{align*}\]</span>
<p>Remember that <span class="math inline">\(A^{-1}b = x\)</span>. From the definition of <span class="math inline">\(r\)</span> we have that <span class="math inline">\({\left \vert \left \vert A^{-1} \right \vert \right \vert} = \frac{r}{{\left \vert \left \vert A \right \vert \right \vert}}\)</span>. These two identities with the assumption that <span class="math inline">\({\left \vert \left \vert \Delta b \right \vert \right \vert} \leq \epsilon b\)</span> gives us</p>
<p><span class="math display">\[
\begin{aligned}
  {\left \vert \left \vert y \right \vert \right \vert} &amp;\leq {\left \vert \left \vert (I + A^{-1}\Delta A)^{-1} \right \vert \right \vert} \left( {\left \vert \left \vert x \right \vert \right \vert} + {\left \vert \left \vert A^{-1}\Delta b \right \vert \right \vert}\right) \\
  &amp;\leq \frac{1}{1-{\left \vert \left \vert A^{-1}\Delta A \right \vert \right \vert}} \left( {\left \vert \left \vert x \right \vert \right \vert} + \frac{r}{\epsilon {\left \vert \left \vert A \right \vert \right \vert}} \cdot {\left \vert \left \vert \Delta b \right \vert \right \vert} \right) \\
  &amp;\leq \frac{1}{1-r} \left( {\left \vert \left \vert x \right \vert \right \vert} + \frac{r}{\epsilon {\left \vert \left \vert A \right \vert \right \vert}} \cdot \epsilon {\left \vert \left \vert b \right \vert \right \vert} \right) \\
  &amp;= \frac{1}{1-r} \left( {\left \vert \left \vert x \right \vert \right \vert} + \frac{r \cdot {\left \vert \left \vert b \right \vert \right \vert}}{{\left \vert \left \vert A \right \vert \right \vert}} \right).
\end{aligned}
\]</span></p>
<p>Finally, recall that <span class="math inline">\(Ax=b\)</span>, hence <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}\cdot{\left \vert \left \vert x \right \vert \right \vert} \geq {\left \vert \left \vert b \right \vert \right \vert}\)</span>, so <span class="math inline">\({\left \vert \left \vert x \right \vert \right \vert} \geq \frac{{\left \vert \left \vert b \right \vert \right \vert}}{{\left \vert \left \vert A \right \vert \right \vert}}\)</span>. So,</p>
<p><span class="math display">\[
\begin{aligned}
  {\left \vert \left \vert y \right \vert \right \vert} \leq \frac{1}{1-r}  \left( {\left \vert \left \vert x \right \vert \right \vert} + r \cdot {\left \vert \left \vert x \right \vert \right \vert} \right) \Leftrightarrow \\
  \frac{{\left \vert \left \vert y \right \vert \right \vert}}{{\left \vert \left \vert x \right \vert \right \vert}} \leq \frac{1+r}{1-r}.
\end{aligned}
\]</span></p>
</div>


<div class="lemma">
<span id="lem:unnamed-chunk-16" class="lemma"><strong>Lemma 1.7  </strong></span><span class="math display">\[\frac{{\left \vert \left \vert y - x \right \vert \right \vert}}{{\left \vert \left \vert x \right \vert \right \vert}} \leq \frac{2\epsilon {\left \vert \left \vert A^{-1} \right \vert \right \vert}\cdot {\left \vert \left \vert A \right \vert \right \vert}}{1-r}.\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span>  <span class="math display">\[
\begin{aligned}
  \left(A+\Delta A\right)y &amp;= b + \Delta b \Leftrightarrow \\
  Ay - b &amp;= \Delta b - \Delta Ay \Leftrightarrow \\
  y - A^{-1}b &amp;= A^{-1} \Delta b - A^{-1}\Delta A y \Leftrightarrow \\
  y - x &amp;= A^{-1} \Delta b - A^{-1}\Delta A y \Leftrightarrow \\
  {\left \vert \left \vert y - x \right \vert \right \vert} &amp;\leq {\left \vert \left \vert A^{-1} \right \vert \right \vert} {\left \vert \left \vert \Delta b \right \vert \right \vert} + {\left \vert \left \vert A^{-1} \right \vert \right \vert} {\left \vert \left \vert \Delta A \right \vert \right \vert}{\left \vert \left \vert y \right \vert \right \vert} \\   &amp;\leq {\left \vert \left \vert A^{-1} \right \vert \right \vert} \epsilon {\left \vert \left \vert b \right \vert \right \vert} + {\left \vert \left \vert A^{-1} \right \vert \right \vert} \epsilon{\left \vert \left \vert A \right \vert \right \vert}{\left \vert \left \vert y \right \vert \right \vert} \\   &amp;\leq \epsilon {\left \vert \left \vert A^{-1} \right \vert \right \vert} {\left \vert \left \vert A \right \vert \right \vert}{\left \vert \left \vert x \right \vert \right \vert} + \epsilon{\left \vert \left \vert A^{-1} \right \vert \right \vert}{\left \vert \left \vert A \right \vert \right \vert}{\left \vert \left \vert y \right \vert \right \vert} \\   &amp;\leq \epsilon {\left \vert \left \vert A^{-1} \right \vert \right \vert} {\left \vert \left \vert A \right \vert \right \vert}\left({\left \vert \left \vert x \right \vert \right \vert} + {\left \vert \left \vert y \right \vert \right \vert}\right) \\
  &amp;= \epsilon {\left \vert \left \vert A^{-1} \right \vert \right \vert} {\left \vert \left \vert A \right \vert \right \vert}\left({\left \vert \left \vert x \right \vert \right \vert} + \frac{1+r}{1-r}{\left \vert \left \vert x \right \vert \right \vert}\right) \Leftrightarrow \\
\frac{{\left \vert \left \vert y-x \right \vert \right \vert}}{{\left \vert \left \vert x \right \vert \right \vert}} &amp;\leq \epsilon {\left \vert \left \vert A^{-1} \right \vert \right \vert} {\left \vert \left \vert A \right \vert \right \vert} \left(\frac{1-r}{1-r} + \frac{1+r}{1-r}\right) \\
  &amp;= 2\epsilon {\left \vert \left \vert A^{-1} \right \vert \right \vert} {\left \vert \left \vert A \right \vert \right \vert} \frac{1}{1-r}
\end{aligned}
\]</span></p>
</div>

</div>
</div>
<div id="orthogonalization" class="section level2">
<h2><span class="header-section-number">1.3</span> Orthogonalization</h2>
<p>Goals</p>
<ol style="list-style-type: decimal">
<li>Introduce and prove the existence of QR decomposition</li>
<li>Overview of the algorithm to perfor QR decomposition</li>
<li>Solve least squares problems</li>
<li>“Large” data problems</li>
</ol>
<p>Outline</p>
<ol style="list-style-type: decimal">
<li>Motivating problems and solutions with QR</li>
<li>Gram-Schmidt procedure, existence of QR</li>
<li>Householder, Givens</li>
<li>“Large” least squares problems datadown</li>
</ol>
<div id="motivating-problems" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Motivating problems</h3>

<div class="example">
<span id="exm:linear-system" class="example"><strong>Example 1.2  (Motivating Problem 1 (Consistent Linear System))  </strong></span>Assume <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}, n \geq m, \text{rank}(A) = m\)</span>, and <span class="math inline">\(b \in \text{range}(A) \subset R^m\)</span>. Find <span class="math inline">\(x \in {\mathbb{R}}^m\)</span> s.t. <span class="math inline">\(Ax = b\)</span>.
</div>


<div class="example">
<span id="exm:least-squares" class="example"><strong>Example 1.3  (Motivating Problem 2 (Least Squares Regression))  </strong></span>Assume <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}, n \geq m, \text{rank}(A) = m\)</span>, and <span class="math inline">\(b \in R^n\)</span>. Find <span class="math inline">\(x \in {\mathbb{R}}^m\)</span> s.t. <span class="math display">\[x \in {\text{argmin}}_{y \in {\mathbb{R}}^m} {\left \vert \left \vert Ay-b \right \vert \right \vert}_2.\]</span>
</div>


<div class="example">
<p><span id="exm:und-linear-system" class="example"><strong>Example 1.4  (Motivating Problem 3 (Underdetermined Linear System)  </strong></span>Assume <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}, n \geq m, \text{rank}(A) &lt; m\)</span>, and <span class="math inline">\(b \in \text{range}(A)\)</span>. Find <span class="math inline">\(x \in {\mathbb{R}}^m\)</span> s.t.</p>
<span class="math display">\[x \in {\text{argmin}}_{y \in {\mathbb{R}}^m} \left\{ {\left \vert \left \vert y \right \vert \right \vert}_2 \left | Ay = b \right\} \right . .\]</span>
</div>


<div class="example">
<p><span id="exm:und-least-squares" class="example"><strong>Example 1.5  (Motivating Problem 4 (Underdetermined Least Squares Regression))  </strong></span>Assume <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}, n \geq m, \text{rank}(A) &lt; m\)</span>, and <span class="math inline">\(b \in {\mathbb{R}}^n\)</span>. Find <span class="math inline">\(x \in {\mathbb{R}}^m\)</span> s.t.</p>
<span class="math display">\[x \in {\text{argmin}}_{z \in {\mathbb{R}}^m} \left\{ {\left \vert \left \vert z \right \vert \right \vert}_2 \left | {\left \vert \left \vert Ay - b \right \vert \right \vert}_2 = \min_{y \in {\mathbb{R}}^m} {\left \vert \left \vert Ay-b \right \vert \right \vert}_2 \right\} \right . .\]</span>
</div>


<div class="example">
<p><span id="exm:constrained-least-squares" class="example"><strong>Example 1.6  (Motivating Problem 5 (Constrained Least Squares Regression))  </strong></span>Assume <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}, n \geq m, \text{rank}(A) &lt; m\)</span>, and <span class="math inline">\(b \in {\mathbb{R}}^n\)</span>. Let <span class="math inline">\(C \in {\mathbb{R}}^{p\times m}, \text{C} = p\)</span>, and <span class="math inline">\(d \in {\mathbb{R}}^{p}\)</span>. Find <span class="math inline">\(x \in {\mathbb{R}}^m\)</span> s.t.</p>
<span class="math display">\[x = {\text{argmin}}_{y \in {\mathbb{R}}^m} {\left \vert \left \vert Ay - b \right \vert \right \vert}_2\quad \text{s.t.} \quad Cy = d.\]</span>
</div>

<p>Before we take a crack at solving these problems, we will need to get some definitions down.</p>

<div class="definition">
<span id="def:perm-matrix" class="definition"><strong>Definition 1.4  (Permutation Matrix)  </strong></span>A permutation matrix is a square matrix such that each column has exactly one element that is <span class="math inline">\(1\)</span>, the rest are <span class="math inline">\(0\)</span>.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-18" class="example"><strong>Example 1.7  </strong></span>The following is a permutation matrix:</p>
<p><span class="math display">\[
  \begin{bmatrix}
    0 &amp; 1 \\
    1 &amp; 0
  \end{bmatrix}
\]</span></p>
</div>


<div class="definition">
<span id="def:orthogonal" class="definition"><strong>Definition 1.5  (Orthogonal Matrix)  </strong></span>A matrix <span class="math inline">\(Q\)</span> is said to be an <em>orthogonal matrix</em> if <span class="math inline">\(Q^T Q = Q Q^T = I\)</span>.
</div>

<p>Note: for an orthogonal matrix <span class="math inline">\(Q \in {\mathbb{R}}^{n\times m}\)</span>, it holds that <span class="math inline">\({\left \vert \left \vert Q_{i*} \right \vert \right \vert}_2 = 1\)</span> for all <span class="math inline">\(i=1,\ldots,n\)</span>, and <span class="math inline">\({\left \vert \left \vert Q_{*j} \right \vert \right \vert}_2 = 1\)</span> for all <span class="math inline">\(j = 1,\ldots, m\)</span>.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>

<div class="definition">
<span id="def:unnamed-chunk-19" class="definition"><strong>Definition 1.6  (Upper Triangular Matrix)  </strong></span>A matrix <span class="math inline">\(R\)</span> is an <em>upper triangular matrix</em> if <span class="math inline">\(R_{ij} = 0\)</span> for all <span class="math inline">\(i&gt;j\)</span>.
</div>

</div>
<div id="lecture-4-918" class="section level3 unnumbered">
<h3>Lecture 4: 9/18</h3>
</div>
<div id="qr-decomposition" class="section level3">
<h3><span class="header-section-number">1.3.2</span> QR Decomposition</h3>
<p>In order to actually solve the problems listed above, we need the QR Decomposition:</p>

<div class="theorem">
<p><span id="thm:qr-decomposition" class="theorem"><strong>Theorem 1.2  (Existence of QR Decomposition)  </strong></span>Let <span class="math inline">\(A \in {\mathbb{R}}^{n \times m}\)</span> and let <span class="math inline">\(r = rank(A)\)</span>. Then there exists:</p>
<ol style="list-style-type: decimal">
<li>an <span class="math inline">\(m\times m\)</span> permutation matrix <span class="math inline">\(\Pi\)</span>,</li>
<li>an <span class="math inline">\(n \times n\)</span> orthogonal matrix <span class="math inline">\(Q\)</span>,</li>
<li>an <span class="math inline">\(r \times r\)</span> upper triangular matrix <span class="math inline">\(R\)</span>, with non-zero diagonal elements (i.e. invertible)</li>
<li>an <span class="math inline">\(r \times (m-r)\)</span> matrix S (if <span class="math inline">\(m &gt; r\)</span>),</li>
</ol>
<p>such that</p>
<p><span class="math display">\[
  A = Q \begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix} \Pi^T.
\]</span></p>
</div>

<p>With this in hand, we can solve the motivating problems stated above.</p>

<div class="solution">
<p> <span class="solution"><em>Solution</em> (Example <a href="lecture-notes.html#exm:linear-system">1.2</a>). </span>  We want to find <span class="math inline">\(x\)</span> such that <span class="math inline">\(Ax = b\)</span>.</p>
<p>We use theorem <a href="lecture-notes.html#thm:qr-decomposition">1.2</a> to rewrite this as <span class="math inline">\(Q \begin{bmatrix} R \\ 0 \end{bmatrix} \Pi^T x = b\)</span>. Note that since <span class="math inline">\(\text{rank}(A) = m\)</span>, there is no <span class="math inline">\(S\)</span> matrix.</p>
<p>Now, since <span class="math inline">\(Q\)</span> is an orthogonal matrix, we know that <span class="math inline">\(Q^{-1} = Q^T\)</span>, so</p>
<span class="math display" id="eq:sol-linear-system">\[\begin{equation}
  \begin{bmatrix} R \\ 0 \end{bmatrix} \Pi^T x = Q^T b = c = \begin{bmatrix} c_1 \\ 0 \end{bmatrix}. \tag{1.1}
\end{equation}\]</span>
<p>So now the equation we are trying to solve becomes</p>
<p><span class="math display">\[
  R \Pi^T x = c_1.
\]</span></p>
<p>Since <span class="math inline">\(R\)</span> is an upper triangular matrix with non-zero diagonal elements, it is invertible. Since <span class="math inline">\(\Pi\)</span> is a permutation matrix, <span class="math inline">\(\Pi^{-1} = \Pi^T\)</span>. Using this we can find the solution:</p>
<p><span class="math display">\[
  x = \Pi R^{-1} c_1.
\]</span></p>
</div>


<div class="solution">
<p> <span class="solution"><em>Solution</em> (Example <a href="lecture-notes.html#exm:least-squares">1.3</a>). </span> We want to find <span class="math inline">\(x\)</span> such that <span class="math inline">\(x \in {\text{argmin}}_{y \in {\mathbb{R}}^m}{\left \vert \left \vert Ay-b \right \vert \right \vert}_2\)</span>.</p>
<p>Once again, <span class="math inline">\(\text{rank}(A) = m\)</span>, so using theorem <a href="lecture-notes.html#thm:qr-decomposition">1.2</a>, we can rewrite the expression we are trying to minimize as</p>
<p><span class="math display" id="eq:least-squares-eq1">\[
  \min {\left \vert \left \vert Q \begin{bmatrix} R \\ 0 \end{bmatrix}\Pi^T y - b \right \vert \right \vert}_2. \tag{1.2}
\]</span></p>
<p>Since <span class="math inline">\(Q^T = Q^{-1}\)</span> is orthogonal, <span class="math inline">\({\left \vert \left \vert Q^T x \right \vert \right \vert}_2 = {\left \vert \left \vert x \right \vert \right \vert}_2\)</span> for all <span class="math inline">\(x\)</span> (homework exercise <a href="homework-assignments.html#exr:q402">2.28</a>). So, we get that <a href="lecture-notes.html#eq:least-squares-eq1">(1.2)</a> is the same as</p>
<p><span class="math display">\[
  \min{\left \vert \left \vert \begin{bmatrix} R \\ 0 \end{bmatrix} \Pi^T y - Q^T b \right \vert \right \vert}_2.
\]</span></p>
<p>Now let <span class="math inline">\(c = Q^T b\)</span>. Then, <span class="math inline">\(c\)</span> is of the form <span class="math inline">\(\begin{bmatrix} c_1 \\ c_2 \end{bmatrix}\)</span>, where <span class="math inline">\(c_2\)</span> is the last <span class="math inline">\(n-r\)</span> rows (i.e. corresponding to the <span class="math inline">\(0\)</span> rows of <span class="math inline">\(\begin{bmatrix} R \\ 0 \end{bmatrix}\)</span>). Then</p>
<p><span class="math display">\[
  \min{\left \vert \left \vert \begin{bmatrix} R \Pi^T y - c_1 \\ -c_2 \end{bmatrix} \right \vert \right \vert}_2 = \min \sqrt{{\left \vert \left \vert R \Pi^T y - c_1 \right \vert \right \vert}_2^2 + {\left \vert \left \vert c_2 \right \vert \right \vert}_2^2}.
\]</span></p>
<p>Now this is minimized by <span class="math inline">\({\text{argmin}}_y {\left \vert \left \vert R \Pi^T y - c_1 \right \vert \right \vert}_2^2\)</span>. As before, <span class="math inline">\(R^{-1}\)</span> exists since <span class="math inline">\(R\)</span> is upper triangular with non-zero diagonal elements, <span class="math inline">\(\Pi^T = \Pi^{-1}\)</span> since <span class="math inline">\(\Pi\)</span> is a permutation matrix, so</p>
<p><span class="math display">\[
  \begin{aligned}
    x &amp;= {\text{argmin}}_y {\left \vert \left \vert R \Pi^T y - c_1 \right \vert \right \vert}_2^2 \Leftrightarrow \\
    R \Pi^T x &amp;= c_1 \Leftrightarrow \\
    x &amp;= \Pi R^{-1} c_1.
  \end{aligned}
\]</span></p>
</div>


<div class="solution">
<p> <span class="solution"><em>Solution</em> (Example <a href="lecture-notes.html#exm:und-linear-system">1.4</a>). </span>  In this scenario, <span class="math inline">\(\text{rank}(A) = r &lt; m\)</span>. We are looking for <span class="math inline">\(x \in {\text{argmin}}_{y}\left\{{\left \vert \left \vert y \right \vert \right \vert}_2 \left | Ay = b\right\}\right .\)</span>. Using theorem <a href="lecture-notes.html#thm:qr-decomposition">1.2</a>, we can rewrite this as <span class="math inline">\({\text{argmin}}_y\left\{{\left \vert \left \vert y \right \vert \right \vert}_2 | Q\begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix} y = b \right\}\)</span>, and multiplying by <span class="math inline">\(Q^T\)</span>, <span class="math inline">\({\text{argmin}}_y\left\{{\left \vert \left \vert y \right \vert \right \vert}_2 \left | \begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix} y = Q^T b \right\} \right .\)</span>. We introduce the vector <span class="math inline">\(c\)</span> such that <span class="math inline">\(Q^T b = \begin{bmatrix} c &amp; 0 \end{bmatrix}^T\)</span> (<span class="math inline">\(0\)</span> entries correspond to <span class="math inline">\(0\)</span> rows in <span class="math inline">\(\begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix}\)</span>). If we furthermore write <span class="math inline">\(\Pi^T y\)</span> as <span class="math inline">\(\begin{bmatrix} z_1 &amp; z_2 \end{bmatrix}^T\)</span>.</p>
<p>Then, since <span class="math inline">\({\left \vert \left \vert y \right \vert \right \vert}_2 = {\left \vert \left \vert z \right \vert \right \vert}_2\)</span>, our problem becomes</p>
<p><span class="math display">\[
  \begin{aligned}
    x &amp;\in {\text{argmin}}_z \left\{{\left \vert \left \vert z \right \vert \right \vert}_2 \left | R z_1 + S z_2 = c \right\}\right. \\
    x &amp;\in {\text{argmin}}_z \left\{{\left \vert \left \vert z \right \vert \right \vert}_2 \left | z_1 = R^{-1} c - R^{-1} S z_2 \right\}\right. \\
    x &amp;\in {\text{argmin}}_z \sqrt{{\left \vert \left \vert R^{-1}c - R^{-1} S z_2 \right \vert \right \vert}_2^2 + {\left \vert \left \vert z_2 \right \vert \right \vert}_2^2} \\
    x &amp;\in {\text{argmin}}_z \left\{{\left \vert \left \vert R^{-1}c - R^{-1} S z_2 \right \vert \right \vert}_2^2 + {\left \vert \left \vert z_2 \right \vert \right \vert}_2^2\right\},
  \end{aligned}
\]</span></p>
<p>where the last equality is a consequence of the result proved in homework <a href="homework-assignments.html#exr:q403">2.29</a>. Now, let <span class="math inline">\(d = R^{-1}c\)</span> and <span class="math inline">\(P = R^{-1}S\)</span>. Then we can find the minimum of the above expression by differentiating and setting equal to zero:</p>
<span class="math display">\[\begin{align}
  0 &amp;= -P^Td + (P^TP + I)z_2 \rightarrow \\
  z_2 &amp;= (P^T P + I)^{-1}P^Td.
\end{align}\]</span>
</div>


<div class="solution">
<p> <span class="solution"><em>Solution</em> (Example <a href="lecture-notes.html#exm:und-least-squares">1.5</a>). </span>  We want to find <span class="math inline">\(\min_z \left\{ {\left \vert \left \vert z \right \vert \right \vert}_2 \left \vert z \in {\text{argmin}}_y {\left \vert \left \vert Ay - b \right \vert \right \vert}_2\right\} \right .\)</span> Use theorem <a href="lecture-notes.html#thm:qr-decomposition">1.2</a>:</p>
<p><span class="math display">\[\begin{aligned}
  \min_z \left\{ {\left \vert \left \vert z \right \vert \right \vert}_2 \left \vert z \in {\text{argmin}}_y {\left \vert \left \vert Ay - b \right \vert \right \vert}_2\right\} \right . &amp;= \left\{ {\left \vert \left \vert z \right \vert \right \vert}_2 \left \vert z \in {\text{argmin}}_y {\left \vert \left \vert \begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix} \Pi^T y - Q^T b \right \vert \right \vert}_2\right\} \right . \\
  &amp;= \left\{ {\left \vert \left \vert w \right \vert \right \vert}_2 \left \vert w \in {\text{argmin}}_y {\left \vert \left \vert \begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix}  y - Q^T b \right \vert \right \vert}_2\right\} \right .,
\end{aligned}\]</span></p>
<p>since <span class="math inline">\({\left \vert \left \vert y \right \vert \right \vert}_2 = {\left \vert \left \vert \Pi^T y \right \vert \right \vert}_2\)</span>. This is exactly the problem solved in example <a href="lecture-notes.html#exm:und-linear-system">1.4</a>. In conclusion,</p>
<p><span class="math display">\[
  w = \begin{bmatrix} R^{-1}(c_1 - Sy_y) \\ y_2 \end{bmatrix}.
\]</span></p>
</div>


<div class="solution">
 <span class="solution"><em>Solution</em> (Example <a href="lecture-notes.html#exm:constrained-least-squares">1.6</a>). </span> 
</div>

</div>
<div id="existence-of-qr-decomposition." class="section level3">
<h3><span class="header-section-number">1.3.3</span> Existence of QR-decomposition.</h3>
<p>To prove the existence of the QR-decomposition, we need the Gram-Schmidt process.</p>

<div class="lemma">
<p><span id="lem:gsp" class="lemma"><strong>Lemma 1.8  (The Gram-Schmidt Process)  </strong></span>Let <span class="math inline">\(r \in {\mathbb{N}}\)</span>. Given a set of linearly independent vectors <span class="math inline">\(\{a_1, \ldots, a_r\}\)</span>, there exists a set of orthonormal vectors <span class="math inline">\(\{q_1, \ldots, q_r\}\)</span> such that <span class="math inline">\(\text{span} \{q_1, \ldots, q_r\} = \text{span}\{a_1, \ldots, a_r\}\)</span>.</p>
<p>The <span class="math inline">\(q_i\)</span>’s are given by…</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We will prove this by induction. For <span class="math inline">\(i=1\)</span>: let <span class="math inline">\(R_{11} = {\left \vert \left \vert a_1 \right \vert \right \vert}_2\)</span>, <span class="math inline">\(q_1 = \frac{1}{R_11}a_1\)</span>. Notice that <span class="math inline">\({\left \vert \left \vert q_1 \right \vert \right \vert} = 1\)</span>.</p>
<p>(At this point, it might be beneficial to check out the intuitive side note (<a href="lecture-notes.html#cnj:gram-schmidt-remark">1.1</a>))</p>
<p>Define <span class="math inline">\(q^r\)</span> in the following way: let <span class="math inline">\(R_{ir} = q_i^\prime a_r\)</span>, <span class="math inline">\(\tilde{q}_r = a_r - \sum_{i=1}^{r-1} R_{ir}q_i\)</span>, and <span class="math inline">\(R_{rr} = {\left \vert \left \vert \tilde{q}_r \right \vert \right \vert}_2\)</span>. Then <span class="math inline">\(q_r = \frac{\tilde{q}_r}{R_{rr}}\)</span>. (Note: <span class="math inline">\(\tilde{q}_r \neq 0\)</span> since the <span class="math inline">\(a_i\)</span>s are linearly independent, and <span class="math inline">\(q_i\)</span> is given as a linear combination of <span class="math inline">\(a_1, \ldots, a_i\)</span>.)</p>
<p>Assume the result holds for <span class="math inline">\(i \le r-1\)</span>. I.e. we have vectors <span class="math inline">\(q_1, \ldots, q_{r-1}\)</span> given as above, and that</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\text{span}\{q_1, \ldots, q_{r-1}\} = \text{span}\{a_1, \ldots, a_{r-1}\}\)</span>,</li>
<li><span class="math inline">\(q_i \cdot q_j\)</span> for all <span class="math inline">\(i,j = 1, \ldots, r-1\)</span> with <span class="math inline">\(i \neq j\)</span>,</li>
<li><span class="math inline">\(q_i^\prime \cdot q_i = 1\)</span> for all <span class="math inline">\(i = 1, \ldots, r-1\)</span>.</li>
</ol>
<p>Now, we want to show that we can construct a <span class="math inline">\(q_r\)</span> such that</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(\text{span}\{q_1, \ldots, q_r\} = \text{span}\{a_1, \ldots, a_r\}\)</span>,</li>
<li><span class="math inline">\(q_r \cdot q_j = 0\)</span> for all <span class="math inline">\(j = 1, \ldots, r-1\)</span>,</li>
<li><span class="math inline">\(q_r^\prime \cdot q_r = 1\)</span>.</li>
</ol>
<p>We start from below.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>By definition of <span class="math inline">\(q_r\)</span>: <span class="math inline">\(q_r^\prime q_r = \frac{\tilde{q}_r^\prime \tilde{q}_r}{R_{rr}^2} = \frac{{\left \vert \left \vert \tilde{q}_r \right \vert \right \vert}^2}{R_{rr}^2} = 1\)</span>.</li>
<li>Let <span class="math inline">\(i &lt; r\)</span>. Then</li>
</ol>
<p><span class="math display">\[\begin{aligned}
  q_i^{\prime} \tilde{q}_r &amp;= q_i^\prime a_r - \sum_{j=1}^{r-1} R_{jr} q_i^\prime q_j \\
                           &amp;= q_i^\prime a_r - R_{ir} q_i^\prime q_i \\
                           &amp;= q_i^\prime a_r - R_{ir} = 0 \text{ (by definition of } R_{ir}\text{)}.
\end{aligned}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>We need to show that <span class="math inline">\(a_r\)</span> can be written as a linear combination of <span class="math inline">\(q_i\)</span>s.</li>
</ol>
<p><span class="math display">\[\begin{aligned}
  \sum_{i=1}^r R_{ir} q_i &amp;= \sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} q_r \\
                          &amp;= \sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} \frac{1}{R_{rr}} \tilde{q}_r \\
                          &amp;= \sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} \frac{1}{R_{rr}} \left(a_r - \sum_{i=1}^{r-1} R_{ir}q_i \right) \\
                          &amp;= \sum_{i=1}^{r-1} R_{ir} q_i + a_r - \sum_{i=1}^{r-1} R_{ir}q_i \\
                          &amp;= a_r.
\end{aligned}\]</span></p>
</div>


<div class="conjecture">
<p><span id="cnj:gram-schmidt-remark" class="conjecture"><strong>Remark 1.1  (Intuitive side note)  </strong></span>It is fairly easy to find <span class="math inline">\(q_2\)</span>. We want to find it such that <span class="math inline">\(a_2 = R_{12}q_1 + R_{22} q_2\)</span>, and <span class="math inline">\({\left \vert \left \vert q_2 \right \vert \right \vert}_2 = 1\)</span> and <span class="math inline">\(q_1 \perp q_2\)</span>, i.e. <span class="math inline">\(q_1 \cdot q_2 = 0\)</span>. So, if we multiply the equation by <span class="math inline">\(q_1\)</span>, we get that <span class="math inline">\(q_1 a_2 = R_{12}\)</span>. Substituting this into the first equation, <span class="math inline">\(q_2 = \frac{a_2 - R_{12} q_1}{R_{22}}\)</span>.</p>
Note that this is a circular argument, and hence not a formal way of doing this.
</div>

</div>
<div id="lecture-5-920" class="section level3 unnumbered">
<h3>Lecture 5: 9/20</h3>
<p>(Finished up proof of The Gram-Schmidt Process (<a href="lecture-notes.html#lem:gsp">1.8</a>))</p>

<div class="conjecture">
<p><span id="cnj:unnamed-chunk-26" class="conjecture"><strong>Remark 1.2  (Gram-Schmidt in Matrix Form)  </strong></span>If we write up <span class="math inline">\(a_1, \ldots, a_r\)</span> in a matrix, we see that</p>
<p><span class="math display">\[
  \begin{bmatrix}
    a_1 &amp; \dots &amp; a_r
  \end{bmatrix} =
      \begin{bmatrix}
        q_1 &amp; \dots &amp; q_r
      \end{bmatrix}
        \begin{bmatrix}
          R_{11} &amp; R_{12} &amp; \dots &amp; R_{1r} \\
          0      &amp; R_{22} &amp; \dots &amp; R_{2r} \\
          \vdots &amp; \ddots &amp; \ddots &amp;  \vdots \\
          0 &amp; \ldots &amp; 0 &amp; R_{rr}
        \end{bmatrix}
\]</span></p>
This is quite similar to the result we are after (the QR-decomposition <a href="lecture-notes.html#thm:qr-decomposition">1.2</a>).
</div>


<div class="proof">
<p> <span class="proof"><em>Proof</em> (Proof of theorem <a href="lecture-notes.html#thm:qr-decomposition">1.2</a>). </span> Since <span class="math inline">\(\text{rank}(A) = r\)</span>, <span class="math inline">\(A\)</span> has <span class="math inline">\(r\)</span> linearly independent columns. Hence, there exists a permutation matrix <span class="math inline">\(\Pi\)</span> such that</p>
<p><span class="math display">\[
  A \Pi = \begin{bmatrix} a_1 &amp; \dots &amp; a_r &amp; a_{r+1} \dots a_m \end{bmatrix},
\]</span></p>
<p>where <span class="math inline">\(a_1, \ldots, a_r\)</span> are linearly independent, and <span class="math inline">\(a_{r+1}, \ldots, a_m\)</span> are linearly dependent on the first <span class="math inline">\(r\)</span> columns.</p>
<p>Using Gram-Schmidt (lemma <a href="lecture-notes.html#lem:gsp">1.8</a>), we know that there exists <span class="math inline">\(\tilde{Q} \in {\mathbb{R}}^{n \times r}, R \in {\mathbb{R}}^{r \times r}\)</span> such that <span class="math inline">\(A\Pi = \tilde{Q} R\)</span>. Since <span class="math inline">\(\text{span}\{\tilde{q}_1, \ldots, \tilde{q}_r\}\)</span> (columns of <span class="math inline">\(\tilde{Q}\)</span>) is equal to <span class="math inline">\(\text{span}\{a_1, \ldots, a_r\}\)</span>, there exists an <span class="math inline">\(s_{k(j-r+2)}\)</span> for any <span class="math inline">\(j \in \{r+1, \ldots, m\}\)</span> and <span class="math inline">\(k \in \{1, \ldots, r\}\)</span> such that <span class="math inline">\(a_j = \sum_{k=1}^r s_{k(j-r+2)}q_k\)</span>. So,</p>
<p><span class="math display">\[
  A\Pi = \tilde{Q} \begin{bmatrix} R &amp; S \end{bmatrix}.
\]</span></p>
<p>This is almost the form we want, BUT <span class="math inline">\(\tilde{Q}\)</span> is not orthonormal (it is not square). However, we know that we can pick <span class="math inline">\(n-r\)</span> vectors from <span class="math inline">\({\mathbb{R}}^n\)</span> such that adding these as columns to <span class="math inline">\(\tilde{Q}\)</span> we get a set of <span class="math inline">\(n\)</span> linearly independent columns. Now, use Gram-Schmidt to normalize. Since the first <span class="math inline">\(r\)</span> columns are already normalized, these will stay the same. The result is a matrix <span class="math inline">\(Q\)</span>, where the columns are all length <span class="math inline">\(1\)</span>, and they are all linearly independent. I.e. <span class="math inline">\(Q^TQ = I\)</span>. So, <span class="math inline">\(A\Pi = Q \begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix}\)</span>, hence</p>
<p><span class="math display">\[
  A = Q \begin{bmatrix} R &amp; S \\ 0 &amp; 0 \end{bmatrix} \Pi^T.
\]</span></p>
</div>

<p>Basically, this gives us a way to perform QR decomposition. However, using the Gram-Schmidt procedure is NOT numerical stable. I.e. we might end up with matrices <span class="math inline">\(Q, R\)</span>, and <span class="math inline">\(S\)</span> from which we CANNOT recover <span class="math inline">\(A\)</span>. To overcome this, there is a different method called the <em>Modified Gram-Schmidt Procedure</em>.</p>

<div class="lemma">
<span id="lem:mod-gsp" class="lemma"><strong>Lemma 1.9  (The Modified Gram-Schmidt Procedure)  </strong></span><strong>HOMEWORK</strong>
</div>


<div class="definition">
<span id="def:householder" class="definition"><strong>Definition 1.7  (Householder Reflections)  </strong></span>A matrix <span class="math inline">\(H = I - 2vv^\prime\)</span>, where <span class="math inline">\({\left \vert \left \vert v \right \vert \right \vert}_2 = 1\)</span>, is called a <em>Householder Reflection</em>.
</div>

<p>A Householder reflection takes any vector and reflects it over <span class="math inline">\(\{tv: t \in {\mathbb{R}}\}\)</span>.</p>

<div class="lemma">
<span id="lem:unnamed-chunk-28" class="lemma"><strong>Lemma 1.10  </strong></span>Householder reflections are orthogonal matrices.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> By definition of a Householder matrix (<a href="lecture-notes.html#def:householder">1.7</a>), <span class="math inline">\(H = I - 2vv^\prime\)</span> for a <span class="math inline">\(v\)</span> with <span class="math inline">\({\left \vert \left \vert v \right \vert \right \vert}_2 = 1\)</span>.</p>
<p>So,</p>
<p><span class="math display">\[\begin{aligned}
  H^\prime H &amp;= (I - 2vv^\prime)^\prime (I - 2vv^\prime) \\
             &amp;= (I^\prime - 2(vv^\prime)^\prime)(I - 2 vv^\prime)\\
             &amp;= (I - 2vv^\prime)(I - 2 vv^\prime) \\
             &amp;= I - 2vv^\prime - 2vv^\prime + 4 vv^\prime v v^\prime \quad\left(\text{ recall: } v^\prime v = {\left \vert \left \vert v \right \vert \right \vert}_2 = 1\right)\\
             &amp;= I - 2vv^\prime - 2vv^\prime + 4 vv^\prime \\
             &amp;= I.
\end{aligned}\]</span></p>
So by definition (<a href="lecture-notes.html#def:orthogonal">1.5</a>), <span class="math inline">\(H\)</span> is an orthogonal matrix.
</div>


<div class="lemma">
<span id="lem:unnamed-chunk-30" class="lemma"><strong>Lemma 1.11  </strong></span>There exists Householder refelctions <span class="math inline">\(H_1, \cdots, H_r\)</span> such that <span class="math inline">\(H_r \cdots H_1 \cdot A\cdot\Pi = R\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(A\Pi = \begin{bmatrix} a_1 \dots a_r \end{bmatrix}\)</span>. Choose <span class="math inline">\(H_1\)</span> s.t. <span class="math inline">\(H_1 a_1 = R_{11} e_1 = a_1 - 2v_1v_1^\prime a_1\)</span> (last equality due to definition of Householder reflections). This is equivalent to <span class="math inline">\(v_1(2v_1^\prime a_1) = a_1 - R_{11}e_1\)</span>.</p>
<p>Now, let <span class="math inline">\(v_1 = \frac{a_1 - R_{11}e_1}{{\left \vert \left \vert a_1 - R_{11}e_2 \right \vert \right \vert}_2}\)</span>. Plug this into the equation for <span class="math inline">\(R_{11}e_1\)</span> above to get</p>
<p><span class="math display">\[
  R_{11}e_1 = a_1 - \frac{(a_1 - R_{11}e_1)}{{\left \vert \left \vert a_1 - R_{11}e_1 \right \vert \right \vert}_2}\frac{a_1^\prime a_1 - R_{11} a_1^\prime e_1}{{\left \vert \left \vert a_1 - R_{11}e_1 \right \vert \right \vert}_2}.
\]</span></p>
<p>If we multiply this by <span class="math inline">\(e_1^\prime\)</span> from the right, we get</p>
<p><span class="math display">\[
  R_{11} = \pm {\left \vert \left \vert a_1 \right \vert \right \vert}_2, v_1 = \frac{a_1 - {\left \vert \left \vert a_1 \right \vert \right \vert}e_1}{{\left \vert \left \vert a_1 - {\left \vert \left \vert a_1 \right \vert \right \vert}_2 e_1 \right \vert \right \vert}_2}.
\]</span></p>
<p><span class="math display">\[
  H_1 = I - \frac{a_1 - {\left \vert \left \vert a_1 \right \vert \right \vert}_2e_1)(a_1 - {\left \vert \left \vert a_1 \right \vert \right \vert}_2 e_1)}{{\left \vert \left \vert a_1 - {\left \vert \left \vert a_1 \right \vert \right \vert}_2 e_1 \right \vert \right \vert}_2^2}
\]</span></p>
</div>


<div class="definition">
<p><span id="def:givens" class="definition"><strong>Definition 1.8  (Givens Rotations)  </strong></span>A <em>Givens Rotation</em> is a matrix <span class="math inline">\(G^{(i,j)}\)</span> with entries <span class="math inline">\((g_{ij})\)</span> such that</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(g_{ii} = g_{jj} = \lambda\)</span> (the <span class="math inline">\(i\)</span>th and <span class="math inline">\(j\)</span>th elements of the diagonal are <span class="math inline">\(\lambda\)</span>).</li>
<li><span class="math inline">\(g_{kk} = 1\)</span> for all <span class="math inline">\(k \notin \{i,j\}\)</span>. (all other diagonal elements are <span class="math inline">\(1\)</span>)</li>
<li><span class="math inline">\(g_{ij} = -g_{ji} = \sigma\)</span></li>
<li><span class="math inline">\(g_{ij} = 0\)</span> for all other pairs of <span class="math inline">\(i,j\)</span>.</li>
</ol>
<p>In words: <span class="math inline">\(G^{(i,j)}\)</span> is the identity matrix with the <span class="math inline">\(i\)</span>th and <span class="math inline">\(j\)</span>th diagonal elements made <span class="math inline">\(\lambda\)</span>, and the entries at <span class="math inline">\((i,j)\)</span> and <span class="math inline">\((j,i)\)</span> are <span class="math inline">\(\sigma\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-32" class="example"><strong>Example 1.8  (2x2 Givens rotation to create upper triangular matrix.)  </strong></span>The general <span class="math inline">\(G^{(1,2)}\)</span> is <span class="math inline">\(\begin{bmatrix} \lambda &amp; \sigma \\ -\sigma &amp; \lambda \end{bmatrix}\)</span>. Let us consider a general <span class="math inline">\(M \in {\mathbb{R}}^{2\times m}\)</span>:</p>
<p><span class="math display">\[
  M = \begin{bmatrix} M_{11} &amp; M_{12} &amp; \dots &amp; M_{1m} \\
                      M_{21} &amp; M_{22} &amp; \dots &amp; M_{2m} \end{bmatrix}.
\]</span></p>
<p>We want this Givens matrix to be orthogonal. By homework exercise <a href="homework-assignments.html#exr:q507">2.42</a>, we know this is the case when <span class="math inline">\(\lambda^2 + \delta^2 = 1\)</span>. We also want <span class="math inline">\(G^{(1,2)}M\)</span> to be an upper triangular matrix. Since</p>
<p><span class="math display">\[
  G^{(i,j)} M =
    \begin{bmatrix}
      \lambda M_{11} + \sigma M_{12} &amp; \dots &amp; \lambda M_{1m} + \sigma M_{2m} \\
      -\sigma M_{11} + \lambda M_{12} &amp; \dots &amp; -\sigma M_{1m} + \lambda M_{2m} \\
    \end{bmatrix},
\]</span></p>
<p>we need <span class="math inline">\(\lambda M_{12} = \sigma M_{11}\)</span>. So, solving these two equations, we find that <span class="math inline">\(\lambda = \frac{M_{11}}{\sqrt{M_{11}^2 + M_{21}^2}}\)</span> and <span class="math inline">\(\sigma = \frac{M_{21}}{\sqrt{M_{11}^2 + M_{21}^2}}\)</span>. Hence, given the matrix <span class="math inline">\(M\)</span>, we can find a Givens rotation which when multiplied by <span class="math inline">\(M\)</span> returns an upper triangular matrix.</p>
<p>This also means that given a vector <span class="math inline">\(a \in {\mathbb{R}}^n\)</span>, we can find a Givens rotation such that <span class="math inline">\(G^{(i,j)} a\)</span> gives back <span class="math inline">\(a\)</span> except for one entry, which has been changed to <span class="math inline">\(0\)</span>.</p>
</div>

</div>
<div id="large-data-problem" class="section level3">
<h3><span class="header-section-number">1.3.4</span> “Large” Data Problem</h3>
<p>Finally, we will take a look at how to solve a “large” data problem using QR decomposition. To do so, let <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}\)</span> be a matrix with <span class="math inline">\(n\)</span> “big”. By “big”, we mean so large that <span class="math inline">\(A\)</span> won’t fit in memory, but the first <span class="math inline">\(m+1\)</span> rows of <span class="math inline">\(A\)</span> will.</p>
<p>Gentleman published a few papers in 1973/1974 describing a method for incremental QR decomposition. The key ideas are as follows:</p>
<ol style="list-style-type: decimal">
<li>Remember that the solution to the linear model is <span class="math inline">\((A&#39;A)^{-1}A&#39;b = R^{-1}(Q&#39;b)\)</span>.</li>
<li>The QR decomposition of <span class="math inline">\(\begin{bmatrix}A &amp; b\end{bmatrix}\)</span> gives <span class="math inline">\(\begin{bmatrix} R &amp; Q&#39;b \\ 0 &amp; S \end{bmatrix}\)</span> (ex. <a href="homework-assignments.html#exr:q407">2.33</a> and <a href="homework-assignments.html#exr:q408">2.34</a>)</li>
</ol>
<p>Now, if we do QR decomposition on the first <span class="math inline">\(m+1\)</span> rows of <span class="math inline">\(A\)</span> we get <span class="math inline">\(\begin{bmatrix} \tilde{R}_{m+1} &amp; \tilde{Q}_{m+1}&#39;b_{m+1} \\ 0 &amp; S_{m+1} \end{bmatrix}\)</span>. Now, add the next row of <span class="math inline">\(A\)</span> to get <span class="math inline">\(\begin{bmatrix} \tilde{R}_{m+1} &amp; \tilde{Q}_{m+1}&#39;b_{m+1} \\ 0 &amp; S_{m+1} \\ a_{m+2} &amp; b_{m+2} \end{bmatrix}\)</span>. Hit this with the right Givens rotation to change entry <span class="math inline">\((m+1, 1)\)</span> to <span class="math inline">\(0\)</span>. This will give us something of the form <span class="math inline">\(\begin{bmatrix} \tilde{R}_{m+2} &amp; \tilde{Q}_{m+2}&#39;b_{m+2} \\ 0 &amp; S_{m+2} \end{bmatrix}\)</span>. Here, <span class="math inline">\(\tilde{R}_{m+2}\)</span> is still an upper triangular matrix with less than <span class="math inline">\(m\)</span> rows. Repeat the procedure until we’ve added all rows of <span class="math inline">\(A\)</span>/elements of <span class="math inline">\(b\)</span>.</p>
</div>
</div>
<div id="singular-value-decomposition-svd" class="section level2">
<h2><span class="header-section-number">1.4</span> Singular Value Decomposition (SVD)</h2>
<p>Outline:</p>
<ol style="list-style-type: upper-roman">
<li>Motivating problems</li>
<li>SVD and solutions</li>
<li>Existence and properties</li>
<li>Random projections (a modern application of SVD)</li>
</ol>
<div id="motivating-problems-1" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Motivating Problems</h3>

<div class="example">
<p><span id="exm:svd-problem-1" class="example"><strong>Example 1.9  </strong></span>Let <span class="math inline">\(A \in {\mathbb{R}}^{n \times m}\)</span> with <span class="math inline">\(\text{rank}(A) &lt; m \le n\)</span>, and let <span class="math inline">\(B \in \text{range}(A)\)</span>. Find <span class="math inline">\(x\)</span> s.t.</p>
<p><span class="math display">\[
  x \in {\text{argmin}}_{y \in {\mathbb{R}}^m} \left\{{\left \vert \left \vert y \right \vert \right \vert}_2 : Ay = b \right \}
\]</span></p>
</div>


<div class="example">
<p><span id="exm:svd-problem-2" class="example"><strong>Example 1.10  </strong></span>Let <span class="math inline">\(A \in {\mathbb{R}}^{n \times m}\)</span>. Find</p>
<p><span class="math display">\[
  {\left \vert \left \vert A \right \vert \right \vert}_2 = \sup_{v \in {\mathbb{R}}^m\setminus\{0\}} \frac{{\left \vert \left \vert Av \right \vert \right \vert}_2}{{\left \vert \left \vert v \right \vert \right \vert}_2}.
\]</span></p>
</div>


<div class="example">
<p><span id="exm:svd-problem-3" class="example"><strong>Example 1.11  </strong></span>Let <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}\)</span>. Find <span class="math inline">\(x\)</span> s.t.</p>
<p><span class="math display">\[
  x = {\text{argmin}}_{rank(Y) \le k}{\left \vert \left \vert A-Y \right \vert \right \vert}_F.
\]</span></p>
</div>

<p>Note: <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_F\)</span> is the <em>Frobenius norm</em> and is defined as <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_F = \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 \right)^{1/2}\)</span>. So it is sort of a Euclideaon norm extended to matrices.</p>
</div>
<div id="svd" class="section level3">
<h3><span class="header-section-number">1.4.2</span> SVD</h3>
<div id="svd-definition" class="section level4">
<h4><span class="header-section-number">1.4.2.1</span> SVD definition</h4>

<div class="theorem">
<p><span id="thm:svd" class="theorem"><strong>Theorem 1.3  </strong></span>Suppose <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}\)</span> and <span class="math inline">\(n\ge m\)</span>. Then there exists <span class="math inline">\(U \in {\mathbb{R}}^{n \times n}\)</span> and <span class="math inline">\(V \in {\mathbb{R}}^{m\times m}\)</span> that are both orthogonal, and a diagonal matrix <span class="math inline">\(\Sigma \in {\mathbb{R}}^{n \times m}\)</span> with diagonal elements <span class="math inline">\(\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_m \ge 0\)</span> such that</p>
<p><span class="math display">\[ A = U \Sigma V&#39;\]</span></p>
</div>

<p>Note:</p>
<ul>
<li>The diagonal elements of <span class="math inline">\(\Sigma\)</span> are called the singular values of <span class="math inline">\(A\)</span>.</li>
<li>The columns of <span class="math inline">\(U\)</span> are called the left singular vectors</li>
<li>The columns of <span class="math inline">\(V\)</span> are called the right singular vectors</li>
</ul>

<div class="corollary">
<span id="cor:cor-svd" class="corollary"><strong>Corollary 1.1  </strong></span><span class="math inline">\(\text{rank}(A)\)</span> is the number of non-zero singular values of <span class="math inline">\(A\)</span>.
</div>

</div>
<div id="solutions-to-motivating-problems" class="section level4">
<h4><span class="header-section-number">1.4.2.2</span> Solutions to motivating problems</h4>

<div class="solution">
<p> <span class="solution"><em>Solution</em> (Solution to example <a href="lecture-notes.html#exm:svd-problem-1">1.9</a>). </span>  Since <span class="math inline">\(b \in \text{range}(A)\)</span>, there exists a <span class="math inline">\(y\)</span> such that <span class="math inline">\(Ay = b\)</span>. Now, by theorem <a href="lecture-notes.html#thm:svd">1.3</a> there exist <span class="math inline">\(U\)</span>,<span class="math inline">\(V\)</span>, and <span class="math inline">\(\Sigma\)</span> such that <span class="math inline">\(A=U\Sigma V^\prime\)</span>. Since <span class="math inline">\(U\)</span> is orthogonal, <span class="math inline">\(U^{-1} = U^\prime\)</span>. So,</p>
<p><span class="math display">\[\begin{aligned}
  U\Sigma V^\prime y &amp;= b \Leftrightarrow \\
  \Sigma V^\prime y &amp;= U^\prime b.
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(z = V^\prime y\)</span> and <span class="math inline">\(c = U^\prime b\)</span>. By corollary <a href="lecture-notes.html#cor:cor-svd">1.1</a> we now that there are exactly <span class="math inline">\(\text{rank}(A) = r\)</span> non-zero singular values. So</p>
<p><span class="math display">\[\begin{aligned}
  \begin{bmatrix}
  \sigma_1 &amp; &amp; &amp; &amp; &amp; \\
    &amp; \ddots &amp; &amp; &amp; &amp; \\
    &amp; &amp; \sigma_r &amp; &amp; &amp; \\
    &amp; &amp; &amp; 0 &amp; &amp; \\
    &amp; &amp; &amp; &amp; \ddots &amp; \\
    &amp; &amp; &amp; &amp; &amp; 0 \\
  \end{bmatrix} \cdot
  \begin{bmatrix} z_1 \\ z_2 \end{bmatrix} = \begin{bmatrix} c_1 \\ 0 \end{bmatrix}.
\end{aligned}\]</span></p>
<p>So, <span class="math inline">\(z_1 = \begin{bmatrix} \sigma_1^{-1} &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \sigma_r^{-1} \end{bmatrix} c_1\)</span>. We want to minimize <span class="math inline">\({\left \vert \left \vert y \right \vert \right \vert}_2 = {\left \vert \left \vert V^\prime z \right \vert \right \vert}_2 = {\left \vert \left \vert z \right \vert \right \vert}_2 = \sqrt{{\left \vert \left \vert z_1 \right \vert \right \vert}_2^2 + {\left \vert \left \vert z_2 \right \vert \right \vert}_2^2}\)</span>. This is done by setting <span class="math inline">\(z_2=0\)</span>, which we can do since <span class="math inline">\(z_2\)</span> does not affect the equation above. (It is multiplied by all the <span class="math inline">\(0\)</span> rows of <span class="math inline">\(\Sigma\)</span>.) So, the minimum norm solution is</p>
<p><span class="math display">\[\begin{aligned}
  x &amp;= V
    \begin{bmatrix}
      \sigma_1^{-1} &amp; &amp; &amp; \\
      &amp; \ddots &amp; &amp; \huge{c_1} \\
      &amp; &amp; \sigma_{r}^{-1} &amp; \\
      &amp; {\huge 0} &amp; &amp;
    \end{bmatrix} \\
    &amp;= V \cdot V^\prime y.
\end{aligned}\]</span></p>
</div>

<p>For the solution to example <a href="lecture-notes.html#exm:svd-problem-2">1.10</a>, we’ll need the following result:</p>

<div class="lemma">
<p><span id="lem:unnamed-chunk-34" class="lemma"><strong>Lemma 1.12  </strong></span> Let <span class="math inline">\(D\)</span> be a non-zero diagonal matrix of size <span class="math inline">\(n \times m\)</span>, <span class="math inline">\(n\ge m\)</span>. Then, <span class="math display">\[{\left \vert \left \vert D \right \vert \right \vert}_2 = \max_i |D_{ii}|\]</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_2 = \max_{{\left \vert \left \vert v \right \vert \right \vert}_2 = 1} {\left \vert \left \vert Av \right \vert \right \vert}_2\)</span>. Also, if we let <span class="math inline">\(D_{ii} = \max_{j} |D_{jj}|\)</span> and <span class="math inline">\(v\)</span> be a vector with norm <span class="math inline">\(1\)</span>,</p>
<p><span class="math display">\[
  {\left \vert \left \vert Dv \right \vert \right \vert}_2^2 = \sum_{j=1}^m (D_{jj}^2 v_j)^2 \leq D_{ii}^2 \sum_{j=1}^m v_j^2 = D_{ii}^2 {\left \vert \left \vert v \right \vert \right \vert}_2^2 = D_{ii}^2.
\]</span></p>
<p>Now, let <span class="math inline">\(z \in {\mathbb{R}}^m\)</span> such that <span class="math inline">\(z_{ij} = \left\{ \begin{matrix} 0, &amp; j \neq i \\ \frac{D_{ii}}{|D_{ii}|}, &amp; j = i \end{matrix} \right .\)</span>. Then <span class="math inline">\({\left \vert \left \vert z \right \vert \right \vert}_2 = \sqrt{\frac{D_{ii}^2}{|D_{ii}|^2}} = 1\)</span>, and</p>
<p><span class="math display">\[
  {\left \vert \left \vert Dz \right \vert \right \vert}_2^2 = \sum_{j=1}^m (D_{jj} z_j)^2 = D_{ii}^2 \frac{D_{ii}^2}{|D_{ii}|^2} = D_{ii}^2.
\]</span></p>
<p>Since <span class="math inline">\({\left \vert \left \vert z \right \vert \right \vert}_2 = 1\)</span>,</p>
<p><span class="math display">\[ D_{ii}^2 {\left \vert \left \vert Dz \right \vert \right \vert}_2^2 \le (\max_{{\left \vert \left \vert v \right \vert \right \vert}_2 = 1} |D_{ii}|)^2 \le D_{ii}^2\]</span>,</p>
<p>so <span class="math inline">\({\left \vert \left \vert D \right \vert \right \vert}_2 = \max_{{\left \vert \left \vert v \right \vert \right \vert}_2 = 1} |D_{ii}| = |D_{ii}| = max_{j} |D_{jj}|\)</span>.</p>
</div>


<div class="solution">
<p> <span class="solution"><em>Solution</em> (Solution to example <a href="lecture-notes.html#exm:svd-problem-2">1.10</a>). </span> First, use the SVD of <span class="math inline">\(A\)</span> to write <span class="math inline">\(A = U \Sigma V^\prime\)</span>. Now, since the 2-norm is invariant under multiplication of orthogonal matrices, we have that</p>
<p><span class="math display">\[\begin{aligned}
  {\left \vert \left \vert A \right \vert \right \vert}_2 &amp;= \max_{{\left \vert \left \vert v \right \vert \right \vert} = 2} {\left \vert \left \vert U\Sigma V^\prime v \right \vert \right \vert}_2 \\
             &amp;= \max_{{\left \vert \left \vert v \right \vert \right \vert} = 2} {\left \vert \left \vert \Sigma V^\prime v \right \vert \right \vert}_2.
\end{aligned}\]</span></p>
<p>If we let <span class="math inline">\(z = V^\prime v\)</span>, we see that <span class="math inline">\({\left \vert \left \vert z \right \vert \right \vert}_2 = {\left \vert \left \vert v \right \vert \right \vert}_2 = 1\)</span>, since <span class="math inline">\(V^\prime\)</span> is an orthogonal matrix. Hence,</p>
<p><span class="math display">\[
  {\left \vert \left \vert A \right \vert \right \vert}_2 = \max_{{\left \vert \left \vert z \right \vert \right \vert}_2 = 1} {\left \vert \left \vert \Sigma z \right \vert \right \vert}_2 = {\left \vert \left \vert \Sigma \right \vert \right \vert}_2 = \max_{i} |\sigma_i| =  \sigma_1,
\]</span></p>
where we used the lemma above to obtain the second to last equality.
</div>


<div class="solution">
<p> <span class="solution"><em>Solution</em> (Solution to example <a href="lecture-notes.html#exm:svd-problem-3">1.11</a>). </span> Recall, <span class="math inline">\(A = \sum_{i=1}^n \sigma_i u_i v_i^\prime = U \Sigma V^\prime\)</span>. We want to find <span class="math inline">\({\text{argmin}}_{\text{rank}(Y) \le k} {\left \vert \left \vert A-Y \right \vert \right \vert}_F\)</span>.</p>
<p>Case 1: If <span class="math inline">\(\text{rank}(A) \le k\)</span>, then <span class="math inline">\(Y = A\)</span> is the solution.</p>
<p>Case 2: <span class="math inline">\(\text{rank}(A) &gt; k\)</span>. Since the Frobenius norm is orthogonally invariant, we can obtain that</p>
<p><span class="math display">\[\begin{aligned}
  {\left \vert \left \vert A-Y \right \vert \right \vert}_F^2 &amp;= {\left \vert \left \vert U\Sigma V^\prime - Y \right \vert \right \vert}_F^2 \\
                 &amp;= {\left \vert \left \vert \Sigma - U^\prime Y V \right \vert \right \vert}_F^2.
\end{aligned}\]</span></p>
<p>If we let <span class="math inline">\(X = U^\prime Y V\)</span>, we get that</p>
<p><span class="math display">\[\begin{aligned}
  {\left \vert \left \vert A-Y \right \vert \right \vert}_F^2 &amp;= \sum_{i=1}^n \sum_{j=1}^m (\sigma_{ij} - x_{ij})^2 \\
                 &amp;= \sum_{i=1}^n \sum_{j=1,i\ne j}^m ( - x_{ij})^2  + \sum_{i=1}^n (\sigma_i - x_{ij})^2,
\end{aligned}\]</span></p>
<p>since <span class="math inline">\(\sigma_{ij}=0\)</span> for all <span class="math inline">\(i \ne j\)</span>, and at most <span class="math inline">\(k\)</span> of the <span class="math inline">\(\sigma_{ii}\)</span> are non-zero. To minimize the expression above, we choose <span class="math inline">\(Y\)</span> such that <span class="math inline">\(x_{ij}=0\)</span> for <span class="math inline">\(i\ne j\)</span>, and <span class="math inline">\(x_{ii} = \sigma_i\)</span>. Hence,</p>
<p><span class="math display">\[
  X = \begin{bmatrix} \sigma_1 &amp; &amp; &amp; &amp; &amp; \\
                      &amp; \ddots &amp; &amp; &amp; &amp; \\
                      &amp; &amp; \sigma_k &amp; &amp; &amp; \\
                      &amp; &amp; &amp; 0 &amp; &amp; \\
                      &amp; &amp; &amp; &amp; \ddots &amp; \\
                      &amp; &amp; &amp; &amp; &amp; 0 \end{bmatrix},
\]</span></p>
<p>and <span class="math inline">\(Y = U X V^\prime\)</span>.</p>
</div>

</div>
</div>
<div id="existence-and-properties" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Existence and Properties</h3>

<div class="proof">
<p> <span class="proof"><em>Proof</em> (SVD (<a href="lecture-notes.html#thm:svd">1.3</a>)). </span> Recall that <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_2 = \sup_{v \ne 0} \frac{{\left \vert \left \vert Av \right \vert \right \vert}_2}{{\left \vert \left \vert v \right \vert \right \vert}_2} = \max_{{\left \vert \left \vert v \right \vert \right \vert}_2 = 1} {\left \vert \left \vert Av \right \vert \right \vert}_2\)</span>. I.e. there exists a <span class="math inline">\(v_1\)</span> such that <span class="math inline">\({\left \vert \left \vert v_1 \right \vert \right \vert}_2 = 1\)</span> and <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_2 = {\left \vert \left \vert Av_1 \right \vert \right \vert}_2\)</span>.</p>
<p>Let <span class="math inline">\(u_1\)</span> be a vector such that <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_2 u_1 = Av_1\)</span>. This implies that <span class="math inline">\({\left \vert \left \vert u_1 \right \vert \right \vert}_2 = 1\)</span>, since <span class="math inline">\({\left \vert \left \vert {\left \vert \left \vert A \right \vert \right \vert}_2u_1 \right \vert \right \vert}_2 = {\left \vert \left \vert A \right \vert \right \vert}_2 {\left \vert \left \vert u_1 \right \vert \right \vert}_2 = {\left \vert \left \vert Av_1 \right \vert \right \vert}_2 = {\left \vert \left \vert A \right \vert \right \vert}_2\)</span>.</p>
<p>Let <span class="math inline">\(\sigma_1 = {\left \vert \left \vert A \right \vert \right \vert}_2\)</span>. So, <span class="math inline">\(\sigma_1 u_1 = A v_1\)</span>. Using the Gram-Schmidt procedure, we can create a matrix <span class="math inline">\(V_1 \in {\mathbb{R}}^{n\times (n-1)}\)</span> and <span class="math inline">\(\tilde{U}_1 = \begin{bmatrix} u_1 V_1 \end{bmatrix} \in {\mathbb{R}}^{n \times n}\)</span> being an orthogonal matrix. Similarly, we can create <span class="math inline">\(\tilde{V}_1 \in {\mathbb{R}}^{m\times m}\)</span> such that <span class="math inline">\(\tilde{V}_1 = \begin{bmatrix} v_1 V_1 \end{bmatrix} \in {\mathbb{R}}^{m \times m}\)</span> is orthogonal.</p>
<p>Now, <span class="math inline">\(A \tilde{V}_1 = \begin{bmatrix} Av_1 &amp; AV_1 \end{bmatrix} = \begin{bmatrix} \sigma_1 u_1 &amp; AV_1 \end{bmatrix}\)</span>. Hence, <span class="math inline">\(\tilde{U}_1^\prime A \tilde{V}_1 = \begin{bmatrix} u_1 \\ U_1^\prime \end{bmatrix} \begin{bmatrix} \sigma_1 u_1 &amp; A V_1 \end{bmatrix} = \begin{bmatrix} \sigma_1 &amp; w^\prime \\ 0 &amp; \tilde{A} \end{bmatrix}\)</span>, since <span class="math inline">\(u_1\)</span> is orthogonal to all columns of <span class="math inline">\(U_1^\prime\)</span>. We need to show that <span class="math inline">\(w=0\)</span>. Since the 2-norm is orthogonally invariant, it holds for any <span class="math inline">\(z \ne 0\)</span>,</p>
<p><span class="math display">\[\begin{aligned}
  \sigma_1^2 &amp;= {\left \vert \left \vert A \right \vert \right \vert}_2^2 \\
             &amp;= {\left \vert \left \vert \tilde{U}_1^\prime A \tilde{V}_1 \right \vert \right \vert}_2^2 \\
             &amp;= {\left \vert \left \vert \begin{bmatrix} \sigma_1 &amp; w^\prime \\ 0 &amp; \tilde{A} \end{bmatrix} \right \vert \right \vert}_2^2 \\
             &amp;\ge \frac{{\left \vert \left \vert \begin{bmatrix} \sigma_1 &amp; w^\prime \\ 0 &amp; \tilde{A} \end{bmatrix} z \right \vert \right \vert}_2^2}{{\left \vert \left \vert z \right \vert \right \vert}_2^2}.
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(z = \begin{bmatrix} \sigma_1 \\ w \end{bmatrix}\)</span>. Then,</p>
<p><span class="math display">\[
  \sigma_1^2 \ge \frac{{\left \vert \left \vert \begin{bmatrix} \sigma_1 &amp; w^\prime \\ 0 &amp; \tilde{A} \end{bmatrix} \begin{bmatrix} \sigma_1 \\ w \end{bmatrix} \right \vert \right \vert}_2^2}{{\left \vert \left \vert \begin{bmatrix} \sigma_1 \\ w \end{bmatrix} \right \vert \right \vert}_2^2} = \frac{(\sigma_1^2 + {\left \vert \left \vert w \right \vert \right \vert}_2^2)^2 + {\left \vert \left \vert \tilde{A}w \right \vert \right \vert}_2^2}{\sigma_1^2 + {\left \vert \left \vert w \right \vert \right \vert}_2^2}\ge \sigma_1^2 + {\left \vert \left \vert w \right \vert \right \vert}_2^2 \ge \sigma_1^2,
\]</span></p>
<p>i.e. <span class="math inline">\({\left \vert \left \vert w \right \vert \right \vert}_2^2 = 0\)</span>. So,</p>
<p><span class="math display">\[
  \tilde{U}_1 A V_1 = \begin{bmatrix} \sigma_1 &amp; 0 \\ 0 &amp; \tilde{A} \end{bmatrix}.
\]</span></p>
Repeat the same procedure to get <span class="math inline">\(U, V\)</span> such that <span class="math inline">\(A = U^\prime \Sigma V\)</span>.
</div>


<div class="corollary">
<span id="cor:unnamed-chunk-39" class="corollary"><strong>Corollary 1.2  </strong></span><span class="math inline">\(\text{rank}(A)\)</span> is exactly the number of non-zero singular values of <span class="math inline">\(A\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span>  <span class="math inline">\(\text{rank}(A) = \text{rank}(U \Sigma V^\prime ) = \text{rank}(\Sigma) =\)</span> number of non-zero diagonal elements.</p>
</div>


<div class="corollary">
<p><span id="cor:sigma-min-max" class="corollary"><strong>Corollary 1.3  </strong></span>Let <span class="math inline">\(A,E \in {\mathbb{R}}^{n\times m}\)</span>, <span class="math inline">\(\sigma_{\text{max}}\)</span> (<span class="math inline">\(\sigma_{\text{min}}\)</span>) denote the largest (smallest) singular value of A. Then</p>
<p><span class="math display">\[
  \sigma_{\text{max}}(A+E) \le \sigma_{\text{max}}(A) + {\left \vert \left \vert E \right \vert \right \vert}_2
\]</span></p>
<p>and</p>
<p><span class="math display">\[
  \sigma_{\text{min}}(A+E) \ge \sigma_{\text{min}}(A) - {\left \vert \left \vert E \right \vert \right \vert}_2.
\]</span></p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-41" class="corollary"><strong>Corollary 1.4  (Hoffman-Wielandt Inequality)  </strong></span>Let <span class="math inline">\(A,E \in {\mathbb{R}}^{n \times m}\)</span>, <span class="math inline">\(\sigma_k(\dot)\)</span> denote the k’th largest singular value. Let <span class="math inline">\(p =\min(m,n) \le {\left \vert \left \vert E \right \vert \right \vert}_F^2\)</span>. Then</p>
<p><span class="math display">\[
  \sum_{k=1}^p (\sigma_k(A+E) - \sigma_k(A))^2 \le {\left \vert \left \vert E \right \vert \right \vert}_F^2.
\]</span></p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-42" class="corollary"><strong>Corollary 1.5  </strong></span> Let <span class="math inline">\(r = \text{rank}(A)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{range}(A) = \text{span}(u_1, \ldots, u_r)\)</span></li>
<li>row space of A <span class="math inline">\(= \text{span}(v_1, \ldots, v_r)\)</span></li>
<li><span class="math inline">\(\text{null}(A) = \text{span}(v_{r+1}, \ldots, v_{m})\)</span></li>
<li><span class="math inline">\(\text{null}(A^\prime) = \text{span}(u_{r+1}, \ldots, v_{m})\)</span></li>
</ol>
<p>Moreover,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(U_r = \begin{bmatrix} u_1 &amp; \cdots &amp; u_r \end{bmatrix}\)</span>, <span class="math inline">\(U_r U_r^\prime = P_{\text{range}(A)}\)</span></li>
<li><span class="math inline">\(U_{n-r} = \begin{bmatrix} u_{r+1} &amp; \cdots &amp; u_n \end{bmatrix}\)</span>, <span class="math inline">\(U_{n-r} U_{n-r}^\prime = P_{\text{null}(A)}\)</span></li>
<li><span class="math inline">\(V_r = \begin{bmatrix} v_1 &amp; \cdots &amp; v_r \end{bmatrix}\)</span>, V_r V_r^= P_{(A)}$</li>
<li><span class="math inline">\(V_{n-r} = \begin{bmatrix} v_{r+1} &amp; \cdots &amp; v_n \end{bmatrix}\)</span>, <span class="math inline">\(V_{n-r} V_{n-r}^\prime = P_{\text{null}(A)}\)</span></li>
</ol>
where <span class="math inline">\(P_{\mathcal{B}}\)</span> is the projection onto the space <span class="math inline">\(\mathcal{B}\)</span>.
</div>


<div class="definition">
<p><span id="def:pseudo-inverse" class="definition"><strong>Definition 1.9  (Pseudo-inverse)  </strong></span>For a matrix <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}\)</span>, we call <span class="math inline">\(A^+ \in {\mathbb{R}}^{m\times n}\)</span> a pseudo-inverse to <span class="math inline">\(A\)</span> if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(AA^+A = A\)</span></li>
<li><span class="math inline">\(A^+AA^+ = A^+\)</span></li>
<li><span class="math inline">\((AA^+)^\prime = AA^+\)</span></li>
<li><span class="math inline">\((A^+A)^\prime = A^+A\)</span></li>
</ol>
</div>


<div class="theorem">
<span id="thm:unnamed-chunk-43" class="theorem"><strong>Theorem 1.4  </strong></span>For any matrix <span class="math inline">\(A \in {\mathbb{R}}^{n \times m}\)</span>, there exists a unique pseudo-inverse <span class="math inline">\(A^+\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Suppose <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are both pseudo-inverses of <span class="math inline">\(A\)</span>. Then</p>
<p><span class="math display">\[\begin{aligned}
  BA &amp;= B(ACA) \\
     &amp;= (BA)(CA) \\
     &amp;= (BA)^\prime (CA)^\prime \\
     &amp;= A^\prime B^\prime A^\prime C^\prime \\
     &amp;= (ABA)^\prime C^\prime \\
     &amp;= A^\prime C^\prime \\
     &amp;= (CA)^\prime \\
     &amp;= CA.
\end{aligned}\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[\begin{aligned}
  AB &amp;= (ACA)B \\
     &amp;= (AC)(AB) \\
     &amp;= (AC)^\prime (AB)^\prime \\
     &amp;= C^\prime A^\prime B^\prime A^\prime \\
     &amp;= C^\prime (ABA)^\prime \\
     &amp;= C^\prime A^\prime \\
     &amp;= (AC)^\prime \\
     &amp;= AC.
\end{aligned}\]</span></p>
<p>So, <span class="math inline">\(B = BAB = CAB = CAC = C\)</span>. This shows that if there exists a pseudo-inverse, it is unique.</p>
<p>Now, let <span class="math inline">\(D \in {\mathbb{R}}^{n\times m}\)</span> be a diagonal matrix with <span class="math inline">\(d_{ii}\)</span> its diagonal elements. Then the diagonal matrix <span class="math inline">\(E \in {\mathbb{R}}^{m\times n}\)</span> with diagonal elements <span class="math inline">\(d_{ii}^{-1}\)</span> for <span class="math inline">\(d_{ii} \neq 0\)</span> and <span class="math inline">\(0\)</span> otherwise is the pseudo-inverse of <span class="math inline">\(D\)</span>.</p>
<p>So,</p>
<p><span class="math display">\[[DE]_{ij} = \sum_{k=1}^m D_{ik} E_{kj} = \left\{\begin{array}{rl} 0, &amp; i \neq j \\ 1, &amp; i = j, d_{ii} \neq 0 \\ 0, &amp; i = j, d_{ii} = 0 \end{array} \right .\]</span></p>
<p>and</p>
<p><span class="math display">\[[ED]_{ij} = \sum_{k=1}^n D_{ik} E_{kj} = \left\{\begin{array}{rl} 0, &amp; i \neq j \\ 1, &amp; i = j, d_{ii} \neq 0 \\ 0, &amp; i = j, d_{ii} = 0 \end{array} \right .\]</span></p>
<p>Since <span class="math inline">\(ED\)</span> and <span class="math inline">\(DE\)</span> are both diagonal matrices, <span class="math inline">\((ED)^{\prime} = ED\)</span> and <span class="math inline">\((DE)^{\prime} = DE\)</span>.</p>
<p>Furthermore,</p>
<p><span class="math display">\[[DED]_{ij} = \sum_{k=1}^n D_{ik}[ED]_{kj} = \left\{\begin{array}{rl} 0, &amp; i \neq j \\ D_{ij}, &amp; i = j \end{array} \right .\]</span></p>
<p>and</p>
<p><span class="math display">\[[EDE]_{ij} = \sum_{k=1}^n E_{ik}[DE]_{kj} = \left\{\begin{array}{rl} 0, &amp; i \neq j \\ E_{ij}, &amp; i = j \end{array} \right .\]</span></p>
<p>So <span class="math inline">\(E\)</span> satisfies the four conditions of a pseudo-inverse. I.e. for any diagonal matrix, there exists a pseudo-inverse.</p>
<p>Now, we know that there exist <span class="math inline">\(U,\Sigma, V\)</span> such that <span class="math inline">\(A = U\Sigma V^\prime\)</span>. Let <span class="math inline">\(B = V \Sigma^+ U^\prime\)</span>. Recall that <span class="math inline">\(U^\prime U = I\)</span> and <span class="math inline">\(V^\prime V = I\)</span>.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(ABA = U\Sigma V^\prime V \Sigma^+ U^\prime U\Sigma V^\prime = U \Sigma \Sigma^+ \Sigma V^\prime = U \Sigma V^\prime = A\)</span></li>
<li><span class="math inline">\(BAB = V \Sigma^+ U^\prime U \Sigma V^\prime V \Sigma^+ U = V \Sigma^+ \Sigma \Sigma^+ U^\prime = V \Sigma^+ U^\prime = B\)</span></li>
<li></li>
</ol>
<p><span class="math display">\[\begin{aligned} 
  (AB)^\prime = B^\prime A^\prime &amp;= (V \Sigma^+ U^\prime)^\prime (U \Sigma V^\prime)^\prime \\
                                  &amp;= U (\Sigma^+)^\prime V^\prime V \Sigma^\prime U^\prime \\
                                  &amp;= U (\Sigma \Sigma^+)^\prime U^\prime \\
                                  &amp;= U \Sigma \Sigma^+ U^\prime \\
                                  &amp;= U \Sigma V^\prime V \Sigma^+ U^\prime \\
                                  &amp;= AB
\end{aligned}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li></li>
</ol>
<p><span class="math display">\[\begin{aligned} 
  (BA)^\prime = A^\prime B^\prime &amp;= (U \Sigma V^\prime)^\prime (V \Sigma^+ U^\prime)^\prime \\
                                  &amp;= V \Sigma^\prime U^\prime U (\Sigma^+)^\prime V^\prime \\
                                  &amp;= V (\Sigma^+ \Sigma)^\prime V^\prime \\
                                  &amp;= V \Sigma^+ \Sigma V^\prime \\
                                  &amp;= V \Sigma^+ U^\prime U \Sigma V^\prime \\
                                  &amp;= BA.
\end{aligned}\]</span></p>
<p>So, <span class="math inline">\(B\)</span> is a pseudo-inverse of <span class="math inline">\(A\)</span>.</p>
<p>Hence, any matrix has a unique pseudo-inverse.</p>
</div>

</div>
<div id="random-projections" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Random Projections</h3>
<p>Let <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}\)</span>, <span class="math inline">\(m &gt;&gt; n\)</span>. When this is the case, it is very hard to compute the SVD. So instead we try to find a matrix <span class="math inline">\(C\)</span> with <span class="math inline">\(\text{range}(C) \approx \text{range}(A)\)</span>. In other words, <span class="math inline">\(A \approx P_{\text{range}(C)}A\)</span>. One way to find such a <span class="math inline">\(C\)</span> is to simply sample columns from <span class="math inline">\(A\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-45" class="theorem"><strong>Theorem 1.5  </strong></span>Suppose <span class="math inline">\(A \in {\mathbb{R}}^{n \times m}\)</span>, <span class="math inline">\(C\)</span> is as in the algorithm <code>inexactRankK</code> (see <a href="&#39;../Lecture%20scripts/Singular%20Value%20Decomposition.jl&#39;">here</a>), and <span class="math inline">\(H\)</span> is the <span class="math inline">\(k\)</span> left singular vectors of <span class="math inline">\(C\)</span>. Then</p>
$$_F^2 _F^2 +
</div>


</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Here we use the notion <span class="math inline">\(Q_{i*}\)</span> to mean the <span class="math inline">\(i\)</span>’th row, and <span class="math inline">\(Q_{*j}\)</span> to mean the <span class="math inline">\(j\)</span>’th column of <span class="math inline">\(Q\)</span>.<a href="lecture-notes.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="homework-assignments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
