<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STAT 771: My notes</title>
  <meta name="description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="STAT 771: My notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STAT 771: My notes" />
  
  <meta name="twitter:description" content="This is my collection of notes for the STAT 771 class taught at UW-Madison." />
  

<meta name="author" content="Ralph Møller Trane">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="singular-value-decomposition-svd.html">
<link rel="next" href="nonlinear-systems-of-equations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="lecture-notes.html"><a href="lecture-notes.html"><i class="fa fa-check"></i><b>1</b> Lecture Notes</a><ul>
<li class="chapter" data-level="" data-path="lecture-1-96.html"><a href="lecture-1-96.html"><i class="fa fa-check"></i>Lecture 1: 9/6</a></li>
<li class="chapter" data-level="1.1" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html"><i class="fa fa-check"></i><b>1.1</b> Positional numeral system</a><ul>
<li class="chapter" data-level="1.1.1" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#floating-point-format"><i class="fa fa-check"></i><b>1.1.1</b> Floating Point Format</a></li>
<li class="chapter" data-level="1.1.2" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#ieee-standards"><i class="fa fa-check"></i><b>1.1.2</b> IEEE Standards</a></li>
<li class="chapter" data-level="1.1.3" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#errors"><i class="fa fa-check"></i><b>1.1.3</b> Errors</a></li>
<li class="chapter" data-level="1.1.4" data-path="positional-numeral-system.html"><a href="positional-numeral-system.html#square-linear-systems"><i class="fa fa-check"></i><b>1.1.4</b> Square Linear Systems</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="orthogonalization.html"><a href="orthogonalization.html"><i class="fa fa-check"></i><b>1.2</b> Orthogonalization</a><ul>
<li class="chapter" data-level="1.2.1" data-path="orthogonalization.html"><a href="orthogonalization.html#motivating-problems"><i class="fa fa-check"></i><b>1.2.1</b> Motivating problems</a></li>
<li class="chapter" data-level="" data-path="orthogonalization.html"><a href="orthogonalization.html#lecture-4-918"><i class="fa fa-check"></i>Lecture 4: 9/18</a></li>
<li class="chapter" data-level="1.2.2" data-path="orthogonalization.html"><a href="orthogonalization.html#qr-decomposition"><i class="fa fa-check"></i><b>1.2.2</b> QR Decomposition</a></li>
<li class="chapter" data-level="1.2.3" data-path="orthogonalization.html"><a href="orthogonalization.html#existence-of-qr-decomposition."><i class="fa fa-check"></i><b>1.2.3</b> Existence of QR-decomposition.</a></li>
<li class="chapter" data-level="" data-path="orthogonalization.html"><a href="orthogonalization.html#lecture-5-920"><i class="fa fa-check"></i>Lecture 5: 9/20</a></li>
<li class="chapter" data-level="1.2.4" data-path="orthogonalization.html"><a href="orthogonalization.html#large-data-problem"><i class="fa fa-check"></i><b>1.2.4</b> “Large” Data Problem</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html"><i class="fa fa-check"></i><b>1.3</b> Singular Value Decomposition (SVD)</a><ul>
<li class="chapter" data-level="1.3.1" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#motivating-problems-1"><i class="fa fa-check"></i><b>1.3.1</b> Motivating Problems</a></li>
<li class="chapter" data-level="1.3.2" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#svd"><i class="fa fa-check"></i><b>1.3.2</b> SVD</a></li>
<li class="chapter" data-level="1.3.3" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#existence-and-properties"><i class="fa fa-check"></i><b>1.3.3</b> Existence and Properties</a></li>
<li class="chapter" data-level="1.3.4" data-path="singular-value-decomposition-svd.html"><a href="singular-value-decomposition-svd.html#random-projections"><i class="fa fa-check"></i><b>1.3.4</b> Random Projections</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="iterative-methods.html"><a href="iterative-methods.html"><i class="fa fa-check"></i><b>1.4</b> Iterative Methods</a><ul>
<li class="chapter" data-level="1.4.1" data-path="iterative-methods.html"><a href="iterative-methods.html#overview"><i class="fa fa-check"></i><b>1.4.1</b> Overview</a></li>
<li class="chapter" data-level="1.4.2" data-path="iterative-methods.html"><a href="iterative-methods.html#outline"><i class="fa fa-check"></i><b>1.4.2</b> Outline</a></li>
<li class="chapter" data-level="1.4.3" data-path="iterative-methods.html"><a href="iterative-methods.html#motivation"><i class="fa fa-check"></i><b>1.4.3</b> Motivation</a></li>
<li class="chapter" data-level="1.4.4" data-path="iterative-methods.html"><a href="iterative-methods.html#splitting-methods"><i class="fa fa-check"></i><b>1.4.4</b> Splitting Methods</a></li>
<li class="chapter" data-level="1.4.5" data-path="iterative-methods.html"><a href="iterative-methods.html#convergence"><i class="fa fa-check"></i><b>1.4.5</b> Convergence</a></li>
<li class="chapter" data-level="1.4.6" data-path="iterative-methods.html"><a href="iterative-methods.html#randomized-kaczmarz-method"><i class="fa fa-check"></i><b>1.4.6</b> Randomized Kaczmarz Method</a></li>
<li class="chapter" data-level="1.4.7" data-path="iterative-methods.html"><a href="iterative-methods.html#gradient-descent"><i class="fa fa-check"></i><b>1.4.7</b> Gradient Descent</a></li>
<li class="chapter" data-level="1.4.8" data-path="iterative-methods.html"><a href="iterative-methods.html#conjugate-gradient"><i class="fa fa-check"></i><b>1.4.8</b> Conjugate Gradient</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html"><i class="fa fa-check"></i><b>1.5</b> Nonlinear Systems of Equations</a><ul>
<li class="chapter" data-level="1.5.1" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#review-of-jacobians"><i class="fa fa-check"></i><b>1.5.1</b> Review of Jacobians</a></li>
<li class="chapter" data-level="1.5.2" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#motivating-problem"><i class="fa fa-check"></i><b>1.5.2</b> Motivating Problem</a></li>
<li class="chapter" data-level="1.5.3" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#non-linear-equations"><i class="fa fa-check"></i><b>1.5.3</b> Non-linear Equations</a></li>
<li class="chapter" data-level="1.5.4" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#picards-method"><i class="fa fa-check"></i><b>1.5.4</b> Picard’s Method</a></li>
<li class="chapter" data-level="1.5.5" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#newtons-method"><i class="fa fa-check"></i><b>1.5.5</b> Newton’s Method</a></li>
<li class="chapter" data-level="1.5.6" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#inexact-newtons-method"><i class="fa fa-check"></i><b>1.5.6</b> Inexact Newton’s Method</a></li>
<li class="chapter" data-level="1.5.7" data-path="nonlinear-systems-of-equations.html"><a href="nonlinear-systems-of-equations.html#semi-smooth-newtons-method"><i class="fa fa-check"></i><b>1.5.7</b> Semi-Smooth Newton’s Method</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html"><i class="fa fa-check"></i><b>1.6</b> Non-linear Unconstrained Optimization</a><ul>
<li class="chapter" data-level="1.6.1" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#motivating-problems-2"><i class="fa fa-check"></i><b>1.6.1</b> Motivating Problems</a></li>
<li class="chapter" data-level="1.6.2" data-path="non-linear-unconstrained-optimization.html"><a href="non-linear-unconstrained-optimization.html#formalize-minimization"><i class="fa fa-check"></i><b>1.6.2</b> Formalize Minimization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="homework-assignments.html"><a href="homework-assignments.html"><i class="fa fa-check"></i><b>2</b> Homework Assignments</a><ul>
<li class="chapter" data-level="2.1" data-path="homework-1.html"><a href="homework-1.html"><i class="fa fa-check"></i><b>2.1</b> Homework 1</a></li>
<li class="chapter" data-level="2.2" data-path="homework-2.html"><a href="homework-2.html"><i class="fa fa-check"></i><b>2.2</b> Homework 2</a></li>
<li class="chapter" data-level="2.3" data-path="homework-3.html"><a href="homework-3.html"><i class="fa fa-check"></i><b>2.3</b> Homework 3</a></li>
<li class="chapter" data-level="2.4" data-path="homework-4.html"><a href="homework-4.html"><i class="fa fa-check"></i><b>2.4</b> Homework 4</a></li>
<li class="chapter" data-level="2.5" data-path="homework-5.html"><a href="homework-5.html"><i class="fa fa-check"></i><b>2.5</b> Homework 5</a></li>
<li class="chapter" data-level="2.6" data-path="homework-6.html"><a href="homework-6.html"><i class="fa fa-check"></i><b>2.6</b> Homework 6</a></li>
<li class="chapter" data-level="2.7" data-path="homework-7.html"><a href="homework-7.html"><i class="fa fa-check"></i><b>2.7</b> Homework 7</a></li>
<li class="chapter" data-level="2.8" data-path="homework-8.html"><a href="homework-8.html"><i class="fa fa-check"></i><b>2.8</b> Homework 8</a></li>
<li class="chapter" data-level="2.9" data-path="homework-9.html"><a href="homework-9.html"><i class="fa fa-check"></i><b>2.9</b> Homework 9</a></li>
<li class="chapter" data-level="2.10" data-path="homework-10.html"><a href="homework-10.html"><i class="fa fa-check"></i><b>2.10</b> Homework 10</a></li>
<li class="chapter" data-level="2.11" data-path="homework-11.html"><a href="homework-11.html"><i class="fa fa-check"></i><b>2.11</b> Homework 11</a></li>
<li class="chapter" data-level="2.12" data-path="homework-12.html"><a href="homework-12.html"><i class="fa fa-check"></i><b>2.12</b> Homework 12</a></li>
<li class="chapter" data-level="2.13" data-path="homework-13.html"><a href="homework-13.html"><i class="fa fa-check"></i><b>2.13</b> Homework 13</a></li>
<li class="chapter" data-level="2.14" data-path="homework-14.html"><a href="homework-14.html"><i class="fa fa-check"></i><b>2.14</b> Homework 14</a></li>
<li class="chapter" data-level="2.15" data-path="homework-15.html"><a href="homework-15.html"><i class="fa fa-check"></i><b>2.15</b> Homework 15</a></li>
<li class="chapter" data-level="2.16" data-path="homework-16.html"><a href="homework-16.html"><i class="fa fa-check"></i><b>2.16</b> Homework 16</a><ul>
<li class="chapter" data-level="2.16.1" data-path="homework-16.html"><a href="homework-16.html#newtons-method-1"><i class="fa fa-check"></i><b>2.16.1</b> Newton’s Method</a></li>
</ul></li>
<li class="chapter" data-level="2.17" data-path="homework-17.html"><a href="homework-17.html"><i class="fa fa-check"></i><b>2.17</b> Homework 17</a></li>
<li class="chapter" data-level="2.18" data-path="homework-18.html"><a href="homework-18.html"><i class="fa fa-check"></i><b>2.18</b> Homework 18</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT 771: My notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="iterative-methods" class="section level2">
<h2><span class="header-section-number">1.4</span> Iterative Methods</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Overview</h3>
<p>First, some references:</p>
<ol style="list-style-type: lower-alpha">
<li>Golub and Van Loan</li>
<li>Saad (2000): <em>Iterative methods for sparse linear systems</em> <a href="https://www-users.cs.umn.edu/~saad/IterMethBook_2ndEd.pdf">link</a></li>
<li>Shewchuk (1994): <em>Introduction to Conjugated Gradients without the Agonizing Pain</em> <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">link</a></li>
<li>Gower and Richtarik (2015): <em>Randomized Iterative Methods for Linear Systems</em> <a href="https://arxiv.org/abs/1506.03296">link</a></li>
</ol>
</div>
<div id="outline" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Outline</h3>
<p>We will be going throuh the following:</p>
<ol style="list-style-type: lower-roman">
<li>Why iterative methods?</li>
<li>Splitting Methods</li>
<li>Randomized Kaezmarz Method</li>
<li>Gradient Descent</li>
<li>Conjugated Gradients</li>
<li>GMRES (optional)</li>
</ol>
</div>
<div id="motivation" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Motivation</h3>
<p>These methods have had a great impact historically speaking. They take advantage of any sparsity found in a matrix, making them “easier” computational.</p>
<p>When working with huge systems (such as weather prediction, computational chemistry, genomics, etc), this is the way to deal with it.</p>
</div>
<div id="splitting-methods" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Splitting Methods</h3>
<p>Let <span class="math inline">\(A\)</span> be a matrix, and <span class="math inline">\(b\)</span> a vector. Let</p>
<ul>
<li><span class="math inline">\(x^c\)</span> denote the current iterate</li>
<li><span class="math inline">\(x^+\)</span> denote the next iterate</li>
<li><span class="math inline">\(r^c\)</span> denote the current residual</li>
<li><span class="math inline">\(r^+\)</span> denote the next residual</li>
</ul>
<p>where <span class="math inline">\(r^c = Ax^c - b\)</span> and <span class="math inline">\(r^+ = Ax^+ - b\)</span>. The goal is to find methods to minimize the residual by iteratively defining <span class="math inline">\(x^+\)</span> based on <span class="math inline">\(x^c\)</span>.</p>
<p>A splitting method takes a matrix <span class="math inline">\(A\)</span> and splits it. One way of doing so is by splitting <span class="math inline">\(A\)</span> into three parts <span class="math inline">\(A = D - E - F\)</span>, where</p>
<ul>
<li><span class="math inline">\(D\)</span> is the diagonal</li>
<li><span class="math inline">\(E\)</span> is the negative lower triangular part of <span class="math inline">\(A\)</span> excluding the diagonal</li>
<li><span class="math inline">\(F\)</span> is the negative upper triangular part of <span class="math inline">\(A\)</span> excluding the diagonal</li>
</ul>
<p>Based on this split, there are a few different ways to iteratively update <span class="math inline">\(r^+\)</span>.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-45" class="definition"><strong>Definition 10  (The Jacobi Method)  </strong></span><em>The Jacobi Method</em> finds <span class="math inline">\(x^+\)</span> from <span class="math inline">\(x^c\)</span> using the following rule:</p>
<p><span class="math display">\[x_i^+ = \frac{b_i - \sum_{k\neq i}A_{ik}x_k^c}{A_{ii}}.\]</span></p>
<p>If we do this for all <span class="math inline">\(i\)</span>, we can write this in matrix form:</p>
<p><span class="math display">\[x^+ = D^{-1}(b + (E+F)x^c).\]</span></p>
</div>

<p>For the Jacobi Method, we can see that <span class="math inline">\(r_i^+ = 0:\)</span></p>
<p><span class="math display">\[\begin{aligned}
  x_i^+ &amp;= \frac{b_i - \sum_{k\neq i}A_{ik}x_k^c}{A_{ii}} \iff \\
  A_{ii}x_i^+ &amp;= b_i - \sum_{k\neq i}A_{ik}x_k^c \iff \\
  0 &amp;= b_i - \sum_{k\neq i}A_{ik}x_k^c - A_{ii}x_i^+ = b_i - \sum_{k}A_{ik}x_k^c = r_i^+.
\end{aligned}\]</span></p>

<div class="definition">
<p><span id="def:unnamed-chunk-46" class="definition"><strong>Definition 11  (The Gauss-Seidel Method)  </strong></span><em>The Gauss-Seidel Method</em> finds <span class="math inline">\(x^+\)</span> from <span class="math inline">\(x^c\)</span> using the following rule:</p>
<p><span class="math display">\[x_i^+ = \frac{1}{A_ii}\left(b_i - \sum_{k=1}^{i-1} A_{ik}x_k^+ - \sum_{k=i+1}^d A_{ik} x_k^c\right),\]</span></p>
<p>which in matrix formulation is</p>
<p><span class="math display">\[x^+ = D^{-1}(b + Ex^+ + Fx^c)\]</span>,</p>
<p>or equivalently</p>
<p><span class="math display">\[x^+ = (D-E)^{-1}(b + Fx^c).\]</span></p>
</div>

<p>As for the Jacobi method, it can be seen that <span class="math inline">\(r_i^+ = 0\)</span>.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-47" class="definition"><strong>Definition 12  (Successive Over Relaxation (SOR))  </strong></span> When we update <span class="math inline">\(x^c\)</span> to <span class="math inline">\(x^+\)</span> using a rule of the form</p>
<p><span class="math display">\[ (D-\omega E)x^+ = (\omega F + (1-\omega)D)x^c + \omega b,\]</span></p>
it is called a <em>Successive Over Relaxation</em> method.
</div>

<p>Note: if we pick <span class="math inline">\(\omega = 1\)</span>, we get back the Gauss-Seidel method.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-48" class="definition"><strong>Definition 13  (Backward SOR)  </strong></span> The following is called the <em>Backward SOR</em>:</p>
<p><span class="math display">\[ (D-\omega E)x^+ = (\omega F + (1-\omega)D)x^c + \omega b,\]</span></p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-49" class="definition"><strong>Definition 14  (Symmetric SOR)  </strong></span> The <em>Symmetric SOR</em> is a method where we first find <span class="math inline">\(z\)</span> based on the rule</p>
<p><span class="math display">\[(D-\omega F)z = (\omega F + (1-\omega)D)x^c+\omega b\]</span></p>
<p>before using the rule</p>
<p><span class="math display">\[(D- \omega F)x^+ = (\omega F + (1-\omega)D)z + \omega b\]</span></p>
to find <span class="math inline">\(x^+\)</span>.
</div>

</div>
<div id="convergence" class="section level3">
<h3><span class="header-section-number">1.4.5</span> Convergence</h3>
<p>All of the schemes mentioned above are of the form <span class="math inline">\(x^+ = G x^c + f\)</span>:</p>
<ul>
<li><p><strong>Jacobi</strong>: <span class="math inline">\(G = D^{-1}(E+F)\)</span>, <span class="math inline">\(f = D^{-1}b\)</span>.</p></li>
<li><p><strong>Gauss-Seidel</strong>: <span class="math inline">\(G = (D-E)^{-1}F\)</span>, <span class="math inline">\(f = (D-E)^{-1} b\)</span>.</p></li>
<li><p><strong>SOR</strong>: <span class="math inline">\(G = (D - \omega E)^{-1}(\omega F + (1- \omega)D)\)</span>, <span class="math inline">\(f = \omega (D-\omega E)^{-1} b\)</span>.</p></li>
</ul>
<p>Now, if we assume that <span class="math inline">\(x^*\)</span> is a vector such that <span class="math inline">\(Ax^* = b\)</span>, then <span class="math inline">\((I-G)x^* = f\)</span>: (for the following, keep in mind that <span class="math inline">\(A = D - E - F\)</span>)</p>
<ul>
<li><strong>Jacobi</strong>:</li>
</ul>
<p><span class="math display">\[\begin{aligned} 
  f &amp;= D^{-1}b \\ 
    &amp;= D^{-1}Ax^* \\
    &amp;= D^{-1}(D-E-F)x^* \\
    &amp;= (DD^{-1} - D^{-1}(E+F))x^*\\
    &amp;= (I - G)x^*.
\end{aligned}\]</span></p>
<ul>
<li><strong>Gauss-Seidel</strong>:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
  f &amp;= (D-E)^{-1}b \\
    &amp;= (D-E)^{-1}Ax^* \\
    &amp;= (D-E)^{-1}(D-E-F)x^* \\
    &amp;= ((D-E)^{-1}(D-E)-(D-E)^{-1}F)x^* \\
    &amp;= (I - G)x^*.
\end{aligned}\]</span></p>
<ul>
<li><strong>SOR</strong>:</li>
</ul>
<p><span class="math display">\[\begin{aligned}
  f &amp;= {\omega}(D-{\omega}E)^{-1}b\\
    &amp;= {\omega}(D-{\omega}E)^{-1}Ax^* \\
    &amp;= {\omega}(D-{\omega}E)^{-1}(D-E-F)x^* \\
    &amp;= (D-{\omega}E)^{-1}({\omega}D-{\omega}E-{\omega}F)x^* \\
    &amp;= (D-{\omega}E)^{-1}(D - D + {\omega}D-{\omega}E-{\omega}F)x^* \\
    &amp;= (D-{\omega}E)^{-1}(D -{\omega}E - [(1-{\omega})D + {\omega}F])x^* \\
    &amp;= ((D-{\omega}E)^{-1}(D -{\omega}E) - (D-{\omega}E)^{-1}[D(1-{\omega}) + {\omega}F])x^* \\
    &amp;= (I - G)x^*.
\end{aligned}\]</span></p>

<div class="lemma">
<span id="lem:next-current-diff" class="lemma"><strong>Lemma 13  </strong></span>For the Jacobi, Gauss-Seidel, and SOR methods, if there exists a <span class="math inline">\(x^*\)</span> s.t. <span class="math inline">\(Ax^* = b\)</span>, then <span class="math inline">\(x^+ - x^* = G(x^c - x^*)\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span>  We just saw that <span class="math inline">\(Ax^* = b\)</span> implies <span class="math inline">\((I-G)x^* = f\)</span>. We also saw that <span class="math inline">\(x^+ = Gx^c + f\)</span>. Hence</p>
<span class="math display">\[\begin{aligned}
  x^+ &amp;= G x^c + f \\
      &amp;= G x^c + (I-G)x^* \\
      &amp;= G x^c + x^* - Gx^* \iff \\
  x^+ - x^* &amp;= G(x^c - x^*).
\end{aligned}\]</span>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-51" class="theorem"><strong>Theorem 6  </strong></span> Suppose there exists a <span class="math inline">\(x^*\)</span> s.t. <span class="math inline">\(A x^* = b\)</span>. Let <span class="math inline">\(x_0\)</span> be arbitrary and define a sequence <span class="math inline">\(\{x_k\}_{k \in {\mathbb{N}}}\)</span> by</p>
<p><span class="math display">\[
  x_k = Gx_{k-1} + f.
\]</span></p>
If <span class="math inline">\(\rho(G) &lt; 1\)</span>, then <span class="math inline">\(x^*\)</span> is unique, and <span class="math inline">\(x_k \rightarrow x^*\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span>.
</div>

<p>In the theorem above, <span class="math inline">\(\rho(G)\)</span> is the spectral radius of the matrix <span class="math inline">\(G\)</span>. This is defined as the largest eigenvalue of the matrix <span class="math inline">\(G\)</span>.</p>

<div class="definition">
<p><span id="def:jordan" class="definition"><strong>Definition 15  (Jordan Canonical Form)  </strong></span>Let <span class="math inline">\(A \in {\mathbb{R}}^{n \times n}\)</span>. There exists an <span class="math inline">\(X\)</span> which is invertible, and a block-diagonal matrix <span class="math inline">\(J\)</span>, whose blocks are of the form <span class="math inline">\(\lambda I + E\)</span>, where <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(A\)</span>, and</p>
<p><span class="math display">\[
  E = \left\{ \begin{array}{rl} 1, &amp; j = i+1 \\ 0, &amp; \text{ otherwise} \end{array} \right .
\]</span></p>
<p>such that <span class="math inline">\(A = XJX^{-1}\)</span>.</p>
</div>

</div>
<div id="randomized-kaczmarz-method" class="section level3">
<h3><span class="header-section-number">1.4.6</span> Randomized Kaczmarz Method</h3>
<p>Consider <span class="math inline">\(Ax = b\)</span>. Let the rows of <span class="math inline">\(A\)</span> be denoted as <span class="math inline">\(a_1^\prime, a_2^\prime, \dots, a_n^\prime\)</span>, and <span class="math inline">\(b = (b_1, \dots, b_n)^\prime\)</span>. Now, the goal is to choose the next iteration of <span class="math inline">\(x\)</span> such that <span class="math inline">\(r^+ = b_i - a_i^\prime x^+ = 0\)</span>. Let’s say we try to set <span class="math inline">\(x^+ = x^c + v\)</span> for some appropriate <span class="math inline">\(v\)</span>. What should <span class="math inline">\(v\)</span> be then? We want <span class="math inline">\(b_i = a_i^\prime (x^c + v)\)</span>, which would imply that <span class="math inline">\(a_i^\prime v = b_i - a_i^\prime x^c\)</span>. This has many possible solutions. So let’s look for a particular one, namely one that is proportional to <span class="math inline">\(a_i\)</span>: <span class="math inline">\(v=\alpha a_i\)</span>. Then <span class="math inline">\(\alpha {\left \vert \left \vert a_i \right \vert \right \vert}_2^2 = b_i - a_i^\prime x^c\)</span> which implies <span class="math inline">\(\alpha = \frac{b_i - a_i^\prime x^c}{{\left \vert \left \vert a_i \right \vert \right \vert}_2^2}\)</span>.</p>
<p>The above approach gives us Kaczmarz Method: the next iterate of <span class="math inline">\(x\)</span> is <span class="math inline">\(x^+ = x^c + \frac{b_i - a_i^\prime x^c}{{\left \vert \left \vert a_i \right \vert \right \vert}_2^2}a_i\)</span>.</p>
<p>Now, the <em>Randomized</em> Kaczmarz Method is the one where we randomly choose which column to use for the next iteration based on the probability distribution over all columns of <span class="math inline">\(A\)</span> given by <span class="math inline">\(P(i = l) = \frac{{\left \vert \left \vert a_l \right \vert \right \vert}_2^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}\)</span>. The following theorem guarantees us that this approach actually works, i.e. we converge towards the solution of <span class="math inline">\(Ax = b\)</span>.</p>

<div class="theorem">
<p><span id="thm:rand-kaczmarz" class="theorem"><strong>Theorem 7  (Randomized Kaczmarz Method)  </strong></span>Suppose <span class="math inline">\(A \in {\mathbb{R}}^{n\times m}\)</span> is invertible and <span class="math inline">\(x^* = A^{-1}b\)</span>.</p>
<p>Given a sequence of i.i.d. random variables <span class="math inline">\(i_1, i_2, \dots\)</span> (<span class="math inline">\(P(i_k = l) = \frac{{\left \vert \left \vert a_l \right \vert \right \vert}_2^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}\)</span>), and <span class="math inline">\(x_0\)</span> an arbitrary initial value, define</p>
<p><span class="math display">\[ x_k = x_{k-1} + \frac{b_{i_{k-1}}-a_{i_{k-1}}^\prime x_{k-1}}{{\left \vert \left \vert a_{i_{k-1}} \right \vert \right \vert}_2^2}a_{i_{k-1}}\]</span></p>
<p>for all <span class="math inline">\(k &gt; 1\)</span>. Then</p>
<p><span class="math display">\[E\left[{\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2 \right] \leq \left(1-\frac{\sigma_{min}(A)^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}\right)^k {\left \vert \left \vert x_0 - x^* \right \vert \right \vert}_2^2.\]</span></p>
</div>

<p>Note that the above result (by Chebyshev) implies</p>
<p><span class="math display">\[P\left[{\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2 &gt; \epsilon \right] \leq \frac{1}{\epsilon^2}E\left[{\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2\right]\]</span></p>
<p>which in turn implies <span class="math inline">\(x_k \rightarrow_p x^*\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span>.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> First of all, note that since <span class="math inline">\(i_k\)</span> is independent of <span class="math inline">\(i_{l}\)</span> for all <span class="math inline">\(l &lt; k\)</span>, <span class="math inline">\(i_k\)</span> is independent of all <span class="math inline">\(x_l\)</span> for all <span class="math inline">\(l &lt; k\)</span>.</p>
<p><strong>Observation 1:</strong> <span class="math inline">\(x_{k+1} - x_k\)</span> is orthogonal to <span class="math inline">\(x_{k+1} - x^*\)</span>. Therefore <span class="math display">\[{\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert}_2^2 = {\left \vert \left \vert x_{k} - x^* \right \vert \right \vert}_2^2 - {\left \vert \left \vert x_{k+1}-x_k \right \vert \right \vert}_2^2.\]</span></p>
<p>Since <span class="math inline">\(b_{i_k} = a_{i_k}^\prime x^*\)</span>, we have that</p>
<p><span class="math display">\[\begin{aligned}
  x_{k+1} - x_k &amp;= \frac{b_{i_k}-a_{i_k}^\prime x_k}{{\left \vert \left \vert a_{i_k} \right \vert \right \vert}_2^2}a_{i_k} \\
                &amp;= \frac{a_{i_k}^\prime x^*-a_{i_k}^\prime x_k}{{\left \vert \left \vert a_{i_k} \right \vert \right \vert}_2^2}a_{i_k} \\
                &amp;= \frac{a_{i_k}^\prime (x^*- x_k)}{{\left \vert \left \vert a_{i_k} \right \vert \right \vert}_2^2}a_{i_k}.
\end{aligned}\]</span></p>
<p>So</p>
<p><span class="math display">\[\begin{aligned}
  {\left \vert \left \vert x_{k+1} - x_k \right \vert \right \vert}_2^2  &amp;= {\left \vert \left \vert a_{i_k} \right \vert \right \vert}_2^2 \frac{[a_{i_k}^\prime (x_k - x^*)]^2}{{\left \vert \left \vert a_{i_k} \right \vert \right \vert}_2^4} \\
                            &amp;= \frac{[a_{i_k}^\prime (x_k - x^*)]^2}{{\left \vert \left \vert a_{i_k} \right \vert \right \vert}_2^2}.
\end{aligned}\]</span></p>
<p><strong>Observation 2:</strong> For any vector <span class="math inline">\(y \in {\mathbb{R}}^n\)</span> it holds that</p>
<p><span class="math display">\[E\left(\frac{(a_{i_0}^\prime y)^2}{{\left \vert \left \vert a_{i_0} \right \vert \right \vert}_2^2}\right) \geq \frac{\sigma_{min}^2(A)}{{\left \vert \left \vert A \right \vert \right \vert}_F^2} {\left \vert \left \vert y \right \vert \right \vert}_2^2.\]</span></p>
<p>“Proof:”</p>
<p>Recall that <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_2^2 = \max_{y \neq 0} \frac{{\left \vert \left \vert Ay \right \vert \right \vert}_2^2}{{\left \vert \left \vert y \right \vert \right \vert}_2^2} = \sigma_{max}(A)^2 &gt;= \sigma_{min}(A)^2\)</span>. With this in mind</p>
<p><span class="math display">\[\begin{aligned}
  \sum_{i=1}^n (a_i^\prime y)^2 &amp;= {\left \vert \left \vert Ay \right \vert \right \vert}_2^2 \ge \sigma_{min}(A)^2 {\left \vert \left \vert y \right \vert \right \vert}_2^2.
\end{aligned}\]</span></p>
<p>Divide through by <span class="math inline">\({\left \vert \left \vert A \right \vert \right \vert}_F^2\)</span> to get <span class="math inline">\(\sum_{i=1}^n \frac{1}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}(a_i^\prime y)^2 \ge \frac{\sigma_{min}(A)^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2} {\left \vert \left \vert y \right \vert \right \vert}_2^2\)</span>, before we simply multiply each term in the sum by <span class="math inline">\(1 = \frac{{\left \vert \left \vert a_i \right \vert \right \vert}_2^2}{{\left \vert \left \vert a_i \right \vert \right \vert}_2^2}\)</span>. So</p>
<p><span class="math display">\[\begin{aligned}
  \frac{\sigma_{min}(A)^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2} {\left \vert \left \vert y \right \vert \right \vert}_2^2 &amp;\le \sum_{i=1}^n \frac{{\left \vert \left \vert a_i \right \vert \right \vert}_2^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}\frac{(a_i^\prime y)^2}{{\left \vert \left \vert a_i \right \vert \right \vert}_2^2} \\
                                                      &amp;= \sum_{i=1}^n P(i_0 = i) \frac{(a_i^\prime y)^2}{{\left \vert \left \vert a_i \right \vert \right \vert}_2^2} \\
                                                      &amp;= E\left(\frac{(a_{i_0}^\prime y)^2}{{\left \vert \left \vert a_{i_0} \right \vert \right \vert}_2^2}\right).
\end{aligned}\]</span></p>
<p>Now, using these two observations together:</p>
<p><span class="math display">\[\begin{aligned}
  E\left[{\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert}_2^2 | x_k \right ] &amp;= E\left[{\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2 | x_k\right] - E\left[ {\left \vert \left \vert x_{k+1} - x_k \right \vert \right \vert}_2^2 | x_k \right] \\
                                                 &amp;= {\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2 - E\left[ \frac{(a_{i_k}^\prime (x_k - x^*))^2}{{\left \vert \left \vert a_{i_k} \right \vert \right \vert}_2^2} \right] \\
                                                 &amp;\le {\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2 - \frac{\sigma_{min}(A)^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2} {\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2 \\
                                                 &amp;= (1 - \frac{\sigma_{min}(A)^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}){\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2.
\end{aligned}\]</span></p>
<p>Finally, taking expectation on both sides gives us that <span class="math inline">\(E\left[{\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert}_2^2\right] \le \left(1 - \frac{\sigma_{min}(A)^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}\right)E\left[{\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2^2\right]\)</span>. This upper bound holds for any <span class="math inline">\(k\)</span>, so we can repeatedly use this to get</p>
<p><span class="math display">\[
  E\left[{\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert}_2^2\right] \le \left(1 - \frac{\sigma_{min}(A)^2}{{\left \vert \left \vert A \right \vert \right \vert}_F^2}\right)^k {\left \vert \left \vert x_0 - x^* \right \vert \right \vert}_2^2
\]</span></p>
since <span class="math inline">\(E[{\left \vert \left \vert x_0 - x^* \right \vert \right \vert}_2^2] = {\left \vert \left \vert x_0 - x^* \right \vert \right \vert}_2^2\)</span>.
</div>

</div>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">1.4.7</span> Gradient Descent</h3>
<p>The Gradient Descent iterative method updates the current <span class="math inline">\(x^c\)</span> to <span class="math inline">\(x^+\)</span> by</p>
<span class="math display">\[\begin{align}
  x^+ &amp;= x^c + \alpha \sum_{i=1}^n a_i (b_i - a_i x^c) \\
      &amp;= x^c + \alpha A^\prime (b - Ax^c)
\end{align}\]</span>
<p>The question now is: how do we choose <span class="math inline">\(\alpha\)</span>?</p>
<p><strong>Strategy 1</strong></p>
<p>Choose the <span class="math inline">\(\alpha\)</span> that minimizes <span class="math inline">\({\left \vert \left \vert r^+ \right \vert \right \vert}_2^2 = {\left \vert \left \vert Ax^+ - b \right \vert \right \vert}_2^2\)</span>.</p>

<div class="lemma">
<p><span id="lem:unnamed-chunk-53" class="lemma"><strong>Lemma 14  </strong></span> <span class="math display">\[{\text{argmin}}_\alpha {\left \vert \left \vert r^+ \right \vert \right \vert}_2^2 = \frac{{\left \vert \left \vert A^\prime r^c \right \vert \right \vert}_2^2}{{\left \vert \left \vert AA^\prime r^c \right \vert \right \vert}_2^2}\]</span></p>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> First, note that by homework exercise <a href="homework-11.html#exr:q1102">67</a>, <span class="math inline">\(A^\prime r^c \neq 0\)</span>, since if this were the case, the problem would be solved already.</p>
<p><span class="math display">\[
  {\left \vert \left \vert r^+ \right \vert \right \vert}_2^2 = {\left \vert \left \vert r^c \right \vert \right \vert}_2^2 - 2\alpha {\left \vert \left \vert A^\prime r^c \right \vert \right \vert}_2^2 + \alpha^2 {\left \vert \left \vert AA^\prime r^c \right \vert \right \vert}_2^2,
\]</span></p>
<p>since <span class="math inline">\(r^+ = Ax^+ - b = A(x^c + \alpha A^\prime (b - Ax^c)) - b = Ax^c - b - \alpha A A^\prime ( Ax^c - b) = r^c - \alpha A A^\prime r^c\)</span>, and <span class="math inline">\({\left \vert \left \vert r^+ \right \vert \right \vert}_2^2 = (r^+)^\prime r^+\)</span>.</p>
Differentiate this equation with respect to <span class="math inline">\(\alpha\)</span>, equate to <span class="math inline">\(0\)</span>, and solve for <span class="math inline">\(\alpha\)</span> will yield the result.
</div>


<div class="theorem">
<p><span id="thm:grad-descent" class="theorem"><strong>Theorem 8  </strong></span>Suppose <span class="math inline">\(A\)</span> is invertible. Given <span class="math inline">\(x_0 {\mathbb{R}}^n\)</span>, let <span class="math inline">\(x_k\)</span> be defined as</p>
<p><span class="math display">\[x_k = x_{k-1} + \alpha_{k-1}A^\prime (b-Ax_{k-1}),\]</span></p>
<p>where</p>
<p><span class="math display">\[ \alpha_k = \frac{{\left \vert \left \vert A^\prime r_k \right \vert \right \vert}_2^2}{{\left \vert \left \vert AA^\prime r_k \right \vert \right \vert}_2^2}.\]</span></p>
<p>Then</p>
<p><span class="math display">\[
  {\left \vert \left \vert r_k \right \vert \right \vert}_2^2 \le \left(1 - \frac{4\sigma_1^2 \sigma_n^2}{\sigma_1^2 + \sigma_n^2}\right)^k {\left \vert \left \vert r_0 \right \vert \right \vert}_2^2.
\]</span></p>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span>  <span class="math display">\[\begin{aligned}
  {\left \vert \left \vert r_{k+1} \right \vert \right \vert}_2^2 &amp;= {\left \vert \left \vert r_k \right \vert \right \vert}_2^2 - \alpha_k 2{\left \vert \left \vert A^\prime r_k \right \vert \right \vert}_2^2 + \alpha_k^2 {\left \vert \left \vert AA^\prime r_k \right \vert \right \vert}_2^2 \\
                     &amp;= {\left \vert \left \vert r_k \right \vert \right \vert}_2^2 - \frac{{\left \vert \left \vert A^\prime r_k \right \vert \right \vert}_2^4}{{\left \vert \left \vert A A^\prime r_k \right \vert \right \vert}_2^2} \\
                     &amp;= {\left \vert \left \vert r_k \right \vert \right \vert}_2^2 \left(1 - \frac{{\left \vert \left \vert A^\prime r_k \right \vert \right \vert}_2^4}{{\left \vert \left \vert r_k \right \vert \right \vert}_2^2 {\left \vert \left \vert AA^\prime r_k \right \vert \right \vert}_2^2}\right).
\end{aligned}\]</span></p>
<p>If we let <span class="math inline">\(A = U\Sigma V^\prime\)</span> be the SVD of <span class="math inline">\(A\)</span>, then</p>
<p><span class="math display">\[\begin{aligned}
  1 - \frac{{\left \vert \left \vert A^\prime r_k \right \vert \right \vert}_2^4}{{\left \vert \left \vert r_k \right \vert \right \vert}_2^2 {\left \vert \left \vert AA^\prime r_k \right \vert \right \vert}_2^2} &amp;= 1 - \frac{{\left \vert \left \vert \Sigma U^\prime r_k \right \vert \right \vert}_2^4}{{\left \vert \left \vert U^\prime r_k \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma \Sigma^\prime U^\prime r_k \right \vert \right \vert}_2^2} \\
    &amp;= 1 - \frac{{\left \vert \left \vert w \right \vert \right \vert}_2^4}{{\left \vert \left \vert \Sigma^{-1}w \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma w \right \vert \right \vert}_2^2}.
\end{aligned}\]</span></p>
<p>We want to find an upper bound on <span class="math inline">\({\left \vert \left \vert \Sigma^{-1}u \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma u \right \vert \right \vert}_2^2\)</span> for any unit vector <span class="math inline">\(u\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
  {\left \vert \left \vert \Sigma^{-1}u \right \vert \right \vert}_2^2 {\left \vert \left \vert \Sigma u \right \vert \right \vert}_2^2 &amp;= \left( \sum_{i=1}^k \frac{u_i^2}{\sigma_i^2} \right) \left( \sum_{i=1}^n \sigma_i^2 u_i^2\right) \\
  \le \frac{(\sigma_1^2 + \sigma_n^2)^2}{4\sigma_1^2 \sigma_n^2},
\end{aligned}\]</span></p>
<p>where the last inequality is the Kontorovich inequality (see homework <a href="homework-11.html#exr:q1104">69</a>).</p>
</div>

<p><strong>Strategy 2</strong></p>
<p>Choose <span class="math inline">\(\alpha = {\text{argmin}}_\alpha {\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert}_2^2\)</span>.</p>
<p><strong>Strategy 3</strong></p>
<p>Note that <span class="math inline">\({\left \vert \left \vert r^+ \right \vert \right \vert}_2 = {\left \vert \left \vert (I - \alpha A A^\prime)r^c \right \vert \right \vert}_2 \le {\left \vert \left \vert I-\alpha A A^\prime \right \vert \right \vert}_2 {\left \vert \left \vert r^c \right \vert \right \vert}_2\)</span>.</p>
<p>Choose <span class="math inline">\(\alpha = {\text{argmin}}_\alpha {\left \vert \left \vert I - \alpha A A^\prime \right \vert \right \vert}_2\)</span>.</p>
<p><strong>Strategy 4</strong></p>
<p>Note that <span class="math inline">\({\left \vert \left \vert x_{k+1} - x^* \right \vert \right \vert}_2 \le {\left \vert \left \vert I-\alpha A A^\prime \right \vert \right \vert}_2 {\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2\)</span>.</p>
<p>Choose <span class="math inline">\(\alpha = {\text{argmin}}_\alpha {\left \vert \left \vert I-\alpha A A^\prime \right \vert \right \vert}_2 {\left \vert \left \vert x_k - x^* \right \vert \right \vert}_2\)</span>.</p>
</div>
<div id="conjugate-gradient" class="section level3">
<h3><span class="header-section-number">1.4.8</span> Conjugate Gradient</h3>
<div id="orthogonal-search-directions" class="section level4">
<h4><span class="header-section-number">1.4.8.1</span> Orthogonal Search Directions</h4>
<p>Let <span class="math inline">\(A \in {\mathbb{R}}^{d \times d}\)</span> be a symmetric positive definite matrix, <span class="math inline">\(\{x_k\}\)</span> be a sequence where <span class="math inline">\(x_0\)</span> is arbitrary, and</p>
<p><span class="math display">\[x_{k+1} = x_k - \alpha_k s_k.\]</span></p>
<p>We call <span class="math inline">\(\alpha_k\)</span> the step length, and <span class="math inline">\(s_k\)</span> the search direction.</p>
<p>The question is then: given an orthonormal set of vectors <span class="math inline">\(\{s_k\}\)</span> and an arbitrary <span class="math inline">\(x_0\)</span>, how do we choose <span class="math inline">\(\alpha_k\)</span> such that we eventually find <span class="math inline">\(x^*\)</span> with <span class="math inline">\(Ax^* = b\)</span>?</p>
<p>From <span class="math inline">\(\{s_k\}\)</span>, take all unique vectors: <span class="math inline">\(s_0, s_1, ..., s_{d_1}\)</span> (note: we there are at most <span class="math inline">\(d\)</span> since these are orthonormal vectors from <span class="math inline">\({\mathbb{R}}^d\)</span>). Since we assumed that <span class="math inline">\(A\)</span> is positive definite, it is invertible. Let <span class="math inline">\(x^* = A^{-1}x\)</span> and take <span class="math inline">\(\alpha_k = \left&lt;x_0 - x^*, s_k\right&gt;\)</span>.</p>
<p>This gives us that</p>
<span class="math display">\[\begin{align}
  x_0 - x^* &amp;= \sum_{i=0}^{d-1} \alpha_i s_i \iff \\
  x_0 - \sum_{i=0}^{d-1} \alpha_i s_i &amp;= x^*
\end{align}\]</span>
<p>But since <span class="math inline">\(x_k - x_{k+1} = \alpha_k s_k\)</span>,</p>
<span class="math display">\[\begin{align}
  x^* &amp;= x_0 - \sum_{i=0}^{d-1} \alpha_i s_i \\
      &amp;= x_0 - \sum_{i=0}^{d-1} (x_i - x_{i+1}) \\ 
      &amp;= x_d.
\end{align}\]</span>
<p>So, this choice of <span class="math inline">\(\alpha_k\)</span> ensures that we will eventually find the solution. Since <span class="math inline">\(A\)</span> is symmetric and positive definite,</p>
<span class="math display">\[\begin{align}
  \alpha_k &amp;= (x_0 - x^*)^\prime s_k \\
           &amp;= (Ax_0 - Ax^*)^\prime A^{-1} s_k \\
           &amp;= (Ax_0 - b)^\prime A^{-1} s_k \\
           &amp;= \left&lt;Ax_0 - b, s_k\right&gt;_{A^{-1}}.
\end{align}\]</span>
<p>This shows us that our choice of <span class="math inline">\(\alpha_k\)</span> is actually the inner product of <span class="math inline">\(Ax_0 - b\)</span> and <span class="math inline">\(s_k\)</span> with respect to the matrix <span class="math inline">\(A^{-1}\)</span>. But calculating <span class="math inline">\(A^{-1}\)</span> requires a lot of effort.</p>
<p>Now, what if we use the inner product <span class="math inline">\(\left&lt;\cdot,\cdot\right&gt;_A\)</span> instead? I.e. <span class="math inline">\(\alpha_k = \left&lt;x_0-x^*,s_k\right&gt;_A = (Ax_0 - Ax^*)^\prime s_k = r_0^\prime s_k\)</span>. This we can actually calculate. The problem now is that the <span class="math inline">\(s_k\)</span>’s are not orthonormal in this new metric. So, we need a way to create a set that is:</p>
</div>
<div id="conjugation" class="section level4">
<h4><span class="header-section-number">1.4.8.2</span> Conjugation</h4>

<div class="definition">
<span id="def:unnamed-chunk-56" class="definition"><strong>Definition 16  (Conjugated Set)  </strong></span>A set of vectors <span class="math inline">\(\{s_0, \ldots, s_{d-1}\} \subset {\mathbb{R}}^d\)</span> are (<span class="math inline">\(A-\)</span>)conjugated if they are orthogonal w.r.t. the metric <span class="math inline">\(&lt;\cdot,\cdot&gt;_A\)</span> (i.e. <span class="math inline">\(s_i^\prime A s_j = 0\)</span> for all <span class="math inline">\(i \neq j\)</span>).
</div>

<p>We can use Gram-Schmidt to do this: given a set of linearly independent vectors <span class="math inline">\(\{a_0, ..., a_{d-1}\}\)</span>, let <span class="math inline">\(s_0 = \frac{a_0}{{\left \vert \left \vert a_0 \right \vert \right \vert}_A}\)</span> and</p>
<p><span class="math display">\[
  s_j = \frac{a_j - \sum_{i=0}^{j-1} &lt;a_j, s_i&gt;_A \cdot s_i}{{\left \vert \left \vert a_j - \sum_{i=0}^{j-1} &lt;a_j, s_i&gt;_A \cdot s_i \right \vert \right \vert}_A}
\]</span></p>
<p>for all <span class="math inline">\(j &gt; 0\)</span>.</p>
<p>Given a set <span class="math inline">\(\{s_0, ..., s_{d-1}\}\)</span> that is <span class="math inline">\(A-\)</span>conjugated and an arbitrary <span class="math inline">\(x_0\)</span>, pick</p>
<p><span class="math display">\[\alpha_k = \frac{&lt;x_0-x^*, s_k&gt;_A}{{\left \vert \left \vert s_k \right \vert \right \vert}_A} = \frac{r_0^\prime s_k}{{\left \vert \left \vert s_k \right \vert \right \vert}_A}.\]</span></p>

<div class="lemma">
<p><span id="lem:unnamed-chunk-57" class="lemma"><strong>Lemma 15  </strong></span>Let <span class="math inline">\(A \in {\mathbb{R}}^{d\times d}\)</span> be a symmetric, positive definite matrix (i.e. all eigenvalues are positive), <span class="math inline">\(b\in {\mathbb{R}}^d\)</span> be arbitrary, and <span class="math inline">\(x^* \in {\mathbb{R}}^d\)</span> such that <span class="math inline">\(Ax^* = b\)</span>. Let <span class="math inline">\(\{s_0, ..., s_{d-1}\}\)</span> be an <span class="math inline">\(A-\)</span>conjugated and <span class="math inline">\(A-\)</span>normalized set, and <span class="math inline">\(x_0 \in {\mathbb{R}}^d\)</span> be arbitrary.</p>
<p>Define the sequence <span class="math inline">\(\{x_k\}\)</span> by <span class="math inline">\(x_{k+1} = x_k - \alpha_k s_k\)</span>, where <span class="math inline">\(\alpha_k = r_k^\prime s_k\)</span>. Then the following hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\alpha_k = r_0^\prime s_k\)</span>,</li>
<li><span class="math inline">\(0 = r_{k+1}^\prime s_j\)</span> for all <span class="math inline">\(j=0,1,...,k\)</span>.</li>
<li><span class="math inline">\(x_d = x^*\)</span>.</li>
</ol>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> By definition, <span class="math inline">\(x_k = x_0 - \sum_{i=0}^{k-1} \alpha_i s_i\)</span>. This implies that <span class="math inline">\(Ax_k = r_k = r_0 - \sum_{i=0}^{k-1} \alpha_i A s_i\)</span>.</p>
<p>WTS (1):</p>
<p><span class="math display">\[\begin{aligned}
  \alpha_k &amp;= s_k^\prime r_k \\
           &amp;= s_k^\prime r_0 - \sum_{i=0}^{k-1}\alpha_i s_k^\prime A s_i \\
           &amp;= s_k^\prime r_0 - \sum_{i=0}^{k-1}\alpha_i &lt;s_k,s_i&gt;_A \\
           &amp;= s_k^\prime r_0
\end{aligned}\]</span></p>
<p>WTS (2): Let <span class="math inline">\(l &lt; k\)</span>. Recall that <span class="math inline">\(A\)</span> is symmetrical, i.e. <span class="math inline">\(A^\prime = A\)</span>, and <span class="math inline">\(s_i^\prime A s_l = 0\)</span> for all <span class="math inline">\(i \neq l\)</span>. Now,</p>
<p><span class="math display">\[\begin{aligned}
  r_{k+1}^\prime s_l &amp;= \left(r_0 - \sum_{i=0}^k \alpha_i A s_i\right)^\prime s_l \\
                     &amp;= r_0^\prime s_l - \sum_{i=0}^k \alpha_i s_i^\prime A^\prime s_l \\
                     &amp;= \alpha_l - \alpha_l s_l^\prime A s_l = 0.
\end{aligned}\]</span></p>
<p>WTS (3): Suppose <span class="math inline">\(x_d \neq x^*\)</span>. Then <span class="math inline">\(r_d = Ax_d - b \neq 0\)</span>. But (2) implies that <span class="math inline">\(r_d^\prime s_l = 0\)</span> for all <span class="math inline">\(l &lt; d\)</span>. I.e. <span class="math inline">\(r_d\)</span> is linearly independent of <span class="math inline">\(\text{span}\{s_0, ..., s_{d-1}\} \subset {\mathbb{R}}^d\)</span>, so <span class="math inline">\(r_d\)</span> must be <span class="math inline">\(0\)</span>, which contradicts the assumption that <span class="math inline">\(x_d \neq x^*\)</span>.</p>
</div>

</div>
<div id="conjugatedgradients" class="section level4">
<h4><span class="header-section-number">1.4.8.3</span> What is the choice of vectors to conjugate?</h4>
<p>Conjugate gradient <span class="math inline">\(A-\)</span>conjugates residuals <span class="math inline">\(r_0, ..., r_{d-1}\)</span>.</p>
<p><strong>Step 0</strong>: given arbitrary <span class="math inline">\(x_0 \in {\mathbb{R}}^d\)</span>, compute <span class="math inline">\(s_0 = \frac{r_0}{{\left \vert \left \vert r_0 \right \vert \right \vert}_A}\)</span>. Update the iterate to <span class="math inline">\(x_1 = x_0 - \alpha_0 s_0\)</span>, where <span class="math inline">\(\alpha_0 = r_0^\prime s_0\)</span>, and update the residual to <span class="math inline">\(r_1 = r_0 - \alpha_0 A s_0\)</span>.</p>
<p><strong>Step 1</strong>:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(U_{12} = &lt;r_1, s_0&gt;_A = r_1^\prime A s_0 = r_1^\prime \frac{r_0-r_1}{\alpha_0} = -\frac{{\left \vert \left \vert r_1 \right \vert \right \vert}^2}{\alpha_0}\)</span>, since <span class="math inline">\(r_0 \perp r_1\)</span>.</li>
<li>Unnormalized <span class="math inline">\(A-\)</span>conjugate direction: <span class="math inline">\(P_1 = r_1 - U_{12} \cdot s_0\)</span>.</li>
<li>Normalize and compute search direction: <span class="math inline">\(U_{22} = {\left \vert \left \vert P_1 \right \vert \right \vert}_A\)</span> and <span class="math inline">\(s_1 = \frac{P_1}{U_{22}}\)</span>.</li>
</ol>
<p><em>Update</em>: <span class="math inline">\(x_2 = x_1 - \alpha_1 s_1\)</span> and <span class="math inline">\(r_2 = r_1 - \alpha_1 A s_1\)</span>, where <span class="math inline">\(\alpha_1 = r_1^\prime s_1 = \left(r_1^\prime r_1 - U_{12} r_1^\prime s_0\right)/U_{22} = \frac{{\left \vert \left \vert r_1 \right \vert \right \vert}_2^2}{U_{22}}\)</span> (since <span class="math inline">\(r_1^\prime s_0 = 0\)</span>).</p>
<p>Check that lemma still holds:</p>
<ol style="list-style-type: lower-roman">
<li>step length: <span class="math inline">\(\alpha_1 = r_1^\prime s_1 = (r_0 - \alpha_0 A s_0)^\prime s_1 = r_0^\prime s_1\)</span></li>
<li>orthogonality: <span class="math inline">\(r_2^\prime s_j = r_0 - \alpha_0 A s_0 - \alpha_1 A s_1)^\prime s_j = r_0^\prime s_j - \alpha_j = 0\)</span>.</li>
</ol>
<p>A few key observations:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(r_0 \in \text{span}\{s_0\}, r_1 \in \text{span}\{s_0,s_1\}\)</span>.</li>
<li>since <span class="math inline">\(r_2 \perp s_0, s_1\)</span>, <span class="math inline">\(r_2^\prime r_j = 0\)</span> for <span class="math inline">\(j=0,1\)</span>.</li>
</ol>
<p>In general, let’s say we’re at step <span class="math inline">\(L\)</span>. This means we have already calculated <span class="math inline">\(s_0, ..., s_{L-1}\)</span>, <span class="math inline">\(r_0,...,r_L\)</span>, and <span class="math inline">\(x_0,...,x_L\)</span>. To get the next iterate, we would do the following:</p>
<ol style="list-style-type: upper-alpha">
<li>Gram-Schmidt:
<ol style="list-style-type: decimal">
<li>Compute the inner products for <span class="math inline">\(x_L\)</span> with <span class="math inline">\(s_j\)</span> for all <span class="math inline">\(j &lt; L\)</span>. Recall that <span class="math inline">\(r_{j+1} = r_j - \alpha_j r_L^\prime A s_j\)</span>. For <span class="math inline">\(j &lt; L-1\)</span>: <span class="math inline">\(&lt;r_L, s_j&gt;_A = r_L^\prime A s_j = r_L^\prime \left(\frac{r_{j+1} - r_j}{-\alpha_j}\right) = 0\)</span>, since <span class="math inline">\(r_L \perp s_j\)</span> for all <span class="math inline">\(j &lt; L-1\)</span>, and <span class="math inline">\(r_j\)</span> is simply a linear combination of these. &lt;&gt; For <span class="math inline">\(j = L-1\)</span>: <span class="math inline">\(&lt;r_L, s_{L-1}&gt;_A = r_A^\prime \left(\frac{r_L - r_{L-1}}{-\alpha_{L-1}}\right) = \frac{{\left \vert \left \vert r_L \right \vert \right \vert}_2^2}{-\alpha_{L-1}}\)</span>.</li>
<li>Find unnormalized search direction:  <span class="math inline">\(P_L = r_L - \sum_{j=0}^{L-1} &lt;r_L, s_j&gt;_A s_j = r_L + \frac{{\left \vert \left \vert r_L \right \vert \right \vert}_2^2}{\alpha_{L-1}}s_{L-1}\)</span></li>
<li>Calculate normalization constant: <span class="math inline">\({\left \vert \left \vert P_L \right \vert \right \vert}_A\)</span></li>
<li>Calculate final normalization direction: <span class="math inline">\(s_L = \frac{P_L}{{\left \vert \left \vert P_L \right \vert \right \vert}_A}\)</span>.</li>
</ol></li>
<li>Update iterates:
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\alpha_L = &lt;s_L, r_L&gt; = \frac{{\left \vert \left \vert r_L \right \vert \right \vert}_2^2}{{\left \vert \left \vert P_L \right \vert \right \vert}_A}\)</span>.</li>
<li><span class="math inline">\(x_{L+1} = x_L - \alpha_L s_L\)</span>.</li>
<li><span class="math inline">\(r_{L+1} = r_L - \alpha_L A s_L\)</span>.</li>
</ol></li>
</ol>
<p>When working with exact arithmetic, CG is guaranteed to converge in a finite number of steps. However, when implementing in algorithms, there’s no such guarentee. It will converge in the limit, and the convergence rate is of the order <span class="math inline">\(\frac{\sqrt{k} - 1}{\sqrt{k} + 1}\)</span>, where <span class="math inline">\(k\)</span> is the condition number. If <span class="math inline">\(k\)</span> is very large, the convergence happens very slowly. This is why there are a lot of work being done in socalled “precondition methods”, that is methods that aim at transforming the matrix <span class="math inline">\(A\)</span> such that the condition number is smaller, but features are not lost.</p>
<p>So far, we have talked abouth iterative methods without much motivation or justification. However, as we will see later, iterative methods are very useful when dealing with non-linear systems of equations.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="singular-value-decomposition-svd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nonlinear-systems-of-equations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["STAT771_notes.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
