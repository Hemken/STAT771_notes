\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={STAT 771: My notes},
            pdfauthor={Ralph Møller Trane},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{STAT 771: My notes}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Ralph Møller Trane}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Fall 2018 (compiled 2018-09-20)}

\usepackage{booktabs}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Remark}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\norm}[1]{\left \vert \left \vert #1 \right \vert \right \vert}

\newcommand{\argmin}{\text{argmin}}
\newcommand{\argmax}{\text{argmax}}

\chapter*{Intro}\label{intro}
\addcontentsline{toc}{chapter}{Intro}

\chapter{Lecture Notes}\label{lecture-notes}

\section*{Lecture 1: 9/6}\label{lecture-1-96}
\addcontentsline{toc}{section}{Lecture 1: 9/6}

Goals for the first few lectures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Develop basic understanding of floating point numbers (\emph{fp}
  numbers)
\item
  Develop some basic notions of errors and their consequences
\end{enumerate}

References:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  David Goldberg (1991)
\item
  John D. Cook (2009)
\item
  Hingham (2002)
\end{enumerate}

\section{Positional numeral system}\label{positional-numeral-system}

We assume we have a decimal representation of numbers. I.e. that it
exists. It is not within the scope of this class to prove this.

Now, this is NOT the optimal way for a computer to represent numbers.
For various reasons, there are more desirable ways to store numbers. So
we need a different way of representing the numbers.

Ingredients for different representation:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  A base, refered to as \(\beta\). It holds that
  \(\beta \in {2,3,4,...}\)
\item
  A significand: a sequence of digits: \(d_0.d_1d_2d_3d_4...\), where
  \(d_j \in \{0, 1,...,\beta-1\}\)
\item
  An exponent: \(e \in \mathbb{Z}\).
\end{enumerate}

The representation \(d_0.d_1d_2... \times \beta^e\) means
\(\left(d_0 + d_1\cdot \beta^{-1} + ... + d_{p-1}\beta^{-(p-1)}\right)\cdot \beta^e\).

\section{Floating Point Format}\label{floating-point-format}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-1}{}{\label{def:unnamed-chunk-1} }A
fp is one that can be represented in a base \(\beta\) with a fixed digit
\(p\) (precision), and whose exponent is between \(e_{min}\) and
\(e_{max}\).
\EndKnitrBlock{definition}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-2}{}{\label{exm:unnamed-chunk-2} }Let
\(\beta = 10, p = 3, e_{min} = -1, e_{max} = 1\). Want to represent
\(0.1\). Several options:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\tightlist
\item
  Let d\_0=0, d\_1 = 0, d\_2 = 1, e = 1.
\item
  Let d\_0=0, d\_1 = 1, d\_2 = 0, e = 0.
\item
  d\_0 = 1, d\_1 = 0, d\_2 = 0, e = -1.
\end{enumerate}

If we fill into the equation above, we get 0.1:

\begin{align*}
  i:  & \left(0 + 0\cdot 10^{-1} + 1\cdot 10^{-2}\right)\cdot 10^{1} \\
  ii: & \left(0 + 1\cdot 10^{-1} + 1\cdot 10^{-2}\right)\cdot 10^{0} \\
  iii:& \left(1 + 0\cdot 10^{-1} + 1\cdot 10^{-2}\right)\cdot 10^{-1}
\end{align*}
\EndKnitrBlock{example}

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-3}{}{\label{def:unnamed-chunk-3} }A
fp number is said to be \emph{normalized} if \(d_0 \neq 0\).
\EndKnitrBlock{definition}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:unnamed-chunk-4}{}{\label{exr:unnamed-chunk-4}
}What is the total number of values that can be represented in the
normalized fp format with base \(\beta, p, e_{min}, e_{max}\)?

We count the different values each of the elements of a \emph{fp} can
take:

\begin{itemize}
\tightlist
\item
  \(d_0\) can be from 1 to \(\beta-1\), so \(\beta-1\) different values.
\item
  \(d_1,...,d_{p-1}\) each takes a value in \(\{0,1,...,\beta-1\}\).
  Hence, we can choose the digits \(d_1,...,d_{p-1}\) in \(\beta^{p-1}\)
  different ways.
\item
  \(e\) can take \(e_{max} - e_{min} + 1\) different values (all
  integers from \(e_{min}\) to \(e_{max}\), both included, hence the
  \(+1\)).
\end{itemize}

So, in total, there are
\((\beta-1)\cdot \beta^{p-1}\cdot (e_{max} - e_{min} + 1)\) different
values that can be represented in the normalized fp format with base
\(\beta\), precision \(p\), and \(e_{min}, e_{max}\) given.
\EndKnitrBlock{exercise}

\section{IEEE Standards}\label{ieee-standards}

IEEE have standards for how to deal with approximations and errors.

For our purposes, a bit is a single unit of storage on a computer, which
can either be 0 or 1. Hence, we'll be focusing on fp formats where
\(\beta = 2\).

\subsection{The 16 bit standard (half precision
standard).}\label{the-16-bit-standard-half-precision-standard.}

The 16 bits of storage are used in the following way, when following the
16 bit standard:

\begin{itemize}
\tightlist
\item
  1 bit for the sign

  \begin{itemize}
  \tightlist
  \item
    0 = positive
  \item
    1 = negative
  \end{itemize}
\item
  5 bits for the exponent

  \begin{itemize}
  \tightlist
  \item
    00000 is reserved for 0
  \item
    11111 is reserved for \(\infty\)
  \item
    30 exponents left: \(2^5 - 2 = 30\)
  \item
    the 16 bit standard dictates that the used exponents are
    \(-14,...,15\).

    \begin{itemize}
    \tightlist
    \item
      \textbf{Note}: \(0\) is also included in this list of 30
      exponents. This is because the \(00000\) representation is
      reserved for integers, while \(01111\) is used with non-integers.
    \end{itemize}
  \end{itemize}
\item
  11 bit for the significand.

  \begin{itemize}
  \tightlist
  \item
    10 are actually stored -- we always work with normalized FP numbers,
    i.e. \(\beta_0 = 1\).
  \end{itemize}
\end{itemize}

\textbf{Question:} What are smallest and largest positive numbers that
can be represented?

\textbf{Answer:} Smallest non-normalized number would be the one with
the smallest possible exponent, and all digits of the significand are 0
except the very last one. So, the smallest non-normalized FP number in
the 16 bit standard would be

\[
\left(0 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-9} + 1\cdot 2^{-10}\right)\cdot 2^{-14} = 2^{-24} \approx 5.96\cdot 10^{-8}
\]

The smallest normalized number is the one with all digits \(0\) (except
for the leading digit, of course, which has to be \(1\) for it to be
normalized), and \(e = -14\). So the smallest normalized FP number:

\[
\left(1 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-10}\right)\cdot 2^{-14} = 2^{-14} \approx 6.10\cdot 10^{-5}
\] Finally, the largest (finite) FP number in the 16 bit standard is the
one where the exponent is as large as possible (\(e = 15\)), and all
digits are \(1\). So

\[
\left(1 + 1\cdot 2^{-1} + ... + 1\cdot 2^{-10}\right)\cdot 2^{15} = 65504
\]

\section*{Lecture 2: 9/11}\label{lecture-2-911}
\addcontentsline{toc}{section}{Lecture 2: 9/11}

\subsection{The 32 bit standard (single
precision)}\label{the-32-bit-standard-single-precision}

The 32 bits of storage are used in the following way, when following the
32 bit standard:

\begin{itemize}
\tightlist
\item
  1 bit for the sign

  \begin{itemize}
  \tightlist
  \item
    0 = positive
  \item
    1 = negative
  \end{itemize}
\item
  8 bits for the exponent

  \begin{itemize}
  \tightlist
  \item
    00000000 is reserved for 0
  \item
    11111111 is reserved for \(\infty\)
  \item
    exponents left: \(2^8 - 2 = 254\)
  \item
    the 32 bit standard dictates that the used exponents are
    \(-126,...,127\).

    \begin{itemize}
    \tightlist
    \item
      \textbf{Note}: \(0\) is also included in this list of the 254
      exponents. This is because the \(00000000\) representation is
      reserved for integers, while \(01111111\) (I think this is the
      representation for \(0\) here\ldots{}) is used with non-integers.
    \end{itemize}
  \end{itemize}
\item
  24 bit for the significand.

  \begin{itemize}
  \tightlist
  \item
    23 are actually stored -- we always work with normalized FP numbers,
    i.e. \(\beta_0 = 1\).
  \end{itemize}
\end{itemize}

\textbf{Question:} What are smallest and largest positive numbers that
can be represented in the 32 bit standard?

\textbf{Answer:} Smallest non-normalized number would be the one with
the smallest possible exponent, and all digits of the significand are 0
except the very last one. So, the smallest non-normalized FP number in
the 32 bit standard would be

\[
\left(0 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-22} + 1\cdot 2^{-23}\right)\cdot 2^{-126} = 2^{-149} \approx 1.40\cdot 10^{-45}
\]

The smallest normalized number is the one with all digits \(0\) (except
for the leading digit, of course, which has to be \(1\) for it to be
normalized), and \(e = -126\). So the smallest normalized FP number:

\[
\left(1 + 0\cdot 2^{-1} + ... + 0\cdot 2^{-23}\right)\cdot 2^{-126} = 2^{-126} \approx 1.18\cdot 10^{-38}
\] Finally, the largest (finite) FP number in the 32 bit standard is the
one where the exponent is as large as possible (\(e = 127\)), and all
digits are \(1\). So

\[
\left(1 + 1\cdot 2^{-1} + ... + 1\cdot 2^{-126}\right)\cdot 2^{127} = 3.40\cdot 10^{38}
\]

\subsection{The 64 bit standard (double
precision)}\label{the-64-bit-standard-double-precision}

The 64 bits of storage are used in the following way, when following the
64 bit standard:

\begin{itemize}
\tightlist
\item
  1 bit for the sign

  \begin{itemize}
  \tightlist
  \item
    0 = positive
  \item
    1 = negative
  \end{itemize}
\item
  11 bits for the exponent

  \begin{itemize}
  \tightlist
  \item
    00000000 is reserved for 0
  \item
    11111111 is reserved for \(\infty\)
  \item
    exponents left: \(2^11 - 2 = 2046\)
  \item
    the 64 bit standard dictates that the used exponents are
    \(-1024,...,1023\).

    \begin{itemize}
    \tightlist
    \item
      \textbf{Note}: \(0\) is also included in this list of the 254
      exponents. This is because the \(00000000\) representation is
      reserved for integers, while \(01111111\) (I think this is the
      representation for \(0\) here\ldots{}) is used with non-integers.
    \end{itemize}
  \end{itemize}
\item
  53 bit for the significand.

  \begin{itemize}
  \tightlist
  \item
    52 are actually stored -- we always work with normalized FP numbers,
    i.e. \(\beta_0 = 1\).
  \end{itemize}
\end{itemize}

\section{Errors}\label{errors}

\subsection{Units in the Last Place
(ULP)}\label{units-in-the-last-place-ulp}

\subsection{Absolute and Relative
Error}\label{absolute-and-relative-error}

Let \(fl: \mathbb{R}_{\geq 0} \rightarrow \mathcal{S}\) be a function
that takes a real value and return a FP number. Then we define the
absolute and relative error as follows:

\BeginKnitrBlock{definition}
\protect\hypertarget{def:unnamed-chunk-5}{}{\label{def:unnamed-chunk-5} }Let
\(z \in \mathbb{R}_{\geq 0}\). The \emph{absolute error} is defined as

\[
\left | fl(z) - z \right | .
\]

The \emph{relative error} is defined as

\[
\left | \frac{fl(z)-z}{z} \right |
\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:lem1}{}{\label{lem:lem1} }If \(z\) has exponent
\(e\), then the maximum absolute error is \(\frac{\beta^{e-p+1}}{2}\).
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}
\EndKnitrBlock{proof}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-7}{}{\label{lem:unnamed-chunk-7} }If
\(z\) has exponent \(e\), then the maximum relative error is
\(\frac{\beta^{1-p}}{2}\).
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}If \(z\) has exponent \(e\), then
\(\beta^{e}\leq z\). Using this with \ref{lem:lem1}, we get that

\[
\left | \frac{fl(z)-z}{z} \right | \leq \frac{\beta^{e-p+1}}{2\beta^e} = \frac{\beta^{1-p}}{2}.
\]
\EndKnitrBlock{proof}

\textbf{Note:} the upper bound of the relative error is called the
\emph{machine epsilon}. This can be obtained in Julia using the function
\texttt{eps}.

\subsubsection{The Fundamental Axiom}\label{the-fundamental-axiom}

\ldots{} is that for any of the four arithmetic operations
(\(+, -, \cdot, /\)), we have the following error bound:

\[
fl(x \text{op} y) = (x \text{op} y)(1+\delta),
\]

with \(|\delta| \leq u\), where \(u\) is commonly \(2\cdot \epsilon\).
(\textbf{NOTE: NEED TO CLARIFY IF THE ABOVE IS CORRECT!})

**Example:* Matrix storage. Let \(A \in \mathbb{R}^{m\times n}\). Then:

\[
\left| fl(A) - A \right | \leq u \left | A \right |
\]

\textbf{Example:} Dot product. Let \(x,y \in \mathbb{R}^{n}\). Recall
that the dot product of \(x\) and \(y\) is definted as
\(x'y = \sum_{i=1}^{n} x_i \cdot y_i\). This can be calculated in the
following way:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fl = }\KeywordTok{function}\NormalTok{(x,y)}
  \CommentTok{# Get length of x}
\NormalTok{  n = length(x)}
  \CommentTok{# Check that length of y is equal to length of x. If not, throw error.}
  \KeywordTok{if}\NormalTok{(length(y) != n)}
    \KeywordTok{return} \StringTok{"ERROR: y does not have same dimension as x"}
  \KeywordTok{end}
  
  \CommentTok{# s will be the result of the dot product calculation}
\NormalTok{  s = }\FloatTok{0}
  
  \KeywordTok{for}\NormalTok{ i = }\FloatTok{1}\NormalTok{:n}
\NormalTok{    s += x[i]*y[i]}
  \KeywordTok{end}
  
  \KeywordTok{return}\NormalTok{(s)}
  
\KeywordTok{end}
\end{Highlighting}
\end{Shaded}

Next we want to prove the following lemma:

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-10}{}{\label{lem:unnamed-chunk-10}
}Let \(x,y \in \mathbb{R}^n\), and \(n\cdot u \leq 0.01\). Then

\[
\left | fl(x'y) - x'y \right | \leq 1.01 \cdot n\cdot u \cdot \left|x\right|'\left|y\right|
\]
\EndKnitrBlock{lemma}

\section*{Lecture 3: 9/13}\label{lecture-3-913}
\addcontentsline{toc}{section}{Lecture 3: 9/13}

To prove the lemma above, we will need another lemma\ldots{}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:lemForProof}{}{\label{lem:lemForProof} }If
\(|\delta_i| \leq u, \forall i=1,\ldots,n\) s.t. \(n\cdot u < 2\). Let
\(1 + \eta = \prod_{i=1}^{n}(1 + \delta_i)\). Then

\[ 
|\eta | \leq \frac{n\cdot u}{1-\tfrac{n\cdot u}{2}}
\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Using the definition of \(\nu\), we can
rewrite it to get

\[
| \eta | = \left | \prod_{i=1}^n (1 + \delta_i) - 1 \right |. 
\]

By induction, we will show that the expression above is less than or
equal to \((1 + u)^n - 1\). {[}TO BE COMPLETED!{]}

Since \(1+u \leq e^{u}\) for all \(u \in \R\), we have that

\begin{align*}
  | \eta | &\leq e^{n\cdot u} - 1 \\
          &\leq n\cdot u + \frac{(n\cdot u)^2}{2!} + \frac{(n\cdot u)^3}{3!} + \ldots \text{(used the Taylor expansion)}\\
          &\leq n\cdot u + \frac{(n\cdot u)^2}{2^1} + \frac{(n\cdot u)^3}{2^2} + \frac{(n\cdot u)^4}{2^3} + \ldots (\text{used that } x! > 2^{x-1} \text{ for } x > 1) \\
          &= \sum_{k=0}^{\infty} n\cdot u \left(\frac{n\cdot u}{2}\right)^k \text{\small (identify this as a geometric series with } r = \tfrac{n\cdot u}{2} \text{, \small which is less than 1 by assumption)} \\
          &= \frac{n\cdot u}{1-\tfrac{n\cdot u}{2}},
\end{align*}

which is exactly what we wanted.
\EndKnitrBlock{proof}

With this in hand, we will prove the previously stated lemma.

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Let \(s_p\) denote the value of \(s\) after
the \(p\)'th iteration of the algorithm described above. Then, since
we're assuming the Fundamental Axiom, we have that
\(s_1 = fl(x_1y_y) = x_1 y_1 (1 + \delta_1)\), where
\(|\delta_1| \leq u\). We can similarly find \(s_p\) as

\begin{align*}
s_p &= fl(s_{p-1} + fl(x_p y_p)) \\
    &= (s_{p-1} + fl(x_p y_p))(1 + \epsilon_p) (\text{where } |\epsilon_p| \leq u) \\
    &= (s_{p-1} + x_py_p(1 + \delta_p))(1 + \epsilon_p) (\text{where } |\delta_p| \leq u).
\end{align*}

Let \(\epsilon_1 = 0\). \(s_p\) is a recursive formula, and can be
rewritten as follows:

\[
s_p = \sum_{i=1}^p x_iy_i (1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j).
\]

So,

\begin{align*}
  | s_n - x^\prime y |  &= \left | \sum_{i=1}^n (x_i y_i)(1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - \sum_{i=1}^{n}x_iy_i\right | \\
                        &= \left | \sum_{i=1}^n (x_i y_i)\left((1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - 1 \right) \right | \\
                        &\leq \sum_{i=1}^n \left |x_i y_i \right |\left | (1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - 1 \right| .
\end{align*}

We now use \ref{lem:lemForProof} to get:

\begin{align*}
\sum_{i=1}^n \left |x_i y_i \right |\left | (1 + \delta_i)\prod_{j=1}^p (1 + \epsilon_j) - 1 \right| &\leq  \frac{nu}{1-\tfrac{nu}{2}} \sum_{i=1}^n \left |x_i y_i \right | \\
&\leq \frac{nu}{0.995} \sum_{i=1}^n |x_i| |y_i | \\
&\leq 1.01 \cdot nu \cdot |x|^\prime |y|
\end{align*}
\EndKnitrBlock{proof}

\section{Square Linear Systems}\label{square-linear-systems}

In the following, let \(A \in \R^{n \times m}\) be an invertible matrix,
and assume \(Ax = b\) for a \(b \neq 0\). This implies that
\(x = A^{-1}b\).

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-13}{}{\label{thm:unnamed-chunk-13}
}Let \(\kappa_\infty = \norm{A}_\infty \norm{A^{-1}}_\infty\). Assume we
can store \(A\) with precision \(E\) (i.e.~as \(A+E\)), where
\(\norm{E}_\infty \leq u \norm{A}_\infty\), and \(b\) with precision
\(e\) (i.e.~as \(b+e\)), where
\(\norm{e}_\infty \leq u \norm{b}_\infty\).

If \(\norm{A+E}\hat{x} = b+e\) and \(u\cdot \kappa_\infty < 1\), then

\[
  \frac{\norm{x-\hat{x}}_\infty}{\norm{x}} \leq \frac{2\cdot u \cdot \kappa_\infty}{1-u\cdot \kappa_\infty}
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:proofHW}{}{\label{lem:proofHW} }Let
\(I\in \R^{n \times n}\) be the identity matrix, and
\(F \in \R^{n\times n}\) s.t. \(\norm{F}_p < 1\) for some
\(p \in [1,\infty]\). Then \(I-F\) is invertible, and

\[
  \norm{(I-F)^{-1}}_p \leq \frac{1}{1-\norm{F}_p}
\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}\textbf{HOMEWORK}
\EndKnitrBlock{proof}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-15}{}{\label{lem:unnamed-chunk-15}
}Suppose \(\exists \epsilon > 0\) s.t.
\(\norm{\Delta A} \leq \epsilon \norm{A}\) and
\(\norm{\Delta b} \leq \epsilon \norm{b}\), and \(y\) s.t.
\((A+\Delta A)y = b+\Delta b\).

If \(\epsilon \norm{A}\norm{A^{-1}} = r < 1\), then \(A+\Delta A\) is
invertible and \[\frac{\norm{y}}{\norm{x}} \leq \frac{1+r}{1-r}.\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Note that
\(A+\Delta A = A\left (I + A^{-1}\Delta A\right ) = A\left (I - \left(- A^{-1}\Delta A\right)\right)\).
Since
\(\norm{-A^{-1}\Delta A} = \norm{A^{-1}\Delta A} \leq \epsilon \norm{A^{-1}}\cdot \norm{A} < 1\)
(by assumptions), Lemma \ref{lem:proofHW} gives us that
\(I + A^{-1}\Delta A\) is invertible. Since \(A\) is also invertible
(again, by assumption), \(A+\Delta A\) is invertible (product of two
invertible matrices is invertible).

Performing some linear algebra:

\begin{align*}
  (A + \Delta A) &=  b + \Delta b \Leftrightarrow \\
  A(I + A^{-1} \Delta A) y &= b + \Delta b \Leftrightarrow \\
  (I + A^{-1} \Delta A) y &= A^{-1}b + A^{-1}\Delta b \Leftrightarrow \\
  y &= (I + A^{-1} \Delta A)^{-1} A^{-1}b + A^{-1}\Delta b.
\end{align*}

Remember that \(A^{-1}b = x\). From the definition of \(r\) we have that
\(\norm{A^{-1}} = \frac{r}{\norm{A}}\). These two identities with the
assumption that \(\norm{\Delta b} \leq \epsilon b\) gives us

\[
\begin{aligned}
  \norm{y} &\leq \norm{(I + A^{-1}\Delta A)^{-1}} \left( \norm{x} + \norm{A^{-1}\Delta b}\right) \\
  &\leq \frac{1}{1-\norm{A^{-1}\Delta A}} \left( \norm{x} + \frac{r}{\epsilon \norm{A}} \cdot \norm{\Delta b} \right) \\
  &\leq \frac{1}{1-r} \left( \norm{x} + \frac{r}{\epsilon \norm{A}} \cdot \epsilon \norm{b} \right) \\
  &= \frac{1}{1-r} \left( \norm{x} + \frac{r \cdot \norm{b}}{\norm{A}} \right).
\end{aligned}
\]

Finally, recall that \(Ax=b\), hence
\(\norm{A}\cdot\norm{x} \geq \norm{b}\), so
\(\norm{x} \geq \frac{\norm{b}}{\norm{A}}\). So,

\[
\begin{aligned}
  \norm{y} \leq \frac{1}{1-r}  \left( \norm{x} + r \cdot \norm{x} \right) \Leftrightarrow \\
  \frac{\norm{y}}{\norm{x}} \leq \frac{1+r}{1-r}.
\end{aligned}
\]
\EndKnitrBlock{proof}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-17}{}{\label{lem:unnamed-chunk-17}
}\[\frac{\norm{y - x}}{\norm{x}} \leq \frac{2\epsilon \norm{A^{-1}}\cdot \norm{A}}{1-r}.\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{} \[
\begin{aligned}
  \left(A+\Delta A\right)y &= b + \Delta b \Leftrightarrow \\
  Ay - b &= \Delta b - \Delta Ay \Leftrightarrow \\
  y - A^{-1}b &= A^{-1} \Delta b - A^{-1}\Delta A y \Leftrightarrow \\
  y - x &= A^{-1} \Delta b - A^{-1}\Delta A y \Leftrightarrow \\
  \norm{y - x} &\leq \norm{A^{-1}} \norm{\Delta b} + \norm{A^{-1}} \norm{\Delta A}\norm{y} \\ %\Leftrightarrow \\
  &\leq \norm{A^{-1}} \epsilon \norm{b} + \norm{A^{-1}} \epsilon\norm{A}\norm{y} \\ %\Leftrightarrow \\
  &\leq \epsilon \norm{A^{-1}} \norm{A}\norm{x} + \epsilon\norm{A^{-1}}\norm{A}\norm{y} \\ % \Leftrightarrow \\
  &\leq \epsilon \norm{A^{-1}} \norm{A}\left(\norm{x} + \norm{y}\right) \\
  &= \epsilon \norm{A^{-1}} \norm{A}\left(\norm{x} + \frac{1+r}{1-r}\norm{x}\right) \Leftrightarrow \\
\frac{\norm{y-x}}{\norm{x}} &\leq \epsilon \norm{A^{-1}} \norm{A} \left(\frac{1-r}{1-r} + \frac{1+r}{1-r}\right) \\
  &= 2\epsilon \norm{A^{-1}} \norm{A} \frac{1}{1-r}
\end{aligned}
\]
\EndKnitrBlock{proof}

\section{Orthogonalization}\label{orthogonalization}

Goals

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Introduce and prove the existence of QR decomposition
\item
  Overview of the algorithm to perfor QR decomposition
\item
  Solve least squares problems
\item
  ``Large'' data problems
\end{enumerate}

Outline

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Motivating problems and solutions with QR
\item
  Gram-Schmidt procedure, existence of QR
\item
  Householder, Givens
\item
  ``Large'' least squares problems datadown
\end{enumerate}

\subsection{Motivating problems}\label{motivating-problems}

\BeginKnitrBlock{example}[Motivating Problem 1 (Consistent Linear System)]
\protect\hypertarget{exm:linear-system}{}{\label{exm:linear-system}
\iffalse (Motivating Problem 1 (Consistent Linear System)) \fi{} }Assume
\(A \in \R^{n\times m}, n \geq m, \text{rank}(A) = m\), and
\(b \in \text{range}(A) \subset R^m\). Find \(x \in \R^m\) s.t.
\(Ax = b\).
\EndKnitrBlock{example}

\BeginKnitrBlock{example}[Motivating Problem 2 (Least Squares Regression)]
\protect\hypertarget{exm:least-squares}{}{\label{exm:least-squares}
\iffalse (Motivating Problem 2 (Least Squares Regression)) \fi{} }Assume
\(A \in \R^{n\times m}, n \geq m, \text{rank}(A) = m\), and
\(b \in R^n\). Find \(x \in \R^m\) s.t.
\[x \in \argmin_{y \in \R^m} \norm{Ay-b}_2.\]
\EndKnitrBlock{example}

\BeginKnitrBlock{example}[Motivating Problem 3 (Underdetermined Linear System]
\protect\hypertarget{exm:und-linear-system}{}{\label{exm:und-linear-system}
\iffalse (Motivating Problem 3 (Underdetermined Linear System) \fi{}
}Assume \(A \in \R^{n\times m}, n \geq m, \text{rank}(A) < m\), and
\(b \in \text{range}(A)\). Find \(x \in \R^m\) s.t.

\[x \in \argmin_{y \in \R^m} \left\{ \norm{y}_2 \left | Ay = b \right\} \right . .\]
\EndKnitrBlock{example}

\BeginKnitrBlock{example}[Motivating Problem 4 (Underdetermined Least Squares Regression)]
\protect\hypertarget{exm:und-least-squares}{}{\label{exm:und-least-squares}
\iffalse (Motivating Problem 4 (Underdetermined Least Squares
Regression)) \fi{} }Assume
\(A \in \R^{n\times m}, n \geq m, \text{rank}(A) < m\), and
\(b \in \R^n\). Find \(x \in \R^m\) s.t.

\[x \in \argmin_{z \in \R^m} \left\{ \norm{z}_2 \left | \norm{Ay - b}_2 = \min_{y \in \R^m} \norm{Ay-b}_2 \right\} \right . .\]
\EndKnitrBlock{example}

\BeginKnitrBlock{example}[Motivating Problem 5 (Constrained Least Squares Regression)]
\protect\hypertarget{exm:constrained-least-squares}{}{\label{exm:constrained-least-squares}
\iffalse (Motivating Problem 5 (Constrained Least Squares Regression))
\fi{} }Assume \(A \in \R^{n\times m}, n \geq m, \text{rank}(A) < m\),
and \(b \in \R^n\). Let \(C \in \R^{p\times m}, \text{C} = p\), and
\(d \in \R^{p}\). Find \(x \in \R^m\) s.t.

\[x = \argmin_{y \in \R^m} \norm{Ay - b}_2\quad \text{s.t.} \quad Cy = d.\]
\EndKnitrBlock{example}

Before we take a crack at solving these problems, we will need to get
some definitions down.

\BeginKnitrBlock{definition}[Permutation Matrix]
\protect\hypertarget{def:perm-matrix}{}{\label{def:perm-matrix}
\iffalse (Permutation Matrix) \fi{} }A permutation matrix is a square
matrix such that each column has exactly one element that is \(1\), the
rest are \(0\).
\EndKnitrBlock{definition}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:unnamed-chunk-19}{}{\label{exm:unnamed-chunk-19}
}The following is a permutation matrix:

\[
  \begin{bmatrix}
    0 & 1 \\
    1 & 0
  \end{bmatrix}
\]
\EndKnitrBlock{example}

\BeginKnitrBlock{definition}[Orthogonal Matrix]
\protect\hypertarget{def:unnamed-chunk-20}{}{\label{def:unnamed-chunk-20}
\iffalse (Orthogonal Matrix) \fi{} }A matrix \(Q\) is said to be an
\emph{orthogonal matrix} if \(Q^T Q = Q Q^T = I\).
\EndKnitrBlock{definition}

Note: for an orthogonal matrix \(Q \in \R^{n\times m}\), it holds that
\(\norm{Q_{i*}}_2 = 1\) for all \(i=1,\ldots,n\), and
\(\norm{Q_{*j}}_2 = 1\) for all \(j = 1,\ldots, m\).\footnote{Here we
  use the notion \(Q_{i*}\) to mean the \(i\)'th row, and \(Q_{*j}\) to
  mean the \(j\)'th column of \(Q\).}

\BeginKnitrBlock{definition}[Upper Triangular Matrix]
\protect\hypertarget{def:unnamed-chunk-21}{}{\label{def:unnamed-chunk-21}
\iffalse (Upper Triangular Matrix) \fi{} }A matrix \(R\) is an
\emph{upper triangular matrix} if \(R_{ij} = 0\) for all \(i>j\).
\EndKnitrBlock{definition}

\section*{Lecture 4: 9/18}\label{lecture-4-918}
\addcontentsline{toc}{section}{Lecture 4: 9/18}

\subsection{QR Decomposition}\label{qr-decomposition}

In order to actually solve the problems listed above, we need the QR
Decomposition:

\BeginKnitrBlock{theorem}[Existence of QR Decomposition]
\protect\hypertarget{thm:qr-decomposition}{}{\label{thm:qr-decomposition}
\iffalse (Existence of QR Decomposition) \fi{} }Let
\(A \in \R^{n \times m}\) and let \(r = rank(A)\). Then there exists:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  an \(m\times m\) permutation matrix \(\Pi\),
\item
  an \(n \times n\) orthogonal matrix \(Q\),
\item
  an \(r \times r\) upper triangular matrix \(R\), with non-zero
  diagonal elements (i.e.~invertible)
\item
  an \(r \times (m-r)\) matrix S (if \(m > r\)),
\end{enumerate}

such that

\[
  A = Q \begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix} \Pi^T.
\]\\
\EndKnitrBlock{theorem}

With this in hand, we can solve the motivating problems stated above.

\BeginKnitrBlock{solution}[Example \@ref(exm:linear-system)]
\iffalse{} {Solution (Example \ref{exm:linear-system}). } \fi{} We want
to find \(x\) such that \(Ax = b\).

We use theorem \ref{thm:qr-decomposition} to rewrite this as
\(Q \begin{bmatrix} R \\ 0 \end{bmatrix} \Pi^T x = b\). Note that since
\(\text{rank}(A) = m\), there is no \(S\) matrix.

Now, since \(Q\) is an orthogonal matrix, we know that \(Q^{-1} = Q^T\),
so

\begin{equation}
  \begin{bmatrix} R \\ 0 \end{bmatrix} \Pi^T x = Q^T b = c = \begin{bmatrix} c_1 \\ 0 \end{bmatrix}. \label{eq:sol-linear-system}
\end{equation}

So now the equation we are trying to solve becomes

\[
  R \Pi^T x = c_1.
\]

Since \(R\) is an upper triangular matrix with non-zero diagonal
elements, it is invertible. Since \(\Pi\) is a permutation matrix,
\(\Pi^{-1} = \Pi^T\). Using this we can find the solution:

\[
  x = \Pi R^{-1} c_1.
\]
\EndKnitrBlock{solution}

\BeginKnitrBlock{solution}[Example \ref@(exm:least-squares]
\iffalse{} {Solution (Example \ref@(exm:least-squares). } \fi{}We want
to find \(x\) such that \(x \in \argmin_{y \in \R^m}\norm{Ay-b}_2\).

Once again, \(\text{rank}(A) = m\), so using theorem
\ref{thm:qr-decomposition}, we can rewrite the expression we are trying
to minimize as

\[
  \min \norm{Q \begin{bmatrix} R \\ 0 \end{bmatrix}\Pi^T y - b}_2. \label{eq:least-squares-eq1}
\]

Since \(Q^T = Q^{-1}\) is orthogonal, \(\norm{Q^T x}_2 = \norm{x}_2\)
for all \(x\) (homework exercise \ref{exr:q402}). So, we get that
\eqref{eq:least-squares-eq1} is the same as

\[
  \min\norm{\begin{bmatrix} R \\ 0 \end{bmatrix} \Pi^T y - Q^T b}_2.
\]

Now let \(c = Q^T b\). Then, \(c\) is of the form
\(\begin{bmatrix} c_1 \\ c_2 \end{bmatrix}\), where \(c_2\) is the last
\(n-r\) rows (i.e.~corresponding to the \(0\) rows of
\(\begin{bmatrix} R \\ 0 \end{bmatrix}\)). Then

\[
  \min\norm{\begin{bmatrix} R \Pi^T y - c_1 \\ -c_2 \end{bmatrix}}_2 = \min \sqrt{\norm{R \Pi^T y - c_1}_2^2 + \norm{c_2}_2^2}.
\]

Now this is minimized by \(\argmin_y \norm{R \Pi^T y - c_1}_2^2\). As
before, \(R^{-1}\) exists since \(R\) is upper triangular with non-zero
diagonal elements, \(\Pi^T = \Pi^{-1}\) since \(\Pi\) is a permutation
matrix, so

\[
  \begin{aligned}
    x &= \argmin_y \norm{R \Pi^T y - c_1}_2^2 \Leftrightarrow \\
    R \Pi^T x &= c_1 \Leftrightarrow \\
    x &= \Pi R^{-1} c_1.
  \end{aligned}
\]
\EndKnitrBlock{solution}

\BeginKnitrBlock{solution}[Example \@ref(exm:und-linear-system)]
\iffalse{} {Solution (Example \ref{exm:und-linear-system}). } \fi{} In
this scenario, \(\text{rank}(A) = r < m\). We are looking for
\(x \in \argmin_{y}\left\{\norm{y}_2 \left | Ay = b\right\}\right .\).
Using theorem \ref{thm:qr-decomposition}, we can rewrite this as
\(\argmin_y\left\{\norm{y}_2 | Q\begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix} y = b \right\}\),
and multiplying by \(Q^T\),
\(\argmin_y\left\{\norm{y}_2 \left | \begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix} y = Q^T b \right\} \right .\).
We introduce the vector \(c\) such that
\(Q^T b = \begin{bmatrix} c & 0 \end{bmatrix}^T\) (\(0\) entries
correspond to \(0\) rows in
\(\begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix}\)). If we furthermore
write \(\Pi^T y\) as \(\begin{bmatrix} z_1 & z_2 \end{bmatrix}^T\).

Then, since \(\norm{y}_2 = \norm{z}_2\), our problem becomes

\[
  \begin{aligned}
    x &\in \argmin_z \left\{\norm{z}_2 \left | R z_1 + S z_2 = c \right\}\right. \\
    x &\in \argmin_z \left\{\norm{z}_2 \left | z_1 = R^{-1} c - R^{-1} S z_2 \right\}\right. \\
    x &\in \argmin_z \sqrt{\norm{R^{-1}c - R^{-1} S z_2}_2^2 + \norm{z_2}_2^2} \\
    x &\in \argmin_z \left\{\norm{R^{-1}c - R^{-1} S z_2}_2^2 + \norm{z_2}_2^2\right\},
  \end{aligned}
\]

where the last equality is a consequence of the result proved in
homework @ref\{exr:q403\}. Now, let \(d = R^{-1}c\) and
\(p = R^{-1}Sz_2\). Then we can find the minimum of the above expression
by differentiating and setting equal to zero:

\begin{align}
  0 &= -P^Td + (P^TP + I)z_2 \rightarrow
  z_2 &= (P^T P + I)^{-1}P^Td.
\end{align}
\EndKnitrBlock{solution}

\BeginKnitrBlock{solution}[Example \@ref(exm:und-least-squares)]
\iffalse{} {Solution (Example \ref{exm:und-least-squares}). } \fi{} We
want to find
\(\min_z \left\{ \norm{z}_2 \left \vert z \in \argmin_y \norm{Ay - b}_2\right\} \right .\)
Use theorem \ref{thm:qr-decomposition}:

\[\begin{aligned}
  \min_z \left\{ \norm{z}_2 \left \vert z \in \argmin_y \norm{Ay - b}_2\right\} \right . &= \left\{ \norm{z}_2 \left \vert z \in \argmin_y \norm{\begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix} \Pi^T y - Q^T b}_2\right\} \right . \\
  &= \left\{ \norm{w}_2 \left \vert w \in \argmin_y \norm{\begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix}  y - Q^T b}_2\right\} \right .,
\end{aligned}\]

since \(\norm{y}_2 = \norm{\Pi^T y}_2\). This is exactly the problem
solved in example \ref{exm:und-linear-system}. In conclusion,

\[
  w = \begin{bmatrix} R^{-1}(c_1 - Sy_y) \\ y_2 \end{bmatrix}.
\]
\EndKnitrBlock{solution}

\BeginKnitrBlock{solution}[Example \@ref(exm:constrained-least-squares]
\iffalse{} {Solution (Example \ref{exm:constrained-least-squares}. }
\fi{}
\EndKnitrBlock{solution}

\subsection{Existence of
QR-decomposition.}\label{existence-of-qr-decomposition.}

To prove the existence of the QR-decomposition, we need the Gram-Schmidt
process.

\BeginKnitrBlock{lemma}[The Gram-Schmidt Process]
\protect\hypertarget{lem:gsp}{}{\label{lem:gsp} \iffalse (The Gram-Schmidt
Process) \fi{} }Let \(r \in \N\). Given a set of linearly independent
vectors \(\{a_1, \ldots, a_r\}\), there exists a set of orthonormal
vectors \(\{q_1, \ldots, q_r\}\) such that
\(\text{span} \{q_1, \ldots, q_r\} = \text{span}\{a_1, \ldots, a_r\}\).

The \(q_i\)'s are given by\ldots{}
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}We will prove this by induction. For \(i=1\):
let \(R_{11} = \norm{a_1}_2\), \(q_1 = \frac{1}{R_11}a_1\). Notice that
\(\norm{q_1} = 1\).

(At this point, it might be beneficial to check out the intuitive side
note (\ref{cnj:gram-schmidt-remark}))

Define \(q^r\) in the following way: let \(R_{ir} = q_i^\prime a_r\),
\(\tilde{q}_r = a_r - \sum_{i=1}^{r-1} R_{ir}q_i\), and
\(R_rr = \norm{\tilde{q}_r}_2\). Then
\(q_r = \frac{\tilde{q}_r}{R_{rr}}\). (Note: \(\tilde{q}_r \neq 0\)
since the \(a_i\)s are linearly independent, and \(q_i\) is given as a
linear combination of \(a_1, \ldots, a_i\).)

Assume the result holds for \(i \le r-1\). I.e. we have vectors
\(q_1, \ldots, q_{r-1}\) given as above, and that

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  \(\text{span}\{q_1, \ldots, q_{r-1}\} = \text{span}\{a_1, \ldots, a_{r-1}\}\),
\item
  \(q_i \cdot q_j\) for all \(i,j = 1, \ldots, r-1\) with \(i \neq j\),
\item
  \(q_i^\prime \cdot q_i = 1\) for all \(i = 1, \ldots, r-1\).
\end{enumerate}

Now, we want to show that we can construct a \(q_r\) such that

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  \(\text{span}\{q_1, \ldots, q_r\} = \text{span}\{a_1, \ldots, a_r\}\),
\item
  \(q_r \cdot q_j = 0\) for all \(j = 1, \ldots, r-1\),
\item
  \(q_r^\prime \cdot q_r = 1\).
\end{enumerate}

We start from below.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  By definition of \(q_r\):
  \(q_r^\prime q_r = \frac{\tilde{q}_r^\prime \tilde{q}_r}{R_{rr}^2} = \frac{\norm{\tilde{q}_r}^2}{R_{rr}^2} = 1\).
\item
  Let \(i < r\). Then
\end{enumerate}

\[\begin{aligned}
  q_i^{\prime} \tilde{q}_r &= q_i^\prime a_r - \sum_{j=1}^{r-1} R_{jr} q_i^\prime q_j \\
                           &= q_i^\prime a_r - R_{ir} q_i^\prime q_i \\
                           &= q_i^\prime a_r - R_{ir} = 0 \text{ (by definition of } R_{ir}\text{)}.
\end{aligned}\]

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  We need to show that \(a_r\) can be written as a linear combination of
  \(q_i\)s.
\end{enumerate}

\[\begin{aligned}
  \sum_{i=1}^r R_{ir} q_i &= \sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} q_r \\
                          &= \sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} \frac{1}{R_{rr}} \tilde{q}_r \\
                          &= \sum_{i=1}^{r-1} R_{ir} q_i + R_{rr} \frac{1}{R_{rr}} \left(a_r - \sum_{i=1}^{r-1} R_{ir}q_i \right) \\
                          &= \sum_{i=1}^{r-1} R_{ir} q_i + a_r - \sum_{i=1}^{r-1} R_{ir}q_i \\
                          &= a_r.
\end{aligned}\]
\EndKnitrBlock{proof}

\BeginKnitrBlock{conjecture}[Intuitive side note]
\protect\hypertarget{cnj:gram-schmidt-remark}{}{\label{cnj:gram-schmidt-remark}
\iffalse (Intuitive side note) \fi{} }It is fairly easy to find \(q_2\).
We want to find it such that \(a_2 = R_{12}q_1 + R_{22} q_2\), and
\(\norm{q_2}_2 = 1\) and \(q_1 \perp q_2\), i.e. \(q_1 \cdot q_2 = 0\).
So, if we multiply the equation by \(q_1\), we get that
\(q_1 a_2 = R_{12}\). Substituting this into the first equation,
\(q_2 = \frac{a_2 - R_{12} q_1}{R_{22}}\).

Note that this is a circular argument, and hence not a formal way of
doing this.
\EndKnitrBlock{conjecture}

\section*{Lecture 5: 9/20}\label{lecture-5-920}
\addcontentsline{toc}{section}{Lecture 5: 9/20}

(Finished up proof of The Gram-Schmidt Process (\ref{lem:gsp}))

\BeginKnitrBlock{conjecture}[Gram-Schmidt in Matrix Form]
\protect\hypertarget{cnj:unnamed-chunk-28}{}{\label{cnj:unnamed-chunk-28}
\iffalse (Gram-Schmidt in Matrix Form) \fi{} }If we write up
\(a_1, \ldots, a_r\) in a matrix, we see that

\[
  \begin{bmatrix} 
    a_1 & \dots & a_r 
  \end{bmatrix} = 
      \begin{bmatrix}
        q_1 & \dots & q_r
      \end{bmatrix} 
        \begin{bmatrix}
          R_{11} & R_{12} & \dots & R_{1r} \\
          0      & R_{22} & \dots & R_{2r} \\
          \vdots & \ddots & \ddots &  \vdots \\
          0 & \ldots & 0 & R_{rr}
        \end{bmatrix}
\]

This is quite similar to the result we are after (the QR-decomposition
\ref{thm:qr-decomposition}).
\EndKnitrBlock{conjecture}

\BeginKnitrBlock{proof}[Proof of theorem \@ref(thm:qr-decomposition)]
\iffalse{} {Proof (Proof of theorem \ref{thm:qr-decomposition}). }
\fi{}Since \(\text{rank}(A) = r\), \(A\) has \(r\) linearly independent
columns. Hence, there exists a permutation matrix \(\Pi\) such that

\[
  A \Pi = \begin{bmatrix} a_1 & \dots & a_r & a_{r+1} \dots a_m \end{bmatrix},
\]

where \(a_1, \ldots, a_r\) are linearly independent, and
\(a_{r+1}, \ldots, a_m\) are linearly dependent on the first \(r\)
columns.

Using Gram-Schmidt (lemma \ref{lem:gsp}), we know that there exists
\(\tilde{Q} \in \R^{n \times r}, R \in \R^{r \times r}\) such that
\(A\Pi = \tilde{Q} R\). Since
\(\text{span}\{\tilde{q}_1, \ldots, \tilde{q}_r\}\) (columns of
\(\tilde{Q}\)) is equal to \(\text{span}\{a_1, \ldots, a_r\}\), there
exists an \(s_{k(j-r+2)}\) for any \(j \in \{r+1, \ldots, m\}\) and
\(k \in \{1, \ldots, r\}\) such that
\(a_j = \sum_{k=1}^r s_{k(j-r+2)}q_k\). So,

\[
  A\Pi = \tilde{Q} \begin{bmatrix} R & S \end{bmatrix}.
\]

This is almost the form we want, BUT \(\tilde{Q}\) is not orthonormal
(it is not square). However, we know that we can pick \(n-r\) vectors
from \(\R^n\) such that adding these as columns to \(\tilde{Q}\) we get
a set of \(n\) linearly independent columns. Now, use Gram-Schmidt to
normalize. Since the first \(r\) columns are already normalized, these
will stay the same. The result is a matrix \(Q\), where the columns are
all length \(1\), and they are all linearly independent. I.e.
\(Q^TQ = I\). So,
\(A\Pi = Q \begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix}\), hence

\[
  A = Q \begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix} \Pi^T.
\]
\EndKnitrBlock{proof}

Basically, this gives us a way to perform QR decomposition. However,
using the Gram-Schmidt procedure is NOT numerical stable. I.e. we might
end up with matrices \(Q, R\), and \(S\) from which we CANNOT recover
\(A\). To overcome this, there is a different method called the
\emph{Modified Gram-Schmidt Procedure}.

\BeginKnitrBlock{lemma}[The Modified Gram-Schmidt Procedure]
\protect\hypertarget{lem:mod-gsp}{}{\label{lem:mod-gsp} \iffalse (The
Modified Gram-Schmidt Procedure) \fi{} }\textbf{HOMEWORK}
\EndKnitrBlock{lemma}

\subsection{Householder}\label{householder}

\BeginKnitrBlock{definition}[Householder Reflections]
\protect\hypertarget{def:unnamed-chunk-30}{}{\label{def:unnamed-chunk-30}
\iffalse (Householder Reflections) \fi{} }A matrix
\(H = I - 2vv^\prime\), where \(\norm{v}_2 = 1\), is called a
\emph{Householder Reflection}.
\EndKnitrBlock{definition}

A Householder reflection takes any vector and reflects it over
\(\{tv: t \in \R\}\).

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-31}{}{\label{lem:unnamed-chunk-31}
}Householder reflections are orthogonal matrices.
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}\emph{HOMEWORK}
\EndKnitrBlock{proof}

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:unnamed-chunk-33}{}{\label{lem:unnamed-chunk-33}
}There exists Householder refelctions \(H_1, \ldots, H_r\) such that
\(H_r \dots H_1 A\Pi = R\).
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Let
\(A\Pi = \begin{bmatrix} a_1 \dots a_r \end{bmatrix}\). Choose \(H_1\)
s.t. \(H_1 a_1 = R_{11} e_1 = a_1 - 2v_1v_1^\prime a_1\) (last equality
due to definition of Householder reflections). This is equivalent to
\(v_1(2v_1^\prime a_1) = a_1 - R_{11}e_1\).

Now, let \(v_1 = \frac{a_1 - R_{11}e_1}{\norm{a_1 - R_{11}e_2}_2}\).
Plug this into the equation for \(R_{11}e_1\) above to get

\[
  R_{11}e_1 = a_1 - \frac{(a_1 - R_{11}e_1)}{\norm{a_1 - R_{11}e_1}_2}\frac{a_1^\prime a_1 - R_{11} a_1^\prime e_1}{\norm{a_1 - R_{11}e_1}_2}.
\]

If we multiply this by \(e_1^\prime\) from the right, we get

\[
  R_{11} = \pm \norm{a_1}_2, v_1 = \frac{a_1 - \norm{a_1}e_1}{\norm{a_1 - \norm{a_1}_2 e_1}_2}.
\]

\[ 
  H_1 = I - \frac{a_1 - \norm{a_1}_2e_1)(a_1 - \norm{a_1}_2 e_1)}{\norm{a_1 - \norm{a_1}_2 e_1}_2^2}
\]
\EndKnitrBlock{proof}

\subsection{Givens Rotations}\label{givens-rotations}

\BeginKnitrBlock{definition}[Givens Rotations]
\protect\hypertarget{def:givens}{}{\label{def:givens} \iffalse (Givens
Rotations) \fi{} }A \emph{Givens Rotation} is a matrix \(G^{(i,j)}\)
with entries \((g_{ij})\) such that

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  \(g_{ii} = g_{jj} = \lambda\) (the \(i\)th and \(j\)th elements of the
  diagonal are \(\lambda\)).
\item
  \(g_{kk} = 1\) for all \(k \notin \{i,j\}\). (all other diagonal
  elements are \(1\))
\item
  \(g_{ij} = g_{ji} = \sigma\)
\item
  \(g_{ij} = 0\) for all other pairs of \(i,j\).
\end{enumerate}

In words: \(G^{(i,j)}\) is the identity matrix with the \(i\)th and
\(j\)th diagonal elements made \(\lambda\), and the entries at \((i,j)\)
and \((j,i)\) are \(\sigma\).
\EndKnitrBlock{definition}

\chapter{Homework Assignments}\label{homework-assignments}

\section{Homework 1}\label{homework-1}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q101}{}{\label{exr:q101} }Can all nonnegative real
numbers be represented in such a manner (i.e.~as a fp number) for an
arbitrary base \(\beta \in \{2,3,...\}\)?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}No. For any given \(\beta\) and a largest
exponent \(e_{max}\), any decimal larger than
\(\beta\cdot\beta^{e_{max}}\) is larger than the largest number possibly
representated.
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q102}{}{\label{exr:q102} }Suppose \(e = -1\), what
are the range of numbers that can be represented for an arbitrary base
\(\beta \in \{2,3,...\}\)?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}The smallest number that can be represented
for an arbitrary base must be
\((0+0\cdot \beta^{-1} + ... + 0\cdot\beta^{-(p-1)})\cdot\beta^{-1}\).

Since \(0 \leq d_i < \beta, \forall i\), the largest value must be
attained when \(d_i = \beta - 1\) for all \(i\). I.e. the largest value
must be

\begin{align*}
  MAX &= (\beta-1 + (\beta-1)\beta^{-1} + ... + (\beta-1)\beta^{-(p-1)})\cdot\beta^{-1}\\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})(\beta-1)\cdot\beta^{-1} \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1}) \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1})
\end{align*}
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q103}{}{\label{exr:q103} }Characterize the numbers
that have a unique representation in a base \(\beta \in \{2,3,...\}\).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}Let

\[f = \left(d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)}\right)\cdot\beta^{e},\]

i.e. \(f\) is not normarlized. Then,

\[
f = \left(d_1+d_2\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-p} + 0\cdot \beta^{-(p-1)}\right)\cdot\beta^{e-1}.
\]

So, non-normalized fp numbers are NOT unique.

Now, let \(f\) be a normalized fp number. I.e.

\[
f = \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^e,
\] where \(d_0 \neq 0\). If we let \(e_n < e\), then

\[
f > \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n},
\]

and if \(e_n > e\), then

\[
f < \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n}
\]

If we let \[d_i^\prime \neq d_i\] for some number of \(i\)'s, then

\[
f \neq \left(d_0^\prime + d_1\prime\cdot\beta^{-1} + \ldots + d_{p-1}\prime\cdot\beta^{-(p-1)} \right)\cdot\beta^e.
\]

Hence, normalized FP numbers are unique.
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q104}{}{\label{exr:q104} }Write a function that
takes a decimal number, base, and precision, and returns the closest
normalized FP representation. I.e. a vector of digits and the exponent.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}The function provided in class is actually
the solution (?). This is guarenteed to give a normalized FP
representation. Using this algorithm gives
\(d_0 = \lfloor \frac{N}{\beta^{\lfloor log_{\beta}\left(N\right) \rfloor}}\rfloor\).
It holds that
\(\lfloor log_{\beta}\left(N\right) \rfloor \le log_\beta\left(N\right)\),
which implies that
\(\beta^{\lfloor log_{\beta}\left(N\right) \rfloor} \le \beta^{log_\beta\left(N\right)} = N\)
(remember, \(\beta \geq 2\)). Hence, \(d_0 > 0\).
\EndKnitrBlock{solution}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get_normalized_FP = }\KeywordTok{function}\NormalTok{(number::}\DataTypeTok{Float64}\NormalTok{, base::}\DataTypeTok{Int64}\NormalTok{, prec::}\DataTypeTok{Int64}\NormalTok{)}
    \CommentTok{#number = 4; base = float(10); prec = 2}
\NormalTok{    si=sign(number)}
\NormalTok{    base = float(base)}
\NormalTok{    e = floor(}\DataTypeTok{Int64}\NormalTok{,log(base,abs(number)))}
\NormalTok{    d = zeros(}\DataTypeTok{Int64}\NormalTok{,prec)}
\NormalTok{    num = abs(number)/(base^e)}
    
    \KeywordTok{for}\NormalTok{ j = }\FloatTok{1}\NormalTok{:prec}
\NormalTok{        d[j] = floor(}\DataTypeTok{Int64}\NormalTok{,num)}
\NormalTok{        num = (num - d[j])*base}
    \KeywordTok{end}

    \KeywordTok{return} \StringTok{"The sign is $si, the exponent is $e, and the vector with d is $d"}
\KeywordTok{end}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## #11 (generic function with 1 method)
\end{verbatim}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q105}{}{\label{exr:q105} }List all normalized fp
numbers that can be representated given base, precision, \(e_{min}\),
and \(e_{max}\).
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all_normalized_fp = }\KeywordTok{function}\NormalTok{(base::}\DataTypeTok{Int64}\NormalTok{, prec::}\DataTypeTok{Int64}\NormalTok{, emin::}\DataTypeTok{Int64}\NormalTok{, emax::}\DataTypeTok{Int64}\NormalTok{)}
    \CommentTok{## Number of possible values for each e:}
\NormalTok{    N = (base-}\FloatTok{1}\NormalTok{)*base^(prec-}\FloatTok{1}\NormalTok{)}\CommentTok{#*(emax-emin+1)}

\NormalTok{    out=zeros(}\DataTypeTok{Int64}\NormalTok{, N, prec, emax-emin+}\FloatTok{1}\NormalTok{)}

\NormalTok{    es = emin:emax}

    \KeywordTok{for}\NormalTok{ e=}\FloatTok{1}\NormalTok{:length(es)}
        \KeywordTok{for}\NormalTok{ b0=}\FloatTok{1}\NormalTok{:(base-}\FloatTok{1}\NormalTok{)}
            \KeywordTok{for}\NormalTok{ i=}\FloatTok{1}\NormalTok{:(base^(prec-}\FloatTok{1}\NormalTok{))}
\NormalTok{                out[(b0-}\FloatTok{1}\NormalTok{)*(base^(prec-}\FloatTok{1}\NormalTok{))+i,}\FloatTok{1}\NormalTok{,e] = b}\FloatTok{0}
                \KeywordTok{for}\NormalTok{ j=}\FloatTok{1}\NormalTok{:(prec-}\FloatTok{1}\NormalTok{)}
\NormalTok{                    out[(b0-}\FloatTok{1}\NormalTok{)*(base^(prec-}\FloatTok{1}\NormalTok{))+i,prec-j+}\FloatTok{1}\NormalTok{,e] = floor((i-}\FloatTok{1}\NormalTok{)/base^(j-}\FloatTok{1}\NormalTok{))%base}
                \KeywordTok{end}
            \KeywordTok{end}
        \KeywordTok{end}
    \KeywordTok{end}

    \KeywordTok{return}\NormalTok{(out)}
\KeywordTok{end}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## #13 (generic function with 1 method)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]

\end{Highlighting}
\end{Shaded}

\section{Homework 2}\label{homework-2}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q201}{}{\label{exr:q201} }Lookup the 64 bit
standard to find allowed exponents.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}According to
\href{https://en.wikipedia.org/wiki/Double-precision_floating-point_format}{Wikipedia},
the allowed exponents for the 64 bit standard are
\(-1022,\ldots, 1023\).
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q202}{}{\label{exr:q202} }What is the smallest
non-normalized positive value for the 64 bit standard?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}The smallest non-normalized positive value
is

\[
  \left(0 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-51} + 1\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1074} \approx 4.94\cdot 10^{324}
\]
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q203}{}{\label{exr:q203} }What is the smallest
normalized positive value?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}The smallest normalized positive value is

\[
  \left(1 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1022} \approx 2.23 \cdot 10^{308}
\]
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q204}{}{\label{exr:q204} }What is the largest
normalized positive value?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}The largest normalized finite value is

\[
  \left(1 + 1\cdot 2^{-1} + \ldots + 1\cdot 2^{-52} \right )\cdot 2^{-1022} \approx 1.80\cdot 10^{308}.
\]
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q205}{}{\label{exr:q205} }Is there a general
formula for determining the largest positive value for a given base
\(\beta\), precision \(p\), and largest exponent \(e_{max}\)?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}The largest positive, finite value is

\[
  \left(\sum_{i=0}^{p-1} (\beta-1)\beta^{-i}\right)\cdot \beta^{e_{max}}.
\]
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q206}{}{\label{exr:q206} }Verify the smallest
non-normalized, positive number that can be represented.
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nextfloat(}\DataTypeTok{Float64}\NormalTok{(}\FloatTok{0}\NormalTok{)) == }\FloatTok{2}\NormalTok{^(-}\FloatTok{1074}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## true
\end{verbatim}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q207}{}{\label{exr:q207} }Verify the smallest
normalized, positive number that can be represented.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q208}{}{\label{exr:q208} }Verify the largest,
finite number that can be represented.
\EndKnitrBlock{exercise}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prevfloat(}\DataTypeTok{Float64}\NormalTok{(Inf))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 1.7976931348623157e308
\end{verbatim}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q209}{}{\label{exr:q209} }Proof lemma (bound of
relative error).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q210}{}{\label{exr:q210} }What happens with lemmas
(bounds of absolute and relative error) if we consider negative numbers?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}They still hold. Let \(z^\prime = -z\).
Then \(fl(z^\prime) = -fl(z)\). Hence,

\[
  \left| fl(z^\prime) - z^\prime \right | = \left | -fl(z) + z \right | = \left | fl(z) - z \right |,
\]

hence the bounds still hold.
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q211}{}{\label{exr:q211} }Show that
\(\left|\left| A \right|\right|_1 =\) max of the \(l^1\) norms of the
columns of \(A\).
\EndKnitrBlock{exercise}

\begin{solution} \label{s211}
By definition, $\norm{A}_1 = \max_{x\neq 0}\frac{\norm{Ax}_1}{\norm{x}_1}$. Let $A \in \R^{m\times n}$ and $x \in \R^n$ s.t. $\norm{x}_1 = 1$. 

Recall that 

$$Ax = \sum_{j=1}^n x_j A_{*,j},$$

where $A_{*,j}$ is the $j^\prime$th column of $A$. So

\begin{align*}
  \norm{Ax}_1 = \sum_{i=1}^m \sum_{j=1}^n | x_j A_{i,j} | \\
              &\leq \sum_{i=1}^m \sum_{j=1}^n | x_j | \cdot | A_{i,j} | \\
              &= \sum_{j=1}^n | x_j | \left\{\sum_{i=1}^m | A_{i,j} | \right \} \\
              &= \sum_{j=1}^n | x_j | \norm{A_{*,j}} \\
              &\leq \sum_{j=1}^n | x_j | \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}} \\
              &= \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}}
\end{align*}

Since $\max_{j \in \{1, \ldots, n\}}\norm{A_{*,j}} = \norm{A\cdot \mathbf{1}_i}$ for $i = \argmax \norm{A_{*,j}}_1$. Here, we let $1_i = (x_j)_{j=1}^n$ be defined as 

$$x_j = \left\{ \begin{array}{rl} 1, & \text{ if } j = i \\ 0, & \text{ otherwise}\end{array} \right .$$
\end{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q212}{}{\label{exr:q212} }Show that
\(\left|\left| A \right|\right|_\infty =\) max of the \(l^1\) norms of
the rows of \(A\).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q213}{}{\label{exr:q213} }Assume the Fundamental
Axiom. Show the following:

\[\left|\left| fl(A)-A \right|\right|_p \leq u \left|\left|A\right|\right|_p\]
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}

\begin{align*}
  \left | \left | fl(A) - A \right | \right |_p &= \left|\left| \left [ fl(a_{ij}) - a_{ij} \right ] \right|\right|_p \\
    &\leq \left|\left| \left[u\cdot a_{ij}\right]\right|\right|_p \\
    &= \left|\left| u\cdot A\right|\right|_p = u\left|\left|A\right|\right|
\end{align*}
\EndKnitrBlock{solution}

\begin{solution}
\begin{align*}
  \left | \left | fl(A) - A \right | \right |_p &= \left|\left| \left [ fl(a_{ij}) - a_{ij} \right ] \right|\right|_p \\
                                                &\leq \left|\left| \left[u\cdot a_{ij}\right]\right|\right|_p \\
                                                &= \norm{u\cdot A}_p = u\left|\left|A\right|\right|
\end{align*}
\end{solution}

\section{Homework 3}\label{homework-3}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q301}{}{\label{exr:q301} }Prove lemma
\ref{lem:proofHW}.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Recall that a matrix \(A\) is invertible if
and only if \(Ax = 0\) implies that \(x = 0\). So to check that \(I-F\)
is invertible, we check this:

\[
  (I-F)x = x-Fx \Rightarrow x = Fx \Rightarrow \norm{x}_p \leq \norm{F}_p \norm{x}_p.
\]

Since \(\norm{F}_p < 1\) by assumption, the only solution to the
inequality above is \(x = 0\). So, \(I-F\) is invertible.
\EndKnitrBlock{proof}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q302}{}{\label{exr:q302} }Consider Theorem and
Lemmas under ``Square Linear Systems''. What happens if we use
\(l^{1}\)-norm instead?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q303}{}{\label{exr:q303} }Generate examples that
show the bound in ??? is too conservative.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q304}{}{\label{exr:q304} }Generate examples that
show the bound is nearly achieved
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q305}{}{\label{exr:q305} }For motivating problems
1-5, when is \(x\) unique?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q306}{}{\label{exr:q306} }For motivating problem 5,
what happens if \(p\geq m\)? Explore the case where \(m >> n\).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q307}{}{\label{exr:q307} }Suppose
\(R \in \R^{m\times m}\) is an upper triangular matrix with
\(R_{ii} \neq 0\) for all \(i = 1,\ldots, n\). Is \(R\) invertible?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}Since \(R\) is an upper triangular matrix,
\(\det(R) = \prod_{i=1}^{m} R_{ii} > 0\). Hence, \(R\) is invertible.
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q308}{}{\label{exr:q308} }Assume \(R\) is an
invertible upper triangular matrix. Implement a solution to invert
\(R\).
\EndKnitrBlock{exercise}

\section{Homework 4}\label{homework-4}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q401}{}{\label{exr:q401} }In the solution to
\ref{exm:linear-system}, why do \(0\) rows on the left-hand side of
\eqref{eq:sol-linear-system} correspond to \(0\) entries of the \(c\)
vector on the right-hand side.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q402}{}{\label{exr:q402} }Let \(Q\) be an
orthogonal matrix. Show that \(\norm{Qx}_2 = \norm{x}_2\).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q403}{}{\label{exr:q403} }Let f be a vector-valued
function over \(\R^d\). When is
\[\min_x \norm{f(x)}_2 = \min_x \norm{f(x)}_2^2.\]
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q404}{}{\label{exr:q404} }Prove that \(P^T P + I\)
from solution to example \ref{exm:und-linear-system} is invertible.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q405}{}{\label{exr:q405} }Write out the solution to
example \ref{exm:constrained-least-squares}. Also consider the case
where \(p \ge m\).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q406}{}{\label{exr:q406} }For all motivating
problems, implement solutions.
\EndKnitrBlock{exercise}

For the following, assume \(A \in \R^{n\times m}\) with
\(\text{rank}(A) = m\), and \(b \in \R^n\). Let
\(C = \begin{bmatrix} A b \end{bmatrix}\).

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q407}{}{\label{exr:q407} }What does the last column
of \(R\) (from the QR decomposition of \(C\)) represent?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q408}{}{\label{exr:q408} }What does the last entry
of last column of \(R\) (from the QR decomposition of \(C\)) represent?
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q409}{}{\label{exr:q409} }How can this be used in
computation?
\EndKnitrBlock{exercise}

\section{Homework 5}\label{homework-5}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q501}{}{\label{exr:q501} }Implement the
Gram-Schmidt procedure for matrices \(A \in \R^{n \times m}\) assuming
\(A\) has full column rank.

Create examples to show that the function works (well enough).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q502}{}{\label{exr:q502} }Find examples where
Gram-Schmidt fails, i.e.~where either \(Q R \neq A\) or \(Q^TQ \neq I\).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q503}{}{\label{exr:q503} }Look up the modified
Gram-Schmidt Procedure and implement it (again assuming \(A\) has full
column rank).
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}[Pivoting (*OPTIONAL*)]
\protect\hypertarget{exr:q504}{}{\label{exr:q504} \iffalse (Pivoting
(\emph{OPTIONAL})) \fi{} }References:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Businger, Galub: Linear Least Squares by Householder Transformation
  (1965)
\item
  Engler: The Behavior of QR-factorization algorithm with column
  pivoting (1997)
\end{enumerate}

Implement modified Gram-Schmidt with column pivoting.

Find example where the modified Gram-Schmidt fails, but the modified
Gram-Schmidt with column pivoting does not.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q505}{}{\label{exr:q505} }Show that Householder
reflections are orthogonal matrices.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}Show that \(H^\prime H = I\).
\EndKnitrBlock{solution}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q506}{}{\label{exr:q506} }Show that
\(\begin{bmatrix} H_r \cdots h_1 \end{bmatrix}\) is orthogonal.
\EndKnitrBlock{exercise}

\BeginKnitrBlock{exercise}
\protect\hypertarget{exr:q507}{}{\label{exr:q507} }Show that a Givens
rotation is an orthonormal matrix when \(\sigma^2 + \lambda^2 = 1\).
\EndKnitrBlock{exercise}


\end{document}
