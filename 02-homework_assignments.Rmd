# Homework Assignments

## Homework 1

```{exercise, q101} 
Can all nonnegative real numbers be represented in such a manner (i.e. as a fp number) for an arbitrary base $\beta \in \{2,3,...\}$? 
```

```{solution}
No. For any given $\beta$ and a largest exponent $e_{max}$, any decimal larger than $\beta\cdot\beta^{e_{max}}$ is larger than the largest number possibly representated. 
```


```{exercise, q102}
Suppose $e = -1$, what are the range of numbers that can be represented for an arbitrary base $\beta \in \{2,3,...\}$?
```

```{solution}
The smallest number that can be represented for an arbitrary base must be $(0+0\cdot \beta^{-1} + ... + 0\cdot\beta^{-(p-1)})\cdot\beta^{-1}$. 

Since $0 \leq d_i < \beta, \forall i$, the largest value must be attained when $d_i = \beta - 1$ for all $i$. I.e. the largest value must be

\begin{align*}
  MAX &= (\beta-1 + (\beta-1)\beta^{-1} + ... + (\beta-1)\beta^{-(p-1)})\cdot\beta^{-1}\\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})(\beta-1)\cdot\beta^{-1} \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1}) \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1})
\end{align*}
```

```{exercise, q103}
Characterize the numbers that have a unique representation in a base $\beta \in \{2,3,...\}$.
```

```{solution}
Let 

$$f = \left(d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)}\right)\cdot\beta^{e},$$ 

i.e. $f$ is not normarlized. Then, 

$$
f = \left(d_1+d_2\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-p} + 0\cdot \beta^{-(p-1)}\right)\cdot\beta^{e-1}.
$$

So, non-normalized fp numbers are NOT unique. 

Now, let $f$ be a normalized fp number. I.e.

$$
f = \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^e,
$$
where $d_0 \neq 0$. If we let $e_n < e$, then 

$$
f > \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n},
$$

and if $e_n > e$, then

$$
f < \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n}
$$

If we let $$d_i^\prime \neq d_i$$ for some number of $i$'s, then

$$
f \neq \left(d_0^\prime + d_1\prime\cdot\beta^{-1} + \ldots + d_{p-1}\prime\cdot\beta^{-(p-1)} \right)\cdot\beta^e.
$$

Hence, normalized FP numbers are unique.
```

```{exercise, q104}
Write a function that takes a decimal number, base, and precision, and returns the closest normalized FP representation. I.e. a vector of digits and the exponent.
```

```{solution}
The function provided in class is actually the solution (?). This is guarenteed to give a normalized FP representation. Using this algorithm gives $d_0 = \lfloor \frac{N}{\beta^{\lfloor log_{\beta}\left(N\right) \rfloor}}\rfloor$. It holds that $\lfloor log_{\beta}\left(N\right) \rfloor \le log_\beta\left(N\right)$, which implies that $\beta^{\lfloor log_{\beta}\left(N\right) \rfloor} \le \beta^{log_\beta\left(N\right)} = N$ (remember, $\beta \geq 2$). Hence, $d_0 > 0$.
```

```{julia}
get_normalized_FP = function(number::Float64, base::Int64, prec::Int64)
    #number = 4; base = float(10); prec = 2
    si=sign(number)
    base = float(base)
    e = floor(Int64,log(base,abs(number)))
    d = zeros(Int64,prec)
    num = abs(number)/(base^e)
    
    for j = 1:prec
        d[j] = floor(Int64,num)
        num = (num - d[j])*base
    end

    return "The sign is $si, the exponent is $e, and the vector with d is $d"
end
```

```{exercise, q105}
List all normalized fp numbers that can be representated given base, precision, $e_{min}$, and $e_{max}$. 
```

```{julia}
all_normalized_fp = function(base::Int64, prec::Int64, emin::Int64, emax::Int64)
    ## Number of possible values for each e:
    N = (base-1)*base^(prec-1)#*(emax-emin+1)

    out=zeros(Int64, N, prec, emax-emin+1)

    es = emin:emax


    for e=1:length(es)
        for b0=1:(base-1)
            for i=1:(base^(prec-1))
                out[(b0-1)*(base^(prec-1))+i,1,e] = b0
                for j=1:(prec-1)
                    out[(b0-1)*(base^(prec-1))+i,prec-j+1,e] = floor((i-1)/base^(j-1))%base
                end
            end
        end
    end

    return(out)
end

```


## Homework 2

```{exercise, q201}
Lookup the 64 bit standard to find allowed exponents.
```

```{solution, s201}
According to [Wikipedia](https://en.wikipedia.org/wiki/Double-precision_floating-point_format), the allowed exponents for the 64 bit standard are $-1022,\ldots, 1023$. 
```

```{exercise, q202}
What is the smallest non-normalized positive value for the 64 bit standard?
```

```{solution, s202}
The smallest non-normalized positive value is 

$$
  \left(0 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-51} + 1\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1074} \approx 4.94\cdot 10^{324}
$$
```

```{exercise, q203}
What is the smallest normalized positive value?
```

```{solution, s203}
The smallest normalized positive value is 

$$
  \left(1 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1022} \approx 2.23 \cdot 10^{308}
$$
```

```{exercise, q204}
What is the largest normalized positive value?
```

```{solution, s204}
The largest normalized finite value is

$$
  \left(1 + 1\cdot 2^{-1} + \ldots + 1\cdot 2^{-52} \right )\cdot 2^{-1022} \approx 1.80\cdot 10^{308}.
$$
```

```{exercise, q205}
Is there a general formula for determining the largest positive value for a given base $\beta$, precision $p$, and largest exponent $e_{max}$? 
```

```{solution, s205}
The largest positive, finite value is 

$$
  \left(\sum_{i=0}^{p-1} (\beta-1)\beta^{-i}\right)\cdot \beta^{e_{max}}.
$$
```

```{exercise, q206}
Verify the smallest non-normalized, positive number that can be represented.
```

```{solution}
See the `Julia` chunk below.
```

```{julia}
nextfloat(Float64(0)) == 2^(-1074)
```

```{exercise, q207}
Verify the smallest normalized, positive number that can be represented.
```

```{exercise, q208}
Verify the largest, finite number that can be represented.
```

```{julia}
prevfloat(Float64(Inf))
```

```{exercise, q209}
Proof lemma (\@ref(lem:absolute-error-bound)). 
```

```{solution}

Let $z = (d_0 + d_1 \beta^{-1} + \dots)\beta^e$ be a number. Let $fl(z) = (d_0^\prime + d_1^\prime\beta^{-1} + \dots + d_{p-1}^\prime\beta^{-(p-1)})\beta^e$ be its fp representation with precision $p$. 

We know that $(d_0 + d_1 \beta^{-1} + \dots + d_{p-1}\beta^{-(p-1)})\beta^e \le z \le (d_0 + d_1 \beta^{-1} + \dots + (d_{p-1}+1)\beta^{-(p-1)})\beta^e$. We know that $fl(z)$ is equal to the one of these two bounds that is closest to $z$. Hence, $|fl(z) - z|$ must be at most half the distance between these two. Subtract the upper bound from the lower bound, and you get $\beta^{-(p-1)+e}$, i.e.

$$
  |fl(z) - z| \le \frac{\beta^{e-p+1}}{2}.
$$

```

```{exercise, q210}
What happens with lemmas (bounds of absolute and relative error) if we consider negative numbers?
```

```{solution, s209}
They still hold. Let $z^\prime = -z$. Then $fl(z^\prime) = -fl(z)$. Hence,

$$
  \left| fl(z^\prime) - z^\prime \right | = \left | -fl(z) + z \right | = \left | fl(z) - z \right |,
$$

hence the bounds still hold. 
```

```{exercise, q211}
Show that $\left|\left| A \right|\right|_1 =$ max of the $l^1$ norms of the columns of $A$.
```

```{solution, label = 's211'}
By definition, $\norm{A}_1 = \max_{x\neq 0}\frac{\norm{Ax}_1}{\norm{x}_1}$. Let $A \in \R^{m\times n}$ and $x \in \R^n$ s.t. $\norm{x}_1 = 1$. 

Recall that 

$$Ax = \sum_{j=1}^n x_j A_{*,j},$$

where $A_{*,j}$ is the $j^\prime$th column of $A$. So

$$
\begin{aligned}
  \norm{Ax}_1 &= \sum_{i=1}^m \sum_{j=1}^n | x_j A_{i,j} | \\
              &\leq \sum_{i=1}^m \sum_{j=1}^n | x_j | \cdot | A_{i,j} | \\
              &= \sum_{j=1}^n | x_j | \left\{\sum_{i=1}^m | A_{i,j} | \right \} \\
              &= \sum_{j=1}^n | x_j | \norm{A_{*,j}} \\
              &\leq \sum_{j=1}^n | x_j | \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}} \\
              &= \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}}
\end{aligned}
$$

Since $\max_{j \in \{1, \ldots, n\}}\norm{A_{*,j}} = \norm{A\cdot \mathbf{1}_i}$ for $i = \argmax \norm{A_{*,j}}_1$. Here, we let $1_i = (x_j)_{j=1}^n$ be defined as 

$$x_j = \left\{ \begin{array}{rl} 1, & \text{ if } j = i \\ 0, & \text{ otherwise}\end{array} \right .$$
```


```{exercise, q212}
Let $A \in \R^{n \times m}$. Show that $\left|\left| A \right|\right|_\infty =$ max of the $l^1$ norms of the rows of $A$.
```

```{solution, label = "s212"}
Let $j \in \{1, \ldots, n\}$ such that $|a_{j1}| + \dots + |a_{jm}| \ge |a_{i1}| + \dots + |a_{im}|$ for all $i \in \{1, \ldots , n\}$, and let $\tilde{x} \in \R^{m}$ such that $\tilde{x}_i = sign(a_{ji})$. Note that for all $x \in \R^{m}$ with $\norm{x}_\infty = 1$ it holds that $|x_i| \le 1$. So,

$$\begin{aligned}
  |a_{i1}x_1 + \dots + a_{im}x_m| &\le |a_{i1}| + \dots + |a_{im}| \\
                                  &\le |a_{j1}| + \dots + |a_{jm}| \\
                                  &= |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1|,
\end{aligned}$$
  
I.e. for any $x \in \R^m$ with $\norm{x}_\infty = 1$, 

$$\begin{aligned}
  |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1| &\ge \max_{1 \le i \le n}\{|a_{i1} x_1 + \dots + a_{im}x_m|\} \\
                                                   & = \norm{Ax}_\infty. 
\end{aligned}$$
  
Since $\norm{\tilde{x}}_\infty = 1$, we have that 

$$
  \max_{\norm{x} = 1} \norm{Ax}_\infty &= |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1| \\
                                       &= |a_{j1}| + \dots + |a_{jm}| \\
                                       &= \max_{1\le j \le n} \sum_{i=1}^n |a_{ji}|,
$$
  
which is exactly the maximum over the $\mathcal{l}^1$-norms of the rows in $A$. 

```

```{exercise, q213}
Assume the Fundamental Axiom. Show the following:
  
$$\left|\left| fl(A)-A \right|\right|_p \leq u \left|\left|A\right|\right|_p$$
```

```{solution, s213}
\begin{align*}
  \left | \left | fl(A) - A \right | \right |_p &= \left|\left| \left [ fl(a_{ij}) - a_{ij} \right ] \right|\right|_p \\
    &\leq \left|\left| \left[u\cdot a_{ij}\right]\right|\right|_p \\
    &= \left|\left| u\cdot A\right|\right|_p = u\left|\left|A\right|\right|
\end{align*}
```

\begin{solution}
\begin{align*}
  \left | \left | fl(A) - A \right | \right |_p &= \left|\left| \left [ fl(a_{ij}) - a_{ij} \right ] \right|\right|_p \\
                                                &\leq \left|\left| \left[u\cdot a_{ij}\right]\right|\right|_p \\
                                                &= \norm{u\cdot A}_p = u\left|\left|A\right|\right|
\end{align*}
\end{solution}


## Homework 3

```{exercise, q301}
Prove lemma \@ref(lem:proofHW). 
```

```{proof}
Recall that a matrix $A$ is invertible if and only if $Ax = 0$ implies that $x = 0$. So to check that $I-F$ is invertible, we check this:
  
$$
  (I-F)x = x-Fx \Rightarrow x = Fx \Rightarrow \norm{x}_p \leq \norm{F}_p \norm{x}_p.
$$
  
Since $\norm{F}_p < 1$ by assumption, the only solution to the inequality above is $x = 0$. So, $I-F$ is invertible. 


  
```

```{exercise, q302}
Consider Theorem and Lemmas under "Square Linear Systems" (\@ref(square-linear-systems)). What happens if we use $l^{1}$-norm instead?
```

```{solution}
Since we never use any properties of the infinity norm to prove these theorems and lemmas, we could replace it with the $\mathcal{l}^1$-norm. 
```



```{exercise, q303}
Generate examples that show the bound in theorem \@ref(thm:kappa-bound) is too conservative.
```



```{exercise, q304}
Generate examples that show the bound is nearly achieved
```


```{exercise, q305}
For motivating problems 1-5, when is $x$ unique?
```

```{solution}

* Motivating Problem 1: x is unique by definition
* Motivating Problem 2: 

```

```{exercise, q306}
For motivating problem 5, what happens if $p\geq m$? Explore the case where $m >> n$.
```

```{exercise, q307}
Suppose $R \in \R^{m\times m}$ is an upper triangular matrix with $R_{ii} \neq 0$ for all $i = 1,\ldots, n$. Is $R$ invertible?
```

```{solution}
Since $R$ is an upper triangular matrix, $\det(R) = \prod_{i=1}^{m} R_{ii} > 0$. Hence, $R$ is invertible. 
```

```{exercise, q308}
Assume $R$ is an invertible upper triangular matrix. Implement a solution to invert $R$. 
```

```{solution, s308}
First, we will show that the inverse of $R$ is also an upper triangular matrix. So, let $B = R^{-1}$. The $RB = I$. We will show this using induction. Let $i = n - 0, j < i$. Then, since $r_{ij} = 0$ for all $i > j$,

$$\begin{aligned}
  0 &= \sum_{k=1}^n r_{n,k}b_{k,j} \\
    &= r_{n,n}b_{n,j}.
\end{aligned}$$
    
Since $R$ is invertible, $r_{n,n} \neq 0$, hence $b_{n,j} = 0$ for all $j < n$. 
  

Now assume that $b_{i,j} = 0$ for all $i=n-0,n-1, \ldots, n-m, j < i$. We then want to show it holds for $i = n - (m+1) = n-m-1$. Let $j < n-(m+1)$. Then

$$\begin{aligned}
  0 &= \sum_{k=1}^n r_{n-(m+1), k} b_{k, j} \\
    &= \sum_{k=n-(m+1)}^{n} r_{n-(m+1), k} b_{k, j} \\
    &= r_{n-(m+1), n-(m+1)} b_{n-(m+1),j},
\end{aligned}$$
    
where the first equality is due to the fact that $r_{ij} = 0$ for all $i > j$, and the last equality holds since $b_{k,j} = 0$ for all $j < k$ when $k \ge n-m$ (per the induction hypothesis). Since $r_{ii} \neq 0$ for all $i$, $b_{n-(m+1),j} = 0$. 
  
So, $b_{ij} = 0$ for all $i > j$. 

Now, lets look at the case where $i=j$, i.e. diagonal elements of the inverse matrix. Then

$$\begin{aligned}
  1 = \sum_{k=1}^n r_{ik} b_{ki} = \sum_{k=i}^{n} r_{ik} b_{ki} = r_{ii}b_{ii}.
\end{aligned}$$

The second equality holds since $r_{ik} = 0$ for all $i > k$, the last equality holds because $b_{ki} = 0$ for all $k > i$. We see that $b_{ii} = r_{ii}^{-1}$.
  
Finally, consider the case where $i < j$. Then, using that $r_{ik} = 0$ for all $i>k$ and $b_{kj} = 0$ for all $k>j$, we see that 

$$
  0 = \sum_{k=1}^n r_{ik}b_{kj} = \sum_{k=i}^j r_{ik} b_{kj},
$$
  
which implies

$$
  b_{ij} = \frac{-\sum_{k=i+1}^j r_{ik} b_{kj}}{a_{ii}}.
$$
  
So, in other words, given $i,j$, we can find $b_{ij}$ if we know $b_{kj}$ for all $k>i$ (i.e. all entries in the same column below the entry we are considering). Since we already know all entries below and on the diagonal, this is true for all columns. Hence, we can construct $B$ this way. 

See the `Julia` chunk below for an implementation of this.

```

```{julia}
function invertUpperTri(A)
    ## Get dimensions of A
    n, m = size(A)

    ## Setup empty array to hold result
    B = zeros(n,m)

    ## Fill out diagonal with inverse diagonal from A
    for k = 1:n
        B[k, k] = 1/A[k,k]
    end

    ## Starting in the lower right corner, fill out the rest of the matrix.
    for i = (n-1):-1:1
        for j = (i+1):n
            B[i,j] = -sum(A[i,(i+1):j].*B[(i+1):j,j])/A[i,i]
        end
    end

    return(B)
end

## Create an upper triangular matrix to check
A = rand(4,4);
A[2:4,1] = [0 0 0];
A[3:4,2] = [0 0];
A[4,3] = 0;

## Check function
B = invertUpperTri(A);
B - inv(A)  # should be 0 matrix
A*B         # should be identity
```

## Homework 4

```{exercise, q401}
In the solution to \@ref(exm:linear-system), why do $0$ rows on the left-hand side of \@ref(eq:sol-linear-system) correspond to $0$ entries of the $c$ vector on the right-hand side. 
```

```{solution, s401}
By definition of matrix multiplication, if the $i$th row is a zero row of $A$, then $c_{ij} = [AB]_{ij} = \sum{j=1}^n a_{ij} b_{ij} = 0$, no matter what $B$ is. 
```

```{exercise, q402}
Let $Q$ be an orthogonal matrix. Show that $\norm{Qx}_2 = \norm{x}_2$. 
```

```{solution, s402}

Since $\norm{x}_2^2 = x^\prime x$ and $Q^\prime Q = 1$ ($Q$ is orthogonal), 

$$ 
  \norm{Qx}^2_2 = (Qx)^\prime Qx = x^\prime Q^\prime Q x = x^\prime x = \norm{x}^2_2.
$$

```

```{exercise, q403}
Let f be a vector-valued function over $\R^d$. When is $$\min_x \norm{f(x)}_2 = \min_x \norm{f(x)}_2^2.$$
```

```{solution, s403}
Should this be $\argmin_x \norm{f(x)}_2 = \argmin_x \norm{f(x)}_2^2$ instead? 
```


```{exercise, q404}
Prove that $P^T P + I$ from solution to example \@ref(exm:und-linear-system) is invertible. 
```

```{solution, s404}

$$
  (P^TP + I)x = P^TPx + x = 0 \Leftrightarrow P^TPx = -x \Leftrightarrow P^TP = -I \lor x = 0.
$$
  
```

```{exercise, q405}
Write out the solution to example \@ref(exm:constrained-least-squares). Also consider the case where $p \ge m$.
```

```{exercise, q406}
For all motivating problems, implement solutions. 
```

</br>

For the following, assume $A \in \R^{n\times m}$ with $\text{rank}(A) = m$, and $b \in \R^n$. Let $C = \begin{bmatrix} A b \end{bmatrix}$. 


```{exercise, q407}
What does the last column of $R$ (from the QR decomposition of $C$) represent?
```

```{exercise, q408}
What does the last entry of last column of $R$ (from the QR decomposition of $C$) represent?
```

```{exercise, q409}
How can this be used in computation?
```

## Homework 5

```{exercise, q501}
Implement the Gram-Schmidt procedure for matrices $A \in \R^{n \times m}$ assuming $A$ has full column rank.

Create examples to show that the function works (well enough).
```

```{exercise, q502}
Find examples where Gram-Schmidt fails, i.e. where either $Q R \neq A$ or $Q^TQ \neq I$. 
```

```{exercise, q503}
Look up the modified Gram-Schmidt Procedure and implement it (again assuming $A$ has full column rank). 
```


```{exercise, q504, name = "Pivoting (*OPTIONAL*)"}
References:
  
  1. Businger, Galub: Linear Least Squares by Householder Transformation (1965)
  2. Engler: The Behavior of QR-factorization algorithm with column pivoting (1997)
  
Implement modified Gram-Schmidt with column pivoting. 

Find example where the modified Gram-Schmidt fails, but the modified Gram-Schmidt with column pivoting does not.
  
```


```{exercise, q505}
Show that Householder reflections are orthogonal matrices.
```

```{solution}
Show that $H^\prime H = I$. 
```

```{exercise, q506}
Show that $\begin{bmatrix} H_r \cdots h_1 \end{bmatrix}$ is orthogonal.
```

```{exercise, q507}
Show that a Givens rotation is an orthonormal matrix when $\sigma^2 + \lambda^2 = 1$. 
```

## Homework 6

```{exercise, q601}
Determine the computational complexity of the QR decomposition using 

a) Gram-Schmidt
b) Modified Gram-Schmidt
c) Householder
d) Givens rotations

for any arbitrary, dense $n \times n$ matrix. (dense = don't know how many entries are $0$.)

```

```{exercise, q602}
Compare the computational complexity of Householder and Givens for a sparse matrix (i.e. a matrix where a substantial number of entries are $0$).
```

```{exercise, q603}
Implement QR decomposition using Householder. Write it as a function that takes a matrix $A \in \R^{n\times m}$ with $\text{rank}(A) = m$ as it's input, and gives back $Q$ and $R$.
```

```{exercise, q604}
Implement QR decomposition using Givens. (As above.)
```

```{exercise, q605}
What happens in theorem \@ref(thm:svd) if $m > n$.
```

```{exercise, q606}
If $u,v$ are respectively left and right singular vectors corresponding to the singular value $\sigma$, show that $Av = \sigma u$, and $A'u = \sigma v$. 
```


## Homework 7

```{exercise, q701}
Show that $\norm{A}_F^2 = \sum_{i=1}^n\sum_{j=1}^n a_{ij}^2$ is orthogonally invariant, i.e. if $Q,P$ are orthogonal matrices, then $\norm{QA}_F^2 = \norm{A}_F^2$ and $\norm{AP}_F^2 = \norm{A}_F^2$. 
```

```{exercise, q702}
Proof corollary \@ref(cor:sigma-min-max).
```