# Homework Assignments

## Homework 1

```{exercise, q101} 
Can all nonnegative real numbers be represented in such a manner (i.e. as a fp number) for an arbitrary base $\beta \in \{2,3,...\}$? 
```

```{solution}
No. For any given $\beta$ and a largest exponent $e_{max}$, any decimal larger than $\beta\cdot\beta^{e_{max}}$ is larger than the largest number possibly representated. 
```


```{exercise, q102}
Suppose $e = -1$, what are the range of numbers that can be represented for an arbitrary base $\beta \in \{2,3,...\}$?
```

```{solution}
The smallest number that can be represented for an arbitrary base must be $(0+0\cdot \beta^{-1} + ... + 0\cdot\beta^{-(p-1)})\cdot\beta^{-1}$. 

Since $0 \leq d_i < \beta, \forall i$, the largest value must be attained when $d_i = \beta - 1$ for all $i$. I.e. the largest value must be

\begin{align*}
  MAX &= (\beta-1 + (\beta-1)\beta^{-1} + ... + (\beta-1)\beta^{-(p-1)})\cdot\beta^{-1}\\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})(\beta-1)\cdot\beta^{-1} \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1}) \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1})
\end{align*}
```

```{exercise, q103}
Characterize the numbers that have a unique representation in a base $\beta \in \{2,3,...\}$.
```

```{solution}
Let 

$$f = \left(d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)}\right)\cdot\beta^{e},$$ 

i.e. $f$ is not normarlized. Then, 

$$
f = \left(d_1+d_2\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-p} + 0\cdot \beta^{-(p-1)}\right)\cdot\beta^{e-1}.
$$

So, non-normalized fp numbers are NOT unique. 

Now, let $f$ be a normalized fp number. I.e.

$$
f = \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^e,
$$
where $d_0 \neq 0$. If we let $e_n < e$, then 

$$
f > \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n},
$$

and if $e_n > e$, then

$$
f < \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n}
$$

If we let $$d_i^\prime \neq d_i$$ for some number of $i$'s, then

$$
f \neq \left(d_0^\prime + d_1\prime\cdot\beta^{-1} + \ldots + d_{p-1}\prime\cdot\beta^{-(p-1)} \right)\cdot\beta^e.
$$

Hence, normalized FP numbers are unique.
```

```{exercise, q104}
Write a function that takes a decimal number, base, and precision, and returns the closest normalized FP representation. I.e. a vector of digits and the exponent.
```

```{solution}
The function provided in class is actually the solution (?). This is guarenteed to give a normalized FP representation. Using this algorithm gives $d_0 = \lfloor \frac{N}{\beta^{\lfloor log_{\beta}\left(N\right) \rfloor}}\rfloor$. It holds that $\lfloor log_{\beta}\left(N\right) \rfloor \le log_\beta\left(N\right)$, which implies that $\beta^{\lfloor log_{\beta}\left(N\right) \rfloor} \le \beta^{log_\beta\left(N\right)} = N$ (remember, $\beta \geq 2$). Hence, $d_0 > 0$.
```

```{julia, eval = FALSE}
get_normalized_FP = function(number::Float64, base::Int64, prec::Int64)
    #number = 4; base = float(10); prec = 2
    si=sign(number)
    base = float(base)
    e = floor(Int64,log(base,abs(number)))
    d = zeros(Int64,prec)
    num = abs(number)/(base^e)
    
    for j = 1:prec
        d[j] = floor(Int64,num)
        num = (num - d[j])*base
    end

    return "The sign is $si, the exponent is $e, and the vector with d is $d"
end
```

```{exercise, q105}
List all normalized fp numbers that can be representated given base, precision, $e_{min}$, and $e_{max}$. 
```

```{julia, eval FALSE}
all_normalized_fp = function(base::Int64, prec::Int64, emin::Int64, emax::Int64)
    ## Number of possible values for each e:
    N = (base-1)*base^(prec-1)#*(emax-emin+1)

    out=zeros(Int64, N, prec, emax-emin+1)

    es = emin:emax


    for e=1:length(es)
        for b0=1:(base-1)
            for i=1:(base^(prec-1))
                out[(b0-1)*(base^(prec-1))+i,1,e] = b0
                for j=1:(prec-1)
                    out[(b0-1)*(base^(prec-1))+i,prec-j+1,e] = floor((i-1)/base^(j-1))%base
                end
            end
        end
    end

    return(out)
end

```


## Homework 2

```{exercise, q201}
Lookup the 64 bit standard to find allowed exponents.
```

```{solution, s201}
According to [Wikipedia](https://en.wikipedia.org/wiki/Double-precision_floating-point_format), the allowed exponents for the 64 bit standard are $-1022,\ldots, 1023$. 
```

```{exercise, q202}
What is the smallest non-normalized positive value for the 64 bit standard?
```

```{solution, s202}
The smallest non-normalized positive value is 

$$
  \left(0 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-51} + 1\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1074} \approx 4.94\cdot 10^{-324}
$$
```

```{exercise, q203}
What is the smallest normalized positive value?
```

```{solution, s203}
The smallest normalized positive value is 

$$
  \left(1 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1022} \approx 2.23 \cdot 10^{-308}
$$
```

```{exercise, q204}
What is the largest normalized positive value?
```

```{solution, s204}
The largest normalized finite value is

$$
  \left(1 + 1\cdot 2^{-1} + \ldots + 1\cdot 2^{-52} \right )\cdot 2^{1023} \approx 1.80\cdot 10^{308}.
$$
```

```{exercise, q205}
Is there a general formula for determining the largest positive value for a given base $\beta$, precision $p$, and largest exponent $e_{max}$? 
```

```{solution, s205}
The largest positive, finite value is 

$$
  \left(\sum_{i=0}^{p-1} (\beta-1)\beta^{-i}\right)\cdot \beta^{e_{max}} = ... = \frac{\beta^p - 1}{\beta^{p-1}}\beta^{e_max}.
$$
```

```{exercise, q206}
Verify the smallest non-normalized, positive number that can be represented.
```

```{solution}
See the `Julia` chunk below.
```

```{julia}
nextfloat(Float64(0)) == 2^(-1074)
```

```{exercise, q207}
Verify the smallest normalized, positive number that can be represented.
```

```{julia}
nextfloat(Float64(0))*2^(52)
```

```{exercise, q208}
Verify the largest, finite number that can be represented.
```

```{julia}
prevfloat(Float64(Inf))
```

```{exercise, q209}
Proof lemma (\@ref(lem:absolute-error-bound)). 
```

```{solution}

Let $z = (d_0 + d_1 \beta^{-1} + \dots)\beta^e$ be a number. Let $fl(z) = (d_0^\prime + d_1^\prime\beta^{-1} + \dots + d_{p-1}^\prime\beta^{-(p-1)})\beta^e$ be its fp representation with precision $p$. 

We know that $(d_0 + d_1 \beta^{-1} + \dots + d_{p-1}\beta^{-(p-1)})\beta^e \le z \le (d_0 + d_1 \beta^{-1} + \dots + (d_{p-1}+1)\beta^{-(p-1)})\beta^e$. We know that $fl(z)$ is equal to the one of these two bounds that is closest to $z$. Hence, $|fl(z) - z|$ must be at most half the distance between these two. Subtract the upper bound from the lower bound, and you get $\beta^{-(p-1)+e}$, i.e.

$$
  |fl(z) - z| \le \frac{\beta^{e-p+1}}{2}.
$$

```

```{exercise, q210}
What happens with lemmas (bounds of absolute and relative error) if we consider negative numbers?
```

```{solution, s209}
They still hold. Let $z^\prime = -z$. Then $fl(z^\prime) = -fl(z)$. Hence,

$$
  \left| fl(z^\prime) - z^\prime \right | = \left | -fl(z) + z \right | = \left | fl(z) - z \right |,
$$

hence the bounds still hold. 
```

```{exercise, q211}
Show that $\left|\left| A \right|\right|_1 =$ max of the $l^1$ norms of the columns of $A$.
```

```{solution, label = 's211'}
By definition, $\norm{A}_1 = \max_{x\neq 0}\frac{\norm{Ax}_1}{\norm{x}_1} = \max_{\norm{x}_1 = 1} \norm{Ax}_1$. Let $A \in \R^{m\times n}$ and $x \in \R^n$ s.t. $\norm{x}_1 = 1$. 

Recall that 

$$Ax = \sum_{j=1}^n x_j A_{*,j},$$

where $A_{*,j}$ is the $j^\prime$th column of $A$. So

$$
\begin{aligned}
  \norm{Ax}_1 &= \sum_{i=1}^m \left | \sum_{j=1}^n  x_j A_{i,j} \right| \\
              &\leq \sum_{i=1}^m \sum_{j=1}^n | x_j | \cdot | A_{i,j} | \\
              &= \sum_{j=1}^n | x_j | \left\{\sum_{i=1}^m | A_{i,j} | \right \} \\
              &= \sum_{j=1}^n | x_j | \norm{A_{*,j}} \\
              &\leq \sum_{j=1}^n | x_j | \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}} \\
              &= \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}}
\end{aligned}
$$

I.e. $\max_{\norm{x}_1 = 1} \norm{Ax}_1 \le \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}}$. 
  
Now, let $1_i = (x_j)_{j=1}^n$ be defined as 

$$x_j = \left\{ \begin{array}{rl} 1, & \text{ if } j = i \\ 0, & \text{ otherwise}\end{array} \right .$$

Then $\norm{1_i}_1 = 1$. Since $\max_{j \in \{1, \ldots, n\}}\norm{A_{*,j}} = \norm{A\cdot \mathbf{1}_i}$ for $i = \argmax \norm{A_{*,j}}_1$, we have that $\max_{j \in \{1,\ldots,n\}} \norm{A_{*,j}}_1 \le \max_{\norm{x}_1 = 1} \norm{Ax}_1$. 


```


```{exercise, q212}
Let $A \in \R^{n \times m}$. Show that $\left|\left| A \right|\right|_\infty =$ max of the $l^1$ norms of the rows of $A$.
```

```{solution, label = "s212"}
Let $j \in \{1, \ldots, n\}$ such that $|a_{j1}| + \dots + |a_{jm}| \ge |a_{i1}| + \dots + |a_{im}|$ for all $i \in \{1, \ldots , n\}$, and let $\tilde{x} \in \R^{m}$ such that $\tilde{x}_i = \text{sign}(a_{ji})$. Note that for all $x \in \R^{m}$ with $\norm{x}_\infty = 1$ it holds that $|x_i| \le 1$. So,

$$\begin{aligned}
  |a_{i1}x_1 + \dots + a_{im}x_m| &\le |a_{i1}| + \dots + |a_{im}| \\
                                  &\le |a_{j1}| + \dots + |a_{jm}| \\
                                  &= |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1|,
\end{aligned}$$
  
I.e. for any $x \in \R^m$ with $\norm{x}_\infty = 1$, 

$$\begin{aligned}
  |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1| &\ge \max_{1 \le i \le n}\{|a_{i1} x_1 + \dots + a_{im}x_m|\} \\
                                                   & = \norm{Ax}_\infty. 
\end{aligned}$$
  
Since $\norm{\tilde{x}}_\infty = 1$, we have that 

$$\begin{aligned}
  \max_{\norm{x} = 1} \norm{Ax}_\infty &= |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1| \\
                                       &= |a_{j1}| + \dots + |a_{jm}| \\
                                       &= \max_{1\le j \le n} \sum_{i=1}^n |a_{ji}|,
\end{aligned}$$
  
which is exactly the maximum over the $\mathcal{l}^1$-norms of the rows in $A$. 

```

```{exercise, q213}
Assume the Fundamental Axiom. Show the following:
  
$$\left|\left| fl(A)-A \right|\right|_p \leq u \left|\left|A\right|\right|_p$$
```

```{solution, s213}
\begin{align*}
  \left | \left | fl(A) - A \right | \right |_p &= \left|\left| \left [ fl(a_{ij}) - a_{ij} \right ] \right|\right|_p \\
    &\leq \left|\left| \left[u\cdot a_{ij}\right]\right|\right|_p \\
    &= \left|\left| u\cdot A\right|\right|_p = u\left|\left|A\right|\right|
\end{align*}
```

## Homework 3

```{exercise, q301}
Prove lemma \@ref(lem:proofHW). 
```

```{solution}
Recall that a matrix $A$ is invertible if and only if $Ax = 0$ implies that $x = 0$. So to check that $I-F$ is invertible, we check this:
  
$$
  (I-F)x = x-Fx = 0 \Rightarrow x = Fx \Rightarrow \norm{x}_p \leq \norm{F}_p \norm{x}_p.
$$
  
Since $\norm{F}_p < 1$ by assumption, the only solution to the inequality above is $x = 0$. So, $I-F$ is invertible. 


Note that $\sum_{k=0}^N F^k (I - F) = I - F^{N+1}$. Since $\norm{F^k}_p < \norm{F}_p^k < 1$, we know that $F^N \rightarrow 0$ as $N \rightarrow \infty$. So, $\sum_{k=0}^\infty F^k (I-F) = I$, and using that $I-F$ is invertible, we get $(I-F)^{-1} = \sum_{k=0}^\infty F^k$. Finally,

$$\norm{(I-F)^{-1}}_p \leq \sum_{k=1}^\infty \norm{F}_p^k = \frac{1}{1- \norm{F}_p}$$. 

```

```{exercise, q302}
Consider Theorem and Lemmas under "Square Linear Systems" (\@ref(square-linear-systems)). What happens if we use $l^{1}$-norm instead?
```

```{solution}
Since we never use any properties of the infinity norm to prove these theorems and lemmas, we could replace it with the $\mathcal{l}^1$-norm. 
```


```{exercise, q303}
Generate examples that show the bound in theorem \@ref(thm:kappa-bound) is too conservative.
```

```{julia}
using LinearAlgebra

p = 53 # precision for float-64
u = 2.0^(-p+1)
A = [
    0. 0 0.000001;
    200000 0 0;
    0 1 20000;
]
x = [1.; 1; 1;]
b = A*x

kappa = norm(A, Inf) * norm(inv(A), Inf)
bound = 2.0*u*kappa/(1.0 - u*kappa)

println("Diff: $(norm(x - inv(A)*b, Inf)/norm(x,Inf))")
println("Bound: $(bound)")
```

```{exercise, q304}
Generate examples that show the bound is nearly achieved
```

```{julia}
using LinearAlgebra

A = [
    0. 0 1.;
    1 0 0;
    0 1 0;
]
x = [1.; 1; 1;]
b = A*x

kappa = norm(A, Inf) * norm(inv(A), Inf)
bound = 2.0*2.0^(-p+1)*kappa/(1.0 - 2.0^(-p+1)*kappa)

println("Diff: $(norm(x - inv(A)*b, Inf)/norm(x,Inf))")
println("Bound: $(bound)")
```

```{exercise, q305}
For motivating problems 1-5, when is $x$ unique?
```

```{solution}

- Motivating Problem 1:
    * when $A$ is invertible
- Motivating Problem 2: Always. 
    * Since $F(y) = \norm{Ay - b}_2 = (Ay-b)^\prime (Ay-b)$ is convex (twice differentiated is positive definite because $\nabla^2 f = A^\prime A$ and $A$ has full rank).
    * (since objective function is $A^\prime A$, and it is positive definite (because A is full rank), then the function is convex, and you always have a unique solution)
- Motivating Problem 3: Always.
- Motivating Problem 4: Always. 
    * We know how to characterize all $z$ that satisfy objective: $Q\begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix} \Pi^\prime = A$.

- Motivating Problem 5: always

```

```{exercise, q306}
For motivating problem 5, what happens if $p\geq m$? Explore the case where $m >> n$.
```

```{solution, s306}
* For m >> n, you have an undertermined system with more unknowns than equations. Since the null space of A has a dimension larger than zero, for any particular solution xp for the system, xp+h with $h \in \text{null}(A)$ is also a solution, and there are infinitely many choices for h. The constraint system might help narrow down the solutions from the null space.
* For p >= m: if C becomes inconsistent, we cannot narrow down the solutions in the null space. If C is consistent and rank(C)=m, then we can pick a unique solution. If the rank of C is less than m, then C constrains some of the possible solutions for y.
```

```{exercise, q307}
What do you get if you multiply a matrix by a permutation matrix from the left? From the right? A permutation with itself?
```

```{solution}
From the left: permute rows.

From the right: permute columns.

Permutation squared gives you the identity.
```

```{exercise, q308}
Suppose $R \in \R^{m\times m}$ is an upper triangular matrix with $R_{ii} \neq 0$ for all $i = 1,\ldots, n$. Is $R$ invertible?
```

```{solution}
Since $R$ is an upper triangular matrix, $\det(R) = \prod_{i=1}^{m} R_{ii} > 0$. Hence, $R$ is invertible. 
```

```{exercise, q309}
Assume $R$ is an invertible upper triangular matrix. Implement a solution to invert $R$. 
```

```{solution, s310}
First, we will show that the inverse of $R$ is also an upper triangular matrix. So, let $B = R^{-1}$. The $RB = I$. We will show this using induction. Let $i = n - 0, j < i$. Then, since $r_{ij} = 0$ for all $i > j$,

$$\begin{aligned}
  0 &= \sum_{k=1}^n r_{n,k}b_{k,j} \\
    &= r_{n,n}b_{n,j}.
\end{aligned}$$
    
Since $R$ is invertible, $r_{n,n} \neq 0$, hence $b_{n,j} = 0$ for all $j < n$. 
  

Now assume that $b_{i,j} = 0$ for all $i=n-0,n-1, \ldots, n-m, j < i$. We then want to show it holds for $i = n - (m+1) = n-m-1$. Let $j < n-(m+1)$. Then

$$\begin{aligned}
  0 &= \sum_{k=1}^n r_{n-(m+1), k} b_{k, j} \\
    &= \sum_{k=n-(m+1)}^{n} r_{n-(m+1), k} b_{k, j} \\
    &= r_{n-(m+1), n-(m+1)} b_{n-(m+1),j},
\end{aligned}$$
    
where the first equality is due to the fact that $r_{ij} = 0$ for all $i > j$, and the last equality holds since $b_{k,j} = 0$ for all $j < k$ when $k \ge n-m$ (per the induction hypothesis). Since $r_{ii} \neq 0$ for all $i$, $b_{n-(m+1),j} = 0$. 
  
So, $b_{ij} = 0$ for all $i > j$. 

Now, lets look at the case where $i=j$, i.e. diagonal elements of the inverse matrix. Then

$$\begin{aligned}
  1 = \sum_{k=1}^n r_{ik} b_{ki} = \sum_{k=i}^{n} r_{ik} b_{ki} = r_{ii}b_{ii}.
\end{aligned}$$

The second equality holds since $r_{ik} = 0$ for all $i > k$, the last equality holds because $b_{ki} = 0$ for all $k > i$. We see that $b_{ii} = r_{ii}^{-1}$.
  
Finally, consider the case where $i < j$. Then, using that $r_{ik} = 0$ for all $i>k$ and $b_{kj} = 0$ for all $k>j$, we see that 

$$
  0 = \sum_{k=1}^n r_{ik}b_{kj} = \sum_{k=i}^j r_{ik} b_{kj},
$$
  
which implies

$$
  b_{ij} = \frac{-\sum_{k=i+1}^j r_{ik} b_{kj}}{a_{ii}}.
$$
  
So, in other words, given $i,j$, we can find $b_{ij}$ if we know $b_{kj}$ for all $k>i$ (i.e. all entries in the same column below the entry we are considering). Since we already know all entries below and on the diagonal, this is true for all columns. Hence, we can construct $B$ this way. 

See the `Julia` chunk below for an implementation of this.

```

```{julia}
function invertUpperTri(A)
    ## Get dimensions of A
    n, m = size(A)

    ## Setup empty array to hold result
    B = zeros(n,m)

    ## Fill out diagonal with inverse diagonal from A
    for k = 1:n
        B[k, k] = 1/A[k,k]
    end

    ## Starting in the lower right corner, fill out the rest of the matrix.
    for i = (n-1):-1:1
        for j = (i+1):n
            B[i,j] = -sum(A[i,(i+1):j].*B[(i+1):j,j])/A[i,i]
        end
    end

    return(B)
end

## Create an upper triangular matrix to check
A = rand(4,4);
A[2:4,1] = [0 0 0];
A[3:4,2] = [0 0];
A[4,3] = 0;

## Check function
B = invertUpperTri(A);
B - inv(A)  # should be 0 matrix
A*B         # should be identity
```

## Homework 4

```{exercise, q401}
In the solution to \@ref(exm:linear-system), why do $0$ rows on the left-hand side of \@ref(eq:sol-linear-system) correspond to $0$ entries of the $c$ vector on the right-hand side. 
```

```{solution, s401}
By definition of matrix multiplication, if the $i$th row is a zero row of $A$, then $c_{ij} = [AB]_{ij} = \sum{j=1}^n a_{ij} b_{ij} = 0$, no matter what $B$ is. 
```

```{exercise, q402}
Let $Q$ be an orthogonal matrix. Show that $\norm{Qx}_2 = \norm{x}_2$. 
```

```{solution, s402}

Since $\norm{x}_2^2 = x^\prime x$ and $Q^\prime Q = 1$ ($Q$ is orthogonal), 

$$ 
  \norm{Qx}^2_2 = (Qx)^\prime Qx = x^\prime Q^\prime Q x = x^\prime x = \norm{x}^2_2.
$$

```

```{exercise, q403}
Let f be a vector-valued function over $\R^d$. When is $$\argmin_x \norm{f(x)}_2 = \argmin_x \norm{f(x)}_2^2.$$
```

```{solution, s403}
Most of the time...?
```


```{exercise, q404}
Prove that $P^T P + I$ from solution to example \@ref(exm:und-linear-system) is invertible. 
```

```{solution, s404}

For all non-zero $x$,

$$
  x^\prime(P^TP + I)x = x^\prime P^\prime P x + x^\prime x = \norm{Px}_2 + \norm{x}_2 > 0.
$$
  
This means $P^\prime P + I$ is positive definite, which in turne implies that it is invertible. 
  
```

```{exercise, q405}
Write out the solution to example \@ref(exm:constrained-least-squares). Also consider the case where $p \ge m$.
```

```{solution, s405}
First we do QR decomposition on $C^\prime$:

$$\begin{aligned}
C^\prime = Q\begin{bmatrix} R \\ 0 \end{bmatrix}
\end{aligned}$$

Then we have:

$$\begin{aligned}
AQ &= \begin{bmatrix} A_1 & A_2 \end{bmatrix} \\
Q^\prime y &= \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \\
\end{aligned}$$

And we can update the objective:

$$\begin{aligned}
O &= \min \|Ay-b\|_2 : Cy = d\\
&= \min \|AQQ^\prime y -b\|_2 : Cy = d\\
&= \min \left\| \begin{bmatrix} A_1 & A_2 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} - b \right\| _2 : Cy = d\\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : Cy = d\\
\end{aligned}$$

We can also update the constraint:

$$\begin{aligned}
O &= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : \begin{bmatrix} R^\prime & 0 \end{bmatrix} Q^\prime y = d \\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : \begin{bmatrix} R^\prime & 0 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = d \\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : R^\prime y_1 = d \\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : y_1 = (R^\prime)^{-1}d  \\
&= \min \left\| A_1(R^\prime)^{-1}d + A_2 y_2 - b \right\|_2\\
&= \min \left\| A_2 y_2 - (b - A_1(R^\prime)^{-1}d)\right\|_2\\
\end{aligned}$$

So $y_1$ can be calculated using a consistent linear system solver, and now the objective is the same as that of a least squares solver, which can be used to calculate $y_2$. We can finally recover $y$:

$$\begin{aligned}
y &= Q \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \\
\end{aligned}$$

```

```{exercise, q406}
For all motivating problems, implement solutions. 
```

</br>

For the following, assume $A \in \R^{n\times m}$ with $\text{rank}(A) = m$, and $b \in \R^n$. Let $C = \begin{bmatrix} A & b \end{bmatrix}$. 


```{exercise, q407}
What does the last column of $R$ (from the QR decomposition of $C$) represent?
```

```{solution, s407}
Generally, $Q^\prime b$, which is the projection of b onto the column space of A. If $b$ is in the column space of $A$, then it is specifically $Rx$ where $x$ is the solution to $Ax = b$. 
```

```{exercise, q408}
What does the last entry of last column of $R$ (from the QR decomposition of $C$) represent?
```

```{solution, s408}
The square of the last entry of the last column of $R$ is the sum of squares of the residuals.
```

```{exercise, q409}
How can this be used in computation?
```

```{exercise, s409}
We can use it for incremental QR for large datasets.
```

## Homework 5

```{exercise, q501}
Implement the Gram-Schmidt procedure for matrices $A \in \R^{n \times m}$ assuming $A$ has full column rank.
  
Create examples to show that the function works (well enough).
```

```{solution, s501}
See [here](../Homework/gram_schmidt.jl).
```

```{exercise, q502}
Find examples where Gram-Schmidt fails, i.e. where either $Q R \neq A$ or $Q^TQ \neq I$. 
```

```{solution, s502}
Let $A = \begin{bmatrix} 1 & 1 & 1 \\ 10^{-8} & 0 & 0 \\ 0 & 10^{-8} & 0 \end{bmatrix}$. Then $Q^\prime Q \neq I$.
```

```{exercise, q503}
Look up the modified Gram-Schmidt Procedure and implement it (again assuming $A$ has full column rank). 
```

```{solution, s503}
See [here](../Homework/modified_gs.jl).
```


```{exercise, q504, name = "Pivoting (*OPTIONAL*)"}
References:
  
  1. Businger, Galub: Linear Least Squares by Householder Transformation (1965)
  2. Engler: The Behavior of QR-factorization algorithm with column pivoting (1997)
  
Implement modified Gram-Schmidt with column pivoting. 

Find example where the modified Gram-Schmidt fails, but the modified Gram-Schmidt with column pivoting does not.
  
```


```{exercise, q505}
Show that Householder reflections are orthogonal matrices.
```

```{solution}
Show that $H^\prime H = I$. By definition of a Householder matrix (\@ref(def:householder)), $H = I - 2vv^\prime$ for a $v$ with $\norm{v}_2 = 1$. Note that $\norm{v}_2 = \sqrt{v^\prime \cdot v}$. 

So,

$$\begin{aligned}
  H^\prime H &= (I - 2vv^\prime)^\prime (I - 2vv^\prime) \\
             &= (I^\prime - 2(vv^\prime)^\prime)(I - 2 vv^\prime)\\
             &= (I - 2vv^\prime)(I - 2 vv^\prime) \\
             &= I - 2vv^\prime - 2vv^\prime + 4 vv^\prime v v^\prime \\
             &= I - 2vv^\prime - 2vv^\prime + 4 v I v^\prime \\
             &= I.
\end{aligned}$$  
    
So by definition (\@ref(def:orthogonal)), $H$ is an orthogonal matrix.
```

```{exercise, q506}
Show that if $H_1, \dots, H_r$ are Householder reflections, then $H_r \cdots H_1$ (i.e. the product) is orthogonal.
```

```{solution, s506}

$$(H_r \cdots H_1)^\prime (H_r \cdots H_1) = H_1^\prime \cdots H_r^\prime H_r \cdots H_1 = I,$$
  
since all $H_i$ are Householder reflections, which implies they are orthogonal, which implies $H_i^\prime H_i = I$. 

```


```{exercise, q507}
Show that a Givens rotation is an orthonormal matrix when $\sigma^2 + \lambda^2 = 1$. 
```

```{solution, s507}

Let $G^{(a,b)}$ be a Givens rotation. I.e. the elements $g_{i,j}$ are 

$$
  g_{i,j} = 
  \left\{ 
    \begin{array}{rl} 
      1, & i = j \notin \{a,b\} \\
      \lambda, & i = j \in \{a,b\} \\
      \sigma, & i = a, j = b \\
      -\sigma, & i = b, j = a \\
      0, & \text{ otherwise}
    \end{array} 
  \right.
$$
        
Transposing this matrix gives us a new matrix $K$ where
      
$$
  k_{i,j} = 
  \left\{ 
    \begin{array}{rl} 
      1, & i = j \notin \{a,b\} \\
      \lambda, & i = j \in \{a,b\} \\
      -\sigma, & i = a, j = b \\
      \sigma, & i = b, j = a \\
      0, & \text{ otherwise}
    \end{array} 
  \right.
$$

Now, let $L = K \cdot G$. Then the elements of $L$ are $l_{i,j} = \sum_{s=1}^n k_{i,s} g_{s,j}$. 
      
If $i \neq a,\ j \neq b$, and $i \neq j$, then $l_{i,j} = k_{i,i} g_{i,j} = 0$ (since $g_{i,j} = 0$). 

If $i \notin \{a, b\}$, then $l_{i,i} = k_{i,i} g_{i,i} = 1$ (since $i \notin \{a,b\}$).

If $j \notin \{a,b\}$, then $l_{a,j} = k_{a,b}g_{b,j} + k_{a,a}g_{a,j} = 0$ (since $j \notin \{a,b\}$ implies $g_{b,j} = 0$ and $g_{a,j} = 0$).

Furthermore, 

* $l_{a,b} = k_{a,b}g_{b,b} + k_{a,a}g_{a,b} = -\sigma \lambda + \lambda \sigma = 0$,
* $l_{a,a} = k_{a,b} g_{b,a} + k_{a,a} g_{a,a} = (-\sigma)(-\sigma) + \lambda \lambda = \sigma^2 + \lambda^2$,
* $l_{b,b} = k_{a,b} g_{b,a} + k_{a,a} g_{a,a} = (-\sigma)(-\sigma) + \lambda \lambda = \sigma^2 + \lambda^2$.

So,
      
$$
  l_{i,j} = 
  \left\{ 
    \begin{array}{rl} 
      1, & i = j \notin \{a,b\} \\
      \sigma^2 + \lambda^2, & i = j \in \{a,b\} \\
      0, & \text{ otherwise}
    \end{array} 
  \right.
$$
        
Similar calculations can be performed for $G \cdot K$. So $G^{(a,b)}$ is orthogonal if and only if $\sigma^2 + \lambda^2 = 1$. 

```

## Homework 6

```{exercise, q601}
Determine the computational complexity of the QR decomposition using 

a) Gram-Schmidt
b) Modified Gram-Schmidt
c) Householder
d) Givens rotations

for any arbitrary, dense $n \times m$ matrix. (dense = don't know how many entries are $0$.)

```

```{solution, s601}

a) Gram-Schmidt requires $O(m^2n)$ computations.
b) Same as Gram-Schmidt. 
c) $O(m^2n)$.
d) $O(m^2n)$.


```

```{exercise, q602}
Compare the computational complexity of Householder and Givens for a sparse matrix (i.e. a matrix where a substantial number of entries are $0$).
```

```{solution, s602}
Householder still have the same complexity even for sparse matrices, whereas for Givens we do not have to complete as many multiplications with Givens rotations. (Everytime we encounter a 0 in the lower triangular matrix that can be paired with a zero above it, we can pair them up, and skip the computation.)
```

```{exercise, q603}
Implement QR decomposition using Householder. Write it as a function that takes a matrix $A \in \R^{n\times m}$ with $\text{rank}(A) = m$ as its input, and gives back $Q$ and $R$.
```

```{solution, s603}
See [here](../Homework/QR_householder.jl).
```

```{exercise, q604}
Implement QR decomposition using Givens. (As above.)
```

```{solution, s604}
See [here](../Homework/QR_givens.jl)
```

```{exercise, q605}
What happens in theorem \@ref(thm:svd) if $m > n$.
```

```{solution}

If $m > n$, the diagonal matrix of the SVD of $A$ will be of the form $\begin{bmatrix} \Sigma & 0 \end{bmatrix}$, where $\Sigma = \begin{bmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_n \end{bmatrix}$. 

```

```{exercise, q606}
If $u,v$ are respectively left and right singular vectors corresponding to the singular value $\sigma$, show that $Av = \sigma u$, and $A'u = \sigma v$. 
```

```{solution, s606}
Using the SVD of $A$, we see that $AV = U\Sigma$, which implies 

$$
  A \begin{bmatrix} v_1 & \cdots & v_m \end{bmatrix} = \begin{bmatrix} \sigma_1 u_1 & \cdots & \sigma_m u_m \end{bmatrix}.
$$
  
where $v_1, \dots, v_m$ are the right singular vectors of $A$, and $u_1, \dots, u_m$ are the left singular vectors of $A$. 

So, $Av_i = \sigma_i u_i$ for all $i$. 

Similarly, $A^\prime = V \Sigma U^\prime$ implies $A^\prime U = V \Sigma$. As above, this gives us $A^\prime u_i = \sigma_i v_i$.

```


## Homework 7

```{exercise, q701}
Show that $\norm{A}_F^2 = \sum_{i=1}^n\sum_{j=1}^n a_{ij}^2$ is orthogonally invariant, i.e. if $Q,P$ are orthogonal matrices, then $\norm{QA}_F^2 = \norm{A}_F^2$ and $\norm{AP}_F^2 = \norm{A}_F^2$. 
```

```{solution, s701}

Recall, that $\norm{A}_F^2 = \text{trace}(A^\prime A)$. So,

$$
  \norm{QA}_F^2 = \text{trace}(A^\prime Q^\prime Q A) = \text{trace}(A^\prime A) = \norm{A}_F^2,
$$

and 

$$
  \norm{AP}_F^2 = \text{trace}(P^\prime A^\prime A P) = \text{trace}(A^\prime A P P^\prime) = \text{trace}(A^\prime A) = \norm{A}_F^2.
$$

```

```{exercise, q702}
Proof corollary \@ref(cor:sigma-min-max).
```

```{solution, s702}
Recall that $\sigma_{\max} (A) = \norm{A}_2$. Hence, $\sigma_{\max}(A+E) = \norm{A+E}_2 \leq \norm{A}_2 + \norm{E}_2 = \sigma_{\max}(A) + \norm{E}_2$. 

For the second inequality, recall that $\sigma_{\min}(A) = \min_{x \neq 0} \frac{\norm{Ax}}{\norm{x}}$. First, consider the case where $\sigma_{\min}(A) = 0$. Then the result holds. Second, consider the case where $\sigma_{\min}(A) > 0$. Then

$$\begin{aligned}
  \sigma_{\min}(A+E) &= \min_{v \neq 0} \frac{\norm{(A+E)v}_2}{\norm{v}_2} \ge \min_{v \ne 0} \frac{\norm{Av}_2}{\norm{v}_2} - \max_{v \ne 0} \frac{\norm{Ev}_2}{\norm{v}_2} \\
  &= \sigma_{\min}(A) - \norm{E}_2,
\end{aligned}$$
  
where we use the inverse triangle equality. 

```

```{exercise, q703}
Proof corollary \@ref(cor:for-q703).
```

```{solution, name = "1"}

Let $b \in \text{range}(A)$. Then there exists some $z$ such that $Az = b$. Use the SVD of A to get 

$$
  b = U\Sigma V^\prime z.
$$
  
Since $\text{rank}(A) = r$, the last $n-r$ rows of $\Sigma$ are all $0$, so $U \Sigma V^\prime = U_r \Sigma_r (V_r)^\prime$, where $U_r$ is the first $r$ columns of $U$, $\Sigma_r$ the first $r$ columns and $r$ rows of $\Sigma$, and $V_r$ the first $r$ columns of $V$. Hence, $\Sigma_r (V_r)^\prime z = y \in \R^r$. So, $b = U_r y$, which means it is in the span of the first $r$ columns of $U$. Hence, $\text{range}(A) \subset \text{span}\{u_1, \dots, u_r\}$. 

Now, let $b \in \text{span}\{u_1, \dots, u_r\}$. Then $b = U_r y$ for some $y \in \R^r$. Recall that $U = A V \Sigma^{-1}$, hence $U_r = A V_r \Sigma_r^{-1}$. So, $b = A V \Sigma_r^{-1} y$. Since $V \Sigma_r^{-1} y = z \in \R^m$, $b$ is in the range of $A$. Hence, $\text{span}\{u_1, \dots, u_r\} \subset \text{range}(A)$.

```

```{solution, name = "2"}
Note that the row space of $A$ is the range of $A^\prime$. Solve as above. 
```

```{solution, name = "3"}

Let $b \in \text{span}\{v_{r+1}, \dots, v_{m}\}$. Then $b = x_{r+1} v_{r+1} + \cdots + x_m v_{r+1}$ for some coefficients $x_j \in \R$. Then 

$$\begin{aligned}
Ab  &= U \Sigma V^\prime b \\ 
    &= U \Sigma (x_{r+1} V^\prime v_{r+1} + \cdots + x_{m} V^\prime v_m) \\
    &= U \Sigma \begin{bmatrix} 0 \\ \vdots  \\ 0 \\ x_{r+1} \\ \vdots \\ x_{m} \end{bmatrix},
\end{aligned}$$

since all columns of $V$ are orthogonal. 

Since the last $n-r$ rows of $\Sigma$ are all $0$ rows, $\Sigma V^\prime b = 0$, so $Ab = 0$, so $b \in \text{null}(A)$. Hence, $\text{span}\{v_{r+1}, \ldots, v_{m}\} \subset \text{null}(A)$.

Now, Assume $b \in$ null($A$), then $Ab = 0$. Because $b \in R^m$ and $v_1, \cdots, v_m$ is a base for $R^m$, there exists a sequence $\{x_i\}$ s.t.  $b = x_1v_1 + \cdots + x_rv_r + x_{r+1}v_{r+1} + \cdots + x_{m}v_m$.

Assume for contradiction that $b \notin$ span($v_{r+1}, \cdots, v_m$), i.e. that $x_1, \cdots, x_r$ are not all zero. Then

$$Ab= \sum_{i=1}^{r} \sigma_i u_i v_i^T b = \sum_{i=1}^{r}[ \sigma_i u_i v_i^T (\sum_{j=1}^{m}x_jv_j)]= 
\sum_{i=1}^{r}[ \sigma_i u_i (\sum_{j=1}^{m}x_jv_i^Tv_j)] = \sum_{i=1}^{r} \sigma_i u_i x_i
\neq 0$$

But this contradicts the assumption that $b \in \text{null}(A)$. So, $b$ must be in $\text{span}\{v_{r+1}, \cdots, v_m\}$. Hence, $\text{null}(A) \subset \text{span}\{v_{r+1}, \cdots, v_m\}$.


```

## Homework 8

```{exercise, q801}
What is the solution to $\min_{x} \norm{Ax-b}_2^2$ in terms of the Moore-Penrose inverse? (pseudo-inverse)
```

```{solution, s801}
To find the solution, we differentiate and set equal to $0$. So, differentiate $g(x) = x^\prime A^\prime A x - x^\prime A b - b^\prime A x + b^\prime b$ and set to $0$:

$$\frac{dg}{dx} = x^\prime (A^\prime A + A^\prime A) - b^\prime A - (A^\prime b)^\prime = 2x^\prime (A^\prime A) - 2b^\prime A = 0$$
  
which implies $(A A^\prime)x = b A^\prime$. Check that $x = A^+b$ satisfies this. 

```

```{exercise, q802}
Let $Q$ be an orthogonal matrix with columns $q_1, \ldots, q_n$. Let $Z \in \R^{n \times n}$ be a symmetric matrix. Show that $\sum_{i=1}^n (q_i^\prime Z q_i)^2 \leq \norm{Z}_F^2$.
```

```{solution, s802}

Since the frobenius norm is invariant to mulitplication by orthogonal matrices: $\norm{Z}_F^2 = \norm{Q^\prime Z Q}_F^2$. Since 

$$\begin{aligned}
   \begin{bmatrix} q_1^\prime & \cdots q_n^\prime \end{bmatrix} Z \begin{bmatrix} q_1 \\ \vdots q_n \end{bmatrix} &= 
          \begin{bmatrix} q_1^\prime & \cdots q_n^\prime \end{bmatrix} \begin{bmatrix} Z q_1 \\ \vdots Z q_n \end{bmatrix} \\
  &= \begin{bmatrix}  q_1^\prime Z q_1 & \cdots & q_1^\prime Z q_n \\
                      \vdots & \ddots & \vdots \\
                      q_n^\prime Z q_1 & \cdots & q_n^\prime Z q_n
\end{bmatrix},
\end{aligned}$$

we have that $\norm{Z}_F^2 = \sum_{i=1}^n \sum_{j=1}^n (q_i^\prime Z q_j)^2 \ge \sum_{i=1}^n (q_i^\prime Z q_i)^2$.

```

## Homework 9

```{exercise, q901}
What do we need from $A$ to ensure that the different iterative method schemes are well-defined?
```

```{solution, s901}
A must be square and diagonal entries must be non-zero.
```

```{exercise, q902}
Does lemma \@ref(lem:next-current-diff) hold for the symmetric SOR?
```

```{exercise, q903}
Compute $J^k$ where $J = I\lambda + E$ (the Jordan Canonical Form from definition \@ref(def:jordan))

Show that if $G = XJX^{-1}$ and $\rho(G) < 1$, then $J^k \rightarrow 0$ as $k\rightarrow \infty$.

```

## Homework 10

```{exercise, q1001}
Implement Jacobi, Gauss-Seidel without using the `\` operator.
```

```{solution, s1001}
See [Jacobi]("../Homework/jacobi.jl") and [Gauss-Seidel]("../Homework/gauss_seidel.jl").
```

```{exercise, q1002}
Implement SOR, SSOR without using the `\` operator.
```

```{solution, s1002}
See [SOR]("../Homework/SOR.jl").
```

```{exercise, q1003}
Randomly generate problems and 

a) at each iteration, record residual norm and absolute error
b) compare rate of convergence against spectral radious
c) put all this information into a narrative using graphics

```

```{exercise, q1004}
Prove that observation 1 from the proof of \@ref(thm:rand-kaczmarz) is true.
```

```{exercise, q1005}
Implement Random Kaczmarz for random permutations.
```

```{exercise, q1006}
Give a detailed comparison of cycle, randomized, and random permutation Kaczmarz.
```


## Homework 11

```{exercise, q1101}

Show that $\alpha \sum_{i=1}^n a_i (b_i - a_i x^c) = \alpha A^\prime (b - Ax^c)$.

```

```{exercise, q1102}
Show that if $A^\prime r^c = 0$, then $x^c = x^*$, where $Ax^* = b$. 
```

```{exercise, q1103}
Why is it enough to find an upper bound on $\norm{\Sigma^{-1}u}_2^2 \norm{\Sigma u}_2^2$ for any unit vector $w$ in the proof of theorem \@ref(thm:grad-descent)? 
```

```{exercise, q1104, name = "Kontorovich's Inequality"}
Let $0 < u_n \le u_{n-1} \le \cdots \le u_1$. Then

$$\left(\sum_{i=1}^n p_i u_i\right)\left(\sum_{i=1}^n \frac{p_i}{u_i}\right) \leq \frac{(u_1 + u_n)^2}{4u_1u_n}$$

```

```{exercise, q1105}
For strategy 2, answer the following questions:
  
a) What is $\alpha$? Is it practical?
b) With this $\alpha$, will we converge? If so, what is the rate of convergence?

```

```{exercise, q1106}
For strategy 3, what is $\alpha$?
```

```{exercise, q1107}
For strategy 4, show the inequality holds.
```

```{exercise, q1108}
For strategy 4, what is $\alpha$?
```

```{exercise, q1109}
Implement a single update of the gradient descent method for a user supplied alpha, assuming that A is invertible.
```

```{exercise, q1110}
Now, implement four algorithms one for each of the four strategies of alpha discussed in class.
```

```{exercise, q1111}
Generate several test problems for your four algorithms, and compare their performance.
```

```{exercise, q1112}
In a narrative, explain which algorithm you would use and when you would use this algorithm.
```

**Understanding Gradient Descent's Dependence on the Condition Number**

```{exercise, q1113}
Write a function to generate 2 equations with 2 unknowns such that the coefficient matrix is dense and symmetric with user specified nonzero eigenvalues.
```

```{exercise, q1114}
How are the eigenvalues related to the singular values?
```

```{exercise, q1115}
How are the eigenvectors related to the left and right singular vectors?
```

```{exercise, q1116}
Write a function to draw the level sets of an arbitrary residual function $f\left(x\right) = \norm{Ax - b}_2^2$.
```

```{exercise, q1117}
Run Gradient Descent using the step size (alpha) from Strategy 2 to solve a sequence of problems where the difference between the singular values of your problem increases in size. Plot the points that Gradient Descent visits (on your level set plot) as it finds its way to the solution.
```

```{exercise, q1118}
What do you observe? What is the impact of the singular values on gradient descent? Why does this make sense based on your graphs?
```