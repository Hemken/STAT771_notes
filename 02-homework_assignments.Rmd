# Homework Assignments

## Homework 1

```{exercise, q101} 
Can all nonnegative real numbers be represented in such a manner (i.e. as a fp number) for an arbitrary base $\beta \in \{2,3,...\}$? 
```

```{solution}
No. For any given $\beta$ and a largest exponent $e_{max}$, any decimal larger than $\beta\cdot\beta^{e_{max}}$ is larger than the largest number possibly representated. 
```


```{exercise, q102}
Suppose $e = -1$, what are the range of numbers that can be represented for an arbitrary base $\beta \in \{2,3,...\}$?
```

```{solution}
The smallest number that can be represented for an arbitrary base must be $(0+0\cdot \beta^{-1} + ... + 0\cdot\beta^{-(p-1)})\cdot\beta^{-1}$. 

Since $0 \leq d_i < \beta, \forall i$, the largest value must be attained when $d_i = \beta - 1$ for all $i$. I.e. the largest value must be

\begin{align*}
  MAX &= (\beta-1 + (\beta-1)\beta^{-1} + ... + (\beta-1)\beta^{-(p-1)})\cdot\beta^{-1}\\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})(\beta-1)\cdot\beta^{-1} \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1}) \\
      &= (1+\beta^{-1}+...+\beta^{-(p-1)})\cdot(1-\beta^{-1})
\end{align*}
```

```{exercise, q103}
Characterize the numbers that have a unique representation in a base $\beta \in \{2,3,...\}$.
```

```{solution}
Let 

$$f = \left(d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)}\right)\cdot\beta^{e},$$ 

i.e. $f$ is not normarlized. Then, 

$$
f = \left(d_1+d_2\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-p} + 0\cdot \beta^{-(p-1)}\right)\cdot\beta^{e-1}.
$$

So, non-normalized fp numbers are NOT unique. 

Now, let $f$ be a normalized fp number. I.e.

$$
f = \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^e,
$$
where $d_0 \neq 0$. If we let $e_n < e$, then 

$$
f > \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n},
$$

and if $e_n > e$, then

$$
f < \left(d_0 + d_1\cdot\beta^{-1} + \ldots + d_{p-1}\cdot\beta^{-(p-1)} \right)\cdot\beta^{e_n}
$$

If we let $$d_i^\prime \neq d_i$$ for some number of $i$'s, then

$$
f \neq \left(d_0^\prime + d_1\prime\cdot\beta^{-1} + \ldots + d_{p-1}\prime\cdot\beta^{-(p-1)} \right)\cdot\beta^e.
$$

Hence, normalized FP numbers are unique.
```

```{exercise, q104}
Write a function that takes a decimal number, base, and precision, and returns the closest normalized FP representation. I.e. a vector of digits and the exponent.
```

```{solution}
The function provided in class is actually the solution (?). This is guarenteed to give a normalized FP representation. Using this algorithm gives $d_0 = \lfloor \frac{N}{\beta^{\lfloor log_{\beta}\left(N\right) \rfloor}}\rfloor$. It holds that $\lfloor log_{\beta}\left(N\right) \rfloor \le log_\beta\left(N\right)$, which implies that $\beta^{\lfloor log_{\beta}\left(N\right) \rfloor} \le \beta^{log_\beta\left(N\right)} = N$ (remember, $\beta \geq 2$). Hence, $d_0 > 0$.
```

```{julia, eval = FALSE}
get_normalized_FP = function(number::Float64, base::Int64, prec::Int64)
    #number = 4; base = float(10); prec = 2
    si=sign(number)
    base = float(base)
    e = floor(Int64,log(base,abs(number)))
    d = zeros(Int64,prec)
    num = abs(number)/(base^e)
    
    for j = 1:prec
        d[j] = floor(Int64,num)
        num = (num - d[j])*base
    end

    return "The sign is $si, the exponent is $e, and the vector with d is $d"
end
```

```{exercise, q105}
List all normalized fp numbers that can be representated given base, precision, $e_{min}$, and $e_{max}$. 
```

```{julia, eval FALSE}
all_normalized_fp = function(base::Int64, prec::Int64, emin::Int64, emax::Int64)
    ## Number of possible values for each e:
    N = (base-1)*base^(prec-1)#*(emax-emin+1)

    out=zeros(Int64, N, prec, emax-emin+1)

    es = emin:emax


    for e=1:length(es)
        for b0=1:(base-1)
            for i=1:(base^(prec-1))
                out[(b0-1)*(base^(prec-1))+i,1,e] = b0
                for j=1:(prec-1)
                    out[(b0-1)*(base^(prec-1))+i,prec-j+1,e] = floor((i-1)/base^(j-1))%base
                end
            end
        end
    end

    return(out)
end

```


## Homework 2

```{exercise, q201}
Lookup the 64 bit standard to find allowed exponents.
```

```{solution, s201}
According to [Wikipedia](https://en.wikipedia.org/wiki/Double-precision_floating-point_format), the allowed exponents for the 64 bit standard are $-1022,\ldots, 1023$. 
```

```{exercise, q202}
What is the smallest non-normalized positive value for the 64 bit standard?
```

```{solution, s202}
The smallest non-normalized positive value is 

$$
  \left(0 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-51} + 1\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1074} \approx 4.94\cdot 10^{-324}
$$
```

```{exercise, q203}
What is the smallest normalized positive value?
```

```{solution, s203}
The smallest normalized positive value is 

$$
  \left(1 + 0\cdot 2^{-1} + \ldots + 0\cdot 2^{-52} \right )\cdot 2^{-1022} = 2^{-1022} \approx 2.23 \cdot 10^{-308}
$$
```

```{exercise, q204}
What is the largest normalized positive value?
```

```{solution, s204}
The largest normalized finite value is

$$
  \left(1 + 1\cdot 2^{-1} + \ldots + 1\cdot 2^{-52} \right )\cdot 2^{1023} \approx 1.80\cdot 10^{308}.
$$
```

```{exercise, q205}
Is there a general formula for determining the largest positive value for a given base $\beta$, precision $p$, and largest exponent $e_{max}$? 
```

```{solution, s205}
The largest positive, finite value is 

$$
  \left(\sum_{i=0}^{p-1} (\beta-1)\beta^{-i}\right)\cdot \beta^{e_{max}} = ... = \frac{\beta^p - 1}{\beta^{p-1}}\beta^{e_max}.
$$
```

```{exercise, q206}
Verify the smallest non-normalized, positive number that can be represented.
```

```{solution}
See the `Julia` chunk below.
```

```{julia}
nextfloat(Float64(0)) == 2^(-1074)
```

```{exercise, q207}
Verify the smallest normalized, positive number that can be represented.
```

```{julia}
nextfloat(Float64(0))*2^(52)
```

```{exercise, q208}
Verify the largest, finite number that can be represented.
```

```{julia}
prevfloat(Float64(Inf))
```

```{exercise, q209}
Proof lemma (\@ref(lem:absolute-error-bound)). 
```

```{solution}

Let $z = (d_0 + d_1 \beta^{-1} + \dots)\beta^e$ be a number. Let $fl(z) = (d_0^\prime + d_1^\prime\beta^{-1} + \dots + d_{p-1}^\prime\beta^{-(p-1)})\beta^e$ be its fp representation with precision $p$. 

We know that $(d_0 + d_1 \beta^{-1} + \dots + d_{p-1}\beta^{-(p-1)})\beta^e \le z \le (d_0 + d_1 \beta^{-1} + \dots + (d_{p-1}+1)\beta^{-(p-1)})\beta^e$. We know that $fl(z)$ is equal to the one of these two bounds that is closest to $z$. Hence, $|fl(z) - z|$ must be at most half the distance between these two. Subtract the upper bound from the lower bound, and you get $\beta^{-(p-1)+e}$, i.e.

$$
  |fl(z) - z| \le \frac{\beta^{e-p+1}}{2}.
$$

```

```{exercise, q210}
What happens with lemmas (bounds of absolute and relative error) if we consider negative numbers?
```

```{solution, s209}
They still hold. Let $z^\prime = -z$. Then $fl(z^\prime) = -fl(z)$. Hence,

$$
  \left| fl(z^\prime) - z^\prime \right | = \left | -fl(z) + z \right | = \left | fl(z) - z \right |,
$$

hence the bounds still hold. 
```

```{exercise, q211}
Show that $\left|\left| A \right|\right|_1 =$ max of the $l^1$ norms of the columns of $A$.
```

```{solution, label = 's211'}
By definition, $\norm{A}_1 = \max_{x\neq 0}\frac{\norm{Ax}_1}{\norm{x}_1} = \max_{\norm{x}_1 = 1} \norm{Ax}_1$. Let $A \in \R^{m\times n}$ and $x \in \R^n$ s.t. $\norm{x}_1 = 1$. 

Recall that 

$$Ax = \sum_{j=1}^n x_j A_{*,j},$$

where $A_{*,j}$ is the $j^\prime$th column of $A$. So

$$
\begin{aligned}
  \norm{Ax}_1 &= \sum_{i=1}^m \left | \sum_{j=1}^n  x_j A_{i,j} \right| \\
              &\leq \sum_{i=1}^m \sum_{j=1}^n | x_j | \cdot | A_{i,j} | \\
              &= \sum_{j=1}^n | x_j | \left\{\sum_{i=1}^m | A_{i,j} | \right \} \\
              &= \sum_{j=1}^n | x_j | \norm{A_{*,j}} \\
              &\leq \sum_{j=1}^n | x_j | \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}} \\
              &= \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}}
\end{aligned}
$$

I.e. $\max_{\norm{x}_1 = 1} \norm{Ax}_1 \le \max_{j \in \{1,\ldots, n\}} \norm{A_{*,j}}$. 
  
Now, let $1_i = (x_j)_{j=1}^n$ be defined as 

$$x_j = \left\{ \begin{array}{rl} 1, & \text{ if } j = i \\ 0, & \text{ otherwise}\end{array} \right .$$

Then $\norm{1_i}_1 = 1$. Since $\max_{j \in \{1, \ldots, n\}}\norm{A_{*,j}} = \norm{A\cdot \mathbf{1}_i}$ for $i = \argmax \norm{A_{*,j}}_1$, we have that $\max_{j \in \{1,\ldots,n\}} \norm{A_{*,j}}_1 \le \max_{\norm{x}_1 = 1} \norm{Ax}_1$. 


```


```{exercise, q212}
Let $A \in \R^{n \times m}$. Show that $\left|\left| A \right|\right|_\infty =$ max of the $l^1$ norms of the rows of $A$.
```

```{solution, label = "s212"}
Let $j \in \{1, \ldots, n\}$ such that $|a_{j1}| + \dots + |a_{jm}| \ge |a_{i1}| + \dots + |a_{im}|$ for all $i \in \{1, \ldots , n\}$, and let $\tilde{x} \in \R^{m}$ such that $\tilde{x}_i = \text{sign}(a_{ji})$. Note that for all $x \in \R^{m}$ with $\norm{x}_\infty = 1$ it holds that $|x_i| \le 1$. So,

$$\begin{aligned}
  |a_{i1}x_1 + \dots + a_{im}x_m| &\le |a_{i1}| + \dots + |a_{im}| \\
                                  &\le |a_{j1}| + \dots + |a_{jm}| \\
                                  &= |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1|,
\end{aligned}$$
  
I.e. for any $x \in \R^m$ with $\norm{x}_\infty = 1$, 

$$\begin{aligned}
  |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1| &\ge \max_{1 \le i \le n}\{|a_{i1} x_1 + \dots + a_{im}x_m|\} \\
                                                   & = \norm{Ax}_\infty. 
\end{aligned}$$
  
Since $\norm{\tilde{x}}_\infty = 1$, we have that 

$$\begin{aligned}
  \max_{\norm{x} = 1} \norm{Ax}_\infty &= |a_{j1} \tilde{x}_1 + \dots + a_{jm}\tilde{x}_1| \\
                                       &= |a_{j1}| + \dots + |a_{jm}| \\
                                       &= \max_{1\le j \le n} \sum_{i=1}^n |a_{ji}|,
\end{aligned}$$
  
which is exactly the maximum over the $\mathcal{l}^1$-norms of the rows in $A$. 

```

```{exercise, q213}
Assume the Fundamental Axiom. Show the following:
  
$$\left|\left| fl(A)-A \right|\right|_p \leq u \left|\left|A\right|\right|_p$$
```

```{solution, s213}
\begin{align*}
  \left | \left | fl(A) - A \right | \right |_p &= \left|\left| \left [ fl(a_{ij}) - a_{ij} \right ] \right|\right|_p \\
    &\leq \left|\left| \left[u\cdot a_{ij}\right]\right|\right|_p \\
    &= \left|\left| u\cdot A\right|\right|_p = u\left|\left|A\right|\right|
\end{align*}
```

## Homework 3

```{exercise, q301}
Prove lemma \@ref(lem:proofHW). 
```

```{solution}
Recall that a matrix $A$ is invertible if and only if $Ax = 0$ implies that $x = 0$. So to check that $I-F$ is invertible, we check this:
  
$$
  (I-F)x = x-Fx = 0 \Rightarrow x = Fx \Rightarrow \norm{x}_p \leq \norm{F}_p \norm{x}_p.
$$
  
Since $\norm{F}_p < 1$ by assumption, the only solution to the inequality above is $x = 0$. So, $I-F$ is invertible. 


Note that $\sum_{k=0}^N F^k (I - F) = I - F^{N+1}$. Since $\norm{F^k}_p < \norm{F}_p^k < 1$, we know that $F^N \rightarrow 0$ as $N \rightarrow \infty$. So, $\sum_{k=0}^\infty F^k (I-F) = I$, and using that $I-F$ is invertible, we get $(I-F)^{-1} = \sum_{k=0}^\infty F^k$. Finally,

$$\norm{(I-F)^{-1}}_p \leq \sum_{k=1}^\infty \norm{F}_p^k = \frac{1}{1- \norm{F}_p}$$. 

```

```{exercise, q302}
Consider Theorem and Lemmas under "Square Linear Systems" (\@ref(square-linear-systems)). What happens if we use $l^{1}$-norm instead?
```

```{solution}
Since we never use any properties of the infinity norm to prove these theorems and lemmas, we could replace it with the $\mathcal{l}^1$-norm. 
```


```{exercise, q303}
Generate examples that show the bound in theorem \@ref(thm:kappa-bound) is too conservative.
```

```{julia}
using LinearAlgebra

p = 53 # precision for float-64
u = 2.0^(-p+1)
A = [
    0. 0 0.000001;
    200000 0 0;
    0 1 20000;
]
x = [1.; 1; 1;]
b = A*x

kappa = norm(A, Inf) * norm(inv(A), Inf)
bound = 2.0*u*kappa/(1.0 - u*kappa)

println("Diff: $(norm(x - inv(A)*b, Inf)/norm(x,Inf))")
println("Bound: $(bound)")
```

```{exercise, q304}
Generate examples that show the bound is nearly achieved
```

```{julia}
using LinearAlgebra

A = [
    0. 0 1.;
    1 0 0;
    0 1 0;
]
x = [1.; 1; 1;]
b = A*x

kappa = norm(A, Inf) * norm(inv(A), Inf)
bound = 2.0*2.0^(-p+1)*kappa/(1.0 - 2.0^(-p+1)*kappa)

println("Diff: $(norm(x - inv(A)*b, Inf)/norm(x,Inf))")
println("Bound: $(bound)")
```

```{exercise, q305}
For motivating problems 1-5, when is $x$ unique?
```

```{solution}

- Motivating Problem 1:
    * when $A$ is invertible
- Motivating Problem 2: Always. 
    * Since $F(y) = \norm{Ay - b}_2 = (Ay-b)^\prime (Ay-b)$ is convex (twice differentiated is positive definite because $\nabla^2 f = A^\prime A$ and $A$ has full rank).
    * (since objective function is $A^\prime A$, and it is positive definite (because A is full rank), then the function is convex, and you always have a unique solution)
- Motivating Problem 3: Always.
- Motivating Problem 4: Always. 
    * We know how to characterize all $z$ that satisfy objective: $Q\begin{bmatrix} R & S \\ 0 & 0 \end{bmatrix} \Pi^\prime = A$.

- Motivating Problem 5: always

```

```{exercise, q306}
For motivating problem 5, what happens if $p\geq m$? Explore the case where $m >> n$.
```

```{solution, s306}
* For m >> n, you have an undertermined system with more unknowns than equations. Since the null space of A has a dimension larger than zero, for any particular solution xp for the system, xp+h with $h \in \text{null}(A)$ is also a solution, and there are infinitely many choices for h. The constraint system might help narrow down the solutions from the null space.
* For p >= m: if C becomes inconsistent, we cannot narrow down the solutions in the null space. If C is consistent and rank(C)=m, then we can pick a unique solution. If the rank of C is less than m, then C constrains some of the possible solutions for y.
```

```{exercise, q307}
What do you get if you multiply a matrix by a permutation matrix from the left? From the right? A permutation with itself?
```

```{solution}
From the left: permute rows.

From the right: permute columns.

Permutation squared gives you the identity.
```

```{exercise, q308}
Suppose $R \in \R^{m\times m}$ is an upper triangular matrix with $R_{ii} \neq 0$ for all $i = 1,\ldots, n$. Is $R$ invertible?
```

```{solution}
Since $R$ is an upper triangular matrix, $\det(R) = \prod_{i=1}^{m} R_{ii} > 0$. Hence, $R$ is invertible. 
```

```{exercise, q309}
Assume $R$ is an invertible upper triangular matrix. Implement a solution to invert $R$. 
```

```{solution, s310}
First, we will show that the inverse of $R$ is also an upper triangular matrix. So, let $B = R^{-1}$. The $RB = I$. We will show this using induction. Let $i = n - 0, j < i$. Then, since $r_{ij} = 0$ for all $i > j$,

$$\begin{aligned}
  0 &= \sum_{k=1}^n r_{n,k}b_{k,j} \\
    &= r_{n,n}b_{n,j}.
\end{aligned}$$
    
Since $R$ is invertible, $r_{n,n} \neq 0$, hence $b_{n,j} = 0$ for all $j < n$. 
  

Now assume that $b_{i,j} = 0$ for all $i=n-0,n-1, \ldots, n-m, j < i$. We then want to show it holds for $i = n - (m+1) = n-m-1$. Let $j < n-(m+1)$. Then

$$\begin{aligned}
  0 &= \sum_{k=1}^n r_{n-(m+1), k} b_{k, j} \\
    &= \sum_{k=n-(m+1)}^{n} r_{n-(m+1), k} b_{k, j} \\
    &= r_{n-(m+1), n-(m+1)} b_{n-(m+1),j},
\end{aligned}$$
    
where the first equality is due to the fact that $r_{ij} = 0$ for all $i > j$, and the last equality holds since $b_{k,j} = 0$ for all $j < k$ when $k \ge n-m$ (per the induction hypothesis). Since $r_{ii} \neq 0$ for all $i$, $b_{n-(m+1),j} = 0$. 
  
So, $b_{ij} = 0$ for all $i > j$. 

Now, lets look at the case where $i=j$, i.e. diagonal elements of the inverse matrix. Then

$$\begin{aligned}
  1 = \sum_{k=1}^n r_{ik} b_{ki} = \sum_{k=i}^{n} r_{ik} b_{ki} = r_{ii}b_{ii}.
\end{aligned}$$

The second equality holds since $r_{ik} = 0$ for all $i > k$, the last equality holds because $b_{ki} = 0$ for all $k > i$. We see that $b_{ii} = r_{ii}^{-1}$.
  
Finally, consider the case where $i < j$. Then, using that $r_{ik} = 0$ for all $i>k$ and $b_{kj} = 0$ for all $k>j$, we see that 

$$
  0 = \sum_{k=1}^n r_{ik}b_{kj} = \sum_{k=i}^j r_{ik} b_{kj},
$$
  
which implies

$$
  b_{ij} = \frac{-\sum_{k=i+1}^j r_{ik} b_{kj}}{a_{ii}}.
$$
  
So, in other words, given $i,j$, we can find $b_{ij}$ if we know $b_{kj}$ for all $k>i$ (i.e. all entries in the same column below the entry we are considering). Since we already know all entries below and on the diagonal, this is true for all columns. Hence, we can construct $B$ this way. 

See the `Julia` chunk below for an implementation of this.

```

```{julia}
function invertUpperTri(A)
    ## Get dimensions of A
    n, m = size(A)

    ## Setup empty array to hold result
    B = zeros(n,m)

    ## Fill out diagonal with inverse diagonal from A
    for k = 1:n
        B[k, k] = 1/A[k,k]
    end

    ## Starting in the lower right corner, fill out the rest of the matrix.
    for i = (n-1):-1:1
        for j = (i+1):n
            B[i,j] = -sum(A[i,(i+1):j].*B[(i+1):j,j])/A[i,i]
        end
    end

    return(B)
end

## Create an upper triangular matrix to check
A = rand(4,4);
A[2:4,1] = [0 0 0];
A[3:4,2] = [0 0];
A[4,3] = 0;

## Check function
B = invertUpperTri(A);
B - inv(A)  # should be 0 matrix
A*B         # should be identity
```

## Homework 4

```{exercise, q401}
In the solution to \@ref(exm:linear-system), why do $0$ rows on the left-hand side of \@ref(eq:sol-linear-system) correspond to $0$ entries of the $c$ vector on the right-hand side. 
```

```{solution, s401}
By definition of matrix multiplication, if the $i$th row is a zero row of $A$, then $c_{ij} = [AB]_{ij} = \sum{j=1}^n a_{ij} b_{ij} = 0$, no matter what $B$ is. 
```

```{exercise, q402}
Let $Q$ be an orthogonal matrix. Show that $\norm{Qx}_2 = \norm{x}_2$. 
```

```{solution, s402}

Since $\norm{x}_2^2 = x^\prime x$ and $Q^\prime Q = 1$ ($Q$ is orthogonal), 

$$ 
  \norm{Qx}^2_2 = (Qx)^\prime Qx = x^\prime Q^\prime Q x = x^\prime x = \norm{x}^2_2.
$$

```

```{exercise, q403}
Let f be a vector-valued function over $\R^d$. When is $$\argmin_x \norm{f(x)}_2 = \argmin_x \norm{f(x)}_2^2.$$
```

```{solution, s403}
Most of the time...?
```


```{exercise, q404}
Prove that $P^T P + I$ from solution to example \@ref(exm:und-linear-system) is invertible. 
```

```{solution, s404}

For all non-zero $x$,

$$
  x^\prime(P^TP + I)x = x^\prime P^\prime P x + x^\prime x = \norm{Px}_2 + \norm{x}_2 > 0.
$$
  
This means $P^\prime P + I$ is positive definite, which in turne implies that it is invertible. 
  
```

```{exercise, q405}
Write out the solution to example \@ref(exm:constrained-least-squares). Also consider the case where $p \ge m$.
```

```{solution, s405}
First we do QR decomposition on $C^\prime$:

$$\begin{aligned}
C^\prime = Q\begin{bmatrix} R \\ 0 \end{bmatrix}
\end{aligned}$$

Then we have:

$$\begin{aligned}
AQ &= \begin{bmatrix} A_1 & A_2 \end{bmatrix} \\
Q^\prime y &= \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \\
\end{aligned}$$

And we can update the objective:

$$\begin{aligned}
O &= \min \|Ay-b\|_2 : Cy = d\\
&= \min \|AQQ^\prime y -b\|_2 : Cy = d\\
&= \min \left\| \begin{bmatrix} A_1 & A_2 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} - b \right\| _2 : Cy = d\\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : Cy = d\\
\end{aligned}$$

We can also update the constraint:

$$\begin{aligned}
O &= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : \begin{bmatrix} R^\prime & 0 \end{bmatrix} Q^\prime y = d \\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : \begin{bmatrix} R^\prime & 0 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = d \\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : R^\prime y_1 = d \\
&= \min \left\| A_1y_1 + A_2 y_2 - b \right\|_2 : y_1 = (R^\prime)^{-1}d  \\
&= \min \left\| A_1(R^\prime)^{-1}d + A_2 y_2 - b \right\|_2\\
&= \min \left\| A_2 y_2 - (b - A_1(R^\prime)^{-1}d)\right\|_2\\
\end{aligned}$$

So $y_1$ can be calculated using a consistent linear system solver, and now the objective is the same as that of a least squares solver, which can be used to calculate $y_2$. We can finally recover $y$:

$$\begin{aligned}
y &= Q \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} \\
\end{aligned}$$

```

```{exercise, q406}
For all motivating problems, implement solutions. 
```

</br>

For the following, assume $A \in \R^{n\times m}$ with $\text{rank}(A) = m$, and $b \in \R^n$. Let $C = \begin{bmatrix} A & b \end{bmatrix}$. 


```{exercise, q407}
What does the last column of $R$ (from the QR decomposition of $C$) represent?
```

```{solution, s407}
Generally, $Q^\prime b$, which is the projection of b onto the column space of A. If $b$ is in the column space of $A$, then it is specifically $Rx$ where $x$ is the solution to $Ax = b$. 
```

```{exercise, q408}
What does the last entry of last column of $R$ (from the QR decomposition of $C$) represent?
```

```{solution, s408}
The square of the last entry of the last column of $R$ is the sum of squares of the residuals.
```

```{exercise, q409}
How can this be used in computation?
```

```{exercise, s409}
We can use it for incremental QR for large datasets.
```

## Homework 5

```{exercise, q501}
Implement the Gram-Schmidt procedure for matrices $A \in \R^{n \times m}$ assuming $A$ has full column rank.
  
Create examples to show that the function works (well enough).
```

```{solution, s501}
See [here](../Homework/gram_schmidt.jl).
```

```{exercise, q502}
Find examples where Gram-Schmidt fails, i.e. where either $Q R \neq A$ or $Q^TQ \neq I$. 
```

```{solution, s502}
Let $A = \begin{bmatrix} 1 & 1 & 1 \\ 10^{-8} & 0 & 0 \\ 0 & 10^{-8} & 0 \end{bmatrix}$. Then $Q^\prime Q \neq I$.
```

```{exercise, q503}
Look up the modified Gram-Schmidt Procedure and implement it (again assuming $A$ has full column rank). 
```

```{solution, s503}
See [here](../Homework/modified_gs.jl).
```


```{exercise, q504, name = "Pivoting (*OPTIONAL*)"}
References:
  
  1. Businger, Galub: Linear Least Squares by Householder Transformation (1965)
  2. Engler: The Behavior of QR-factorization algorithm with column pivoting (1997)
  
Implement modified Gram-Schmidt with column pivoting. 

Find example where the modified Gram-Schmidt fails, but the modified Gram-Schmidt with column pivoting does not.
  
```


```{exercise, q505}
Show that Householder reflections are orthogonal matrices.
```

```{solution}
Show that $H^\prime H = I$. By definition of a Householder matrix (\@ref(def:householder)), $H = I - 2vv^\prime$ for a $v$ with $\norm{v}_2 = 1$. Note that $\norm{v}_2 = \sqrt{v^\prime \cdot v}$. 

So,

$$\begin{aligned}
  H^\prime H &= (I - 2vv^\prime)^\prime (I - 2vv^\prime) \\
             &= (I^\prime - 2(vv^\prime)^\prime)(I - 2 vv^\prime)\\
             &= (I - 2vv^\prime)(I - 2 vv^\prime) \\
             &= I - 2vv^\prime - 2vv^\prime + 4 vv^\prime v v^\prime \\
             &= I - 2vv^\prime - 2vv^\prime + 4 v I v^\prime \\
             &= I.
\end{aligned}$$  
    
So by definition (\@ref(def:orthogonal)), $H$ is an orthogonal matrix.
```

```{exercise, q506}
Show that if $H_1, \dots, H_r$ are Householder reflections, then $H_r \cdots H_1$ (i.e. the product) is orthogonal.
```

```{solution, s506}

$$(H_r \cdots H_1)^\prime (H_r \cdots H_1) = H_1^\prime \cdots H_r^\prime H_r \cdots H_1 = I,$$
  
since all $H_i$ are Householder reflections, which implies they are orthogonal, which implies $H_i^\prime H_i = I$. 

```


```{exercise, q507}
Show that a Givens rotation is an orthonormal matrix when $\sigma^2 + \lambda^2 = 1$. 
```

```{solution, s507}

Let $G^{(a,b)}$ be a Givens rotation. I.e. the elements $g_{i,j}$ are 

$$
  g_{i,j} = 
  \left\{ 
    \begin{array}{rl} 
      1, & i = j \notin \{a,b\} \\
      \lambda, & i = j \in \{a,b\} \\
      \sigma, & i = a, j = b \\
      -\sigma, & i = b, j = a \\
      0, & \text{ otherwise}
    \end{array} 
  \right.
$$
        
Transposing this matrix gives us a new matrix $K$ where
      
$$
  k_{i,j} = 
  \left\{ 
    \begin{array}{rl} 
      1, & i = j \notin \{a,b\} \\
      \lambda, & i = j \in \{a,b\} \\
      -\sigma, & i = a, j = b \\
      \sigma, & i = b, j = a \\
      0, & \text{ otherwise}
    \end{array} 
  \right.
$$

Now, let $L = K \cdot G$. Then the elements of $L$ are $l_{i,j} = \sum_{s=1}^n k_{i,s} g_{s,j}$. 
      
If $i \neq a,\ j \neq b$, and $i \neq j$, then $l_{i,j} = k_{i,i} g_{i,j} = 0$ (since $g_{i,j} = 0$). 

If $i \notin \{a, b\}$, then $l_{i,i} = k_{i,i} g_{i,i} = 1$ (since $i \notin \{a,b\}$).

If $j \notin \{a,b\}$, then $l_{a,j} = k_{a,b}g_{b,j} + k_{a,a}g_{a,j} = 0$ (since $j \notin \{a,b\}$ implies $g_{b,j} = 0$ and $g_{a,j} = 0$).

Furthermore, 

* $l_{a,b} = k_{a,b}g_{b,b} + k_{a,a}g_{a,b} = -\sigma \lambda + \lambda \sigma = 0$,
* $l_{a,a} = k_{a,b} g_{b,a} + k_{a,a} g_{a,a} = (-\sigma)(-\sigma) + \lambda \lambda = \sigma^2 + \lambda^2$,
* $l_{b,b} = k_{a,b} g_{b,a} + k_{a,a} g_{a,a} = (-\sigma)(-\sigma) + \lambda \lambda = \sigma^2 + \lambda^2$.

So,
      
$$
  l_{i,j} = 
  \left\{ 
    \begin{array}{rl} 
      1, & i = j \notin \{a,b\} \\
      \sigma^2 + \lambda^2, & i = j \in \{a,b\} \\
      0, & \text{ otherwise}
    \end{array} 
  \right.
$$
        
Similar calculations can be performed for $G \cdot K$. So $G^{(a,b)}$ is orthogonal if and only if $\sigma^2 + \lambda^2 = 1$. 

```

## Homework 6

```{exercise, q601}
Determine the computational complexity of the QR decomposition using 

a) Gram-Schmidt
b) Modified Gram-Schmidt
c) Householder
d) Givens rotations

for any arbitrary, dense $n \times m$ matrix. (dense = don't know how many entries are $0$.)

```

```{solution, s601}

a) Gram-Schmidt requires $O(m^2n)$ computations.
b) Same as Gram-Schmidt. 
c) $O(m^2n)$.
d) $O(m^2n)$.


```

```{exercise, q602}
Compare the computational complexity of Householder and Givens for a sparse matrix (i.e. a matrix where a substantial number of entries are $0$).
```

```{solution, s602}
Householder still have the same complexity even for sparse matrices, whereas for Givens we do not have to complete as many multiplications with Givens rotations. (Everytime we encounter a 0 in the lower triangular matrix that can be paired with a zero above it, we can pair them up, and skip the computation.)
```

```{exercise, q603}
Implement QR decomposition using Householder. Write it as a function that takes a matrix $A \in \R^{n\times m}$ with $\text{rank}(A) = m$ as its input, and gives back $Q$ and $R$.
```

```{solution, s603}
See [here](../Homework/QR_householder.jl).
```

```{exercise, q604}
Implement QR decomposition using Givens. (As above.)
```

```{solution, s604}
See [here](../Homework/QR_givens.jl)
```

```{exercise, q605}
What happens in theorem \@ref(thm:svd) if $m > n$.
```

```{solution}

If $m > n$, the diagonal matrix of the SVD of $A$ will be of the form $\begin{bmatrix} \Sigma & 0 \end{bmatrix}$, where $\Sigma = \begin{bmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_n \end{bmatrix}$. 

```

```{exercise, q606}
If $u,v$ are respectively left and right singular vectors corresponding to the singular value $\sigma$, show that $Av = \sigma u$, and $A'u = \sigma v$. 
```

```{solution, s606}
Using the SVD of $A$, we see that $AV = U\Sigma$, which implies 

$$
  A \begin{bmatrix} v_1 & \cdots & v_m \end{bmatrix} = \begin{bmatrix} \sigma_1 u_1 & \cdots & \sigma_m u_m \end{bmatrix}.
$$
  
where $v_1, \dots, v_m$ are the right singular vectors of $A$, and $u_1, \dots, u_m$ are the left singular vectors of $A$. 

So, $Av_i = \sigma_i u_i$ for all $i$. 

Similarly, $A^\prime = V \Sigma U^\prime$ implies $A^\prime U = V \Sigma$. As above, this gives us $A^\prime u_i = \sigma_i v_i$.

```


## Homework 7

```{exercise, q701}
Show that $\norm{A}_F^2 = \sum_{i=1}^n\sum_{j=1}^n a_{ij}^2$ is orthogonally invariant, i.e. if $Q,P$ are orthogonal matrices, then $\norm{QA}_F^2 = \norm{A}_F^2$ and $\norm{AP}_F^2 = \norm{A}_F^2$. 
```

```{solution, s701}

Recall, that $\norm{A}_F^2 = \text{trace}(A^\prime A)$. So,

$$
  \norm{QA}_F^2 = \text{trace}(A^\prime Q^\prime Q A) = \text{trace}(A^\prime A) = \norm{A}_F^2,
$$

and 

$$
  \norm{AP}_F^2 = \text{trace}(P^\prime A^\prime A P) = \text{trace}(A^\prime A P P^\prime) = \text{trace}(A^\prime A) = \norm{A}_F^2.
$$

```

```{exercise, q702}
Proof corollary \@ref(cor:sigma-min-max).
```

```{solution, s702}
Recall that $\sigma_{\max} (A) = \norm{A}_2$. Hence, $\sigma_{\max}(A+E) = \norm{A+E}_2 \leq \norm{A}_2 + \norm{E}_2 = \sigma_{\max}(A) + \norm{E}_2$. 

For the second inequality, recall that $\sigma_{\min}(A) = \min_{x \neq 0} \frac{\norm{Ax}}{\norm{x}}$. First, consider the case where $\sigma_{\min}(A) = 0$. Then the result holds. Second, consider the case where $\sigma_{\min}(A) > 0$. Then

$$\begin{aligned}
  \sigma_{\min}(A+E) &= \min_{v \neq 0} \frac{\norm{(A+E)v}_2}{\norm{v}_2} \ge \min_{v \ne 0} \frac{\norm{Av}_2}{\norm{v}_2} - \max_{v \ne 0} \frac{\norm{Ev}_2}{\norm{v}_2} \\
  &= \sigma_{\min}(A) - \norm{E}_2,
\end{aligned}$$
  
where we use the inverse triangle equality. 

```

```{exercise, q703}
Proof corollary \@ref(cor:for-q703).
```

```{solution, name = "1"}

Let $b \in \text{range}(A)$. Then there exists some $z$ such that $Az = b$. Use the SVD of A to get 

$$
  b = U\Sigma V^\prime z.
$$
  
Since $\text{rank}(A) = r$, the last $n-r$ rows of $\Sigma$ are all $0$, so $U \Sigma V^\prime = U_r \Sigma_r (V_r)^\prime$, where $U_r$ is the first $r$ columns of $U$, $\Sigma_r$ the first $r$ columns and $r$ rows of $\Sigma$, and $V_r$ the first $r$ columns of $V$. Hence, $\Sigma_r (V_r)^\prime z = y \in \R^r$. So, $b = U_r y$, which means it is in the span of the first $r$ columns of $U$. Hence, $\text{range}(A) \subset \text{span}\{u_1, \dots, u_r\}$. 

Now, let $b \in \text{span}\{u_1, \dots, u_r\}$. Then $b = U_r y$ for some $y \in \R^r$. Recall that $U = A V \Sigma^{-1}$, hence $U_r = A V_r \Sigma_r^{-1}$. So, $b = A V \Sigma_r^{-1} y$. Since $V \Sigma_r^{-1} y = z \in \R^m$, $b$ is in the range of $A$. Hence, $\text{span}\{u_1, \dots, u_r\} \subset \text{range}(A)$.

```

```{solution, name = "2"}
Note that the row space of $A$ is the range of $A^\prime$. Solve as above. 
```

```{solution, name = "3"}

Let $b \in \text{span}\{v_{r+1}, \dots, v_{m}\}$. Then $b = x_{r+1} v_{r+1} + \cdots + x_m v_{r+1}$ for some coefficients $x_j \in \R$. Then 

$$\begin{aligned}
Ab  &= U \Sigma V^\prime b \\ 
    &= U \Sigma (x_{r+1} V^\prime v_{r+1} + \cdots + x_{m} V^\prime v_m) \\
    &= U \Sigma \begin{bmatrix} 0 \\ \vdots  \\ 0 \\ x_{r+1} \\ \vdots \\ x_{m} \end{bmatrix},
\end{aligned}$$

since all columns of $V$ are orthogonal. 

Since the last $n-r$ rows of $\Sigma$ are all $0$ rows, $\Sigma V^\prime b = 0$, so $Ab = 0$, so $b \in \text{null}(A)$. Hence, $\text{span}\{v_{r+1}, \ldots, v_{m}\} \subset \text{null}(A)$.

Now, Assume $b \in$ null($A$), then $Ab = 0$. Because $b \in R^m$ and $v_1, \cdots, v_m$ is a base for $R^m$, there exists a sequence $\{x_i\}$ s.t.  $b = x_1v_1 + \cdots + x_rv_r + x_{r+1}v_{r+1} + \cdots + x_{m}v_m$.

Assume for contradiction that $b \notin$ span($v_{r+1}, \cdots, v_m$), i.e. that $x_1, \cdots, x_r$ are not all zero. Then

$$Ab= \sum_{i=1}^{r} \sigma_i u_i v_i^T b = \sum_{i=1}^{r}[ \sigma_i u_i v_i^T (\sum_{j=1}^{m}x_jv_j)]= 
\sum_{i=1}^{r}[ \sigma_i u_i (\sum_{j=1}^{m}x_jv_i^Tv_j)] = \sum_{i=1}^{r} \sigma_i u_i x_i
\neq 0$$

But this contradicts the assumption that $b \in \text{null}(A)$. So, $b$ must be in $\text{span}\{v_{r+1}, \cdots, v_m\}$. Hence, $\text{null}(A) \subset \text{span}\{v_{r+1}, \cdots, v_m\}$.


```

## Homework 8

```{exercise, q801}
What is the solution to $\min_{x} \norm{Ax-b}_2^2$ in terms of the Moore-Penrose inverse? (pseudo-inverse)
```

```{solution, s801}
To find the solution, we differentiate and set equal to $0$. So, differentiate $g(x) = x^\prime A^\prime A x - x^\prime A b - b^\prime A x + b^\prime b$ and set to $0$:

$$\frac{dg}{dx} = x^\prime (A^\prime A + A^\prime A) - b^\prime A - (A^\prime b)^\prime = 2x^\prime (A^\prime A) - 2b^\prime A = 0$$
  
which implies $(A A^\prime)x = b A^\prime$. Check that $x = A^+b$ satisfies this. 

```

```{exercise, q802}
Let $Q$ be an orthogonal matrix with columns $q_1, \ldots, q_n$. Let $Z \in \R^{n \times n}$ be a symmetric matrix. Show that $\sum_{i=1}^n (q_i^\prime Z q_i)^2 \leq \norm{Z}_F^2$.
```

```{solution, s802}

Since the frobenius norm is invariant to mulitplication by orthogonal matrices: $\norm{Z}_F^2 = \norm{Q^\prime Z Q}_F^2$. Since 

$$\begin{aligned}
   \begin{bmatrix} q_1^\prime & \cdots q_n^\prime \end{bmatrix} Z \begin{bmatrix} q_1 \\ \vdots q_n \end{bmatrix} &= 
          \begin{bmatrix} q_1^\prime & \cdots q_n^\prime \end{bmatrix} \begin{bmatrix} Z q_1 \\ \vdots Z q_n \end{bmatrix} \\
  &= \begin{bmatrix}  q_1^\prime Z q_1 & \cdots & q_1^\prime Z q_n \\
                      \vdots & \ddots & \vdots \\
                      q_n^\prime Z q_1 & \cdots & q_n^\prime Z q_n
\end{bmatrix},
\end{aligned}$$

we have that $\norm{Z}_F^2 = \sum_{i=1}^n \sum_{j=1}^n (q_i^\prime Z q_j)^2 \ge \sum_{i=1}^n (q_i^\prime Z q_i)^2$.

```

## Homework 9

```{exercise, q901}
What do we need from $A$ to ensure that the different iterative method schemes are well-defined?
```

```{solution, s901}
A must be square and diagonal entries must be non-zero.
```

```{exercise, q902}
Does lemma \@ref(lem:next-current-diff) hold for the symmetric SOR?
```

```{exercise, q903}
Compute $J^k$ where $J = I\lambda + E$ (the Jordan Canonical Form from definition \@ref(def:jordan))

Show that if $G = XJX^{-1}$ and $\rho(G) < 1$, then $J^k \rightarrow 0$ as $k\rightarrow \infty$.

```

## Homework 10

```{exercise, q1001}
Implement Jacobi, Gauss-Seidel without using the `\` operator.
```

```{solution, s1001}
See [Jacobi]("../Homework/jacobi.jl") and [Gauss-Seidel]("../Homework/gauss_seidel.jl").
```

```{exercise, q1002}
Implement SOR, SSOR without using the `\` operator.
```

```{solution, s1002}
See [SOR]("../Homework/SOR.jl").
```

```{exercise, q1003}

Randomly generate problems and 

a) at each iteration, record residual norm and absolute error
b) compare rate of convergence against spectral radious
c) put all this information into a narrative using graphics

```

```{exercise, q1004}
Prove that observation 1 from the proof of \@ref(thm:rand-kaczmarz) is true.
```

```{exercise, q1005}
Implement Random Kaczmarz for random permutations.
```

```{exercise, q1006}
Give a detailed comparison of cycle, randomized, and random permutation Kaczmarz.
```


## Homework 11

```{exercise, q1101}
Show that $\alpha \sum_{i=1}^n a_i (b_i - a_i x^c) = \alpha A^\prime (b - Ax^c)$.
```

```{solution}
Recall that $a_i$ is a column vector, and that $a_i^\prime$ is the row vector of matrix $A$

$A^T(b-Ax^c) = \begin{pmatrix} a_1, \cdots, a_n \end{pmatrix}(b - Ax^c) = \sum_{i=1}^{n}a_i(b_i - a_i^Tx^c)$

```

```{exercise, q1102}
Show that if $A^\prime r^c = 0$, then $x^c = x^*$, where $Ax^* = b$. 
```

```{solution}

$x^+= x^c + \alpha A^T(b-Ax^c) = x^c + \alpha A^Tr^c = x^c$

```

```{exercise, q1103}
Why is it enough to find an upper bound on $\norm{\Sigma^{-1}u}_2^2 \norm{\Sigma u}_2^2$ for any unit vector $u$ in the proof of theorem \@ref(thm:grad-descent)? 
```

```{solution}

Recall that $\norm{A} = \sup_{\norm{v}=1} \norm{Av} = \sup_{\norm{v} \neq 0} \frac{\norm{Av}}{\norm{v}}$. Hence, 

$$\begin{aligned}
\norm{\Sigma^{-1} u}_2^2 \norm{\Sigma u}_2^2 \le \frac{(\sigma_1^2 + \sigma_n^2)^2}{4\sigma_1^2 \cdot \sigma_n^2} \quad \forall u: \norm{u} = 1 \\
\Rightarrow \frac{1}{\norm{\Sigma^{-1} u}_2^2 \norm{\Sigma u}_2^2} \le \frac{4\sigma_1^2 \cdot \sigma_n^2}{(\sigma_1^2 + \sigma_n^2)^2} \quad \forall u: \norm{u} = 1 \\ 
\Rightarrow \frac{\norm{w}}{\norm{\Sigma^{-1} w}_2^2}\frac{\norm{w}}{\norm{\Sigma w}_2^2} \le \frac{4\sigma_1^2 \cdot \sigma_n^2}{(\sigma_1^2 + \sigma_n^2)^2} \quad \forall w: \norm{w} \neq 0.
\end{aligned}$$
  
So, find an upper bound on $\norm{\Sigma^{-1} u}_2^2 \norm{\Sigma u}_2^2$ that holds for all unit vectors $u$, and we have a lower bound on $\frac{\norm{w}}{\norm{\Sigma^{-1} w}_2^2}\frac{\norm{w}}{\norm{\Sigma w}_2^2}$ that holds for all non-zero vectors $w$. 

```

```{exercise, q1104, name = "Kontorovich's Inequality"}
Let $0 < u_n \le u_{n-1} \le \cdots \le u_1$. Then

$$\left(\sum_{i=1}^n p_i u_i\right)\left(\sum_{i=1}^n \frac{p_i}{u_i}\right) \leq \frac{(u_1 + u_n)^2}{4u_1u_n}$$

```

```{solution}
http://mathworld.wolfram.com/KantorovichInequality.html
http://services.aops.com/download.php?id=YXR0YWNobWVudHMvMS84LzM2YWVhMTg3Yzc0NDA5YTZmNDRhZDVjNmZlNmM4NzllMjRjNmE3LnBkZg==&rn=a2FudG9yb3ZpY2hfcHJvb2YucGRm
```

```{exercise, q1105}
For strategy 2, answer the following questions:
  
a) What is $\alpha$? Is it practical?
b) With this $\alpha$, will we converge? If so, what is the rate of convergence?

```

```{solution}

According to strategy 2, we choose $\alpha$ such that it minimizes $\norm{x_{k+1} - x^*}_2^2$. So, to minimize this, we differentiate with respect to $\alpha$, set to $0$, and solve for $\alpha$ -- the result is $\alpha = \frac{\norm{x_k - x^*}_2^2}{\norm{A^\prime r_k}_2^2}$.

```

```{exercise, q1106}
For strategy 3, what is $\alpha$?
```

```{solution}

According to strategy 3, we choose $\alpha$ such that it minimizes $\norm{I-\alpha AA^\prime}_2^2$. Again, differentiate with respect to $\alpha$, set to $0$, and solve for $\alpha$ -- the result is $\alpha = \frac{\norm{A}_2}{\norm{AA^\prime}_2}$.

```

```{exercise, q1107}
For strategy 4, show the inequality holds.
```

```{solution}

Recall $Ax^* = b$. 

$$\begin{aligned}
  \norm{x_{k+1} - x^*} &= \norm{x_k + \alpha A^\prime(b-Ax_k) - x^*} \\
                       &= \norm{x_k - x^* - \alpha A^\prime (Ax^* - Ax_k)} \\
                       &= \norm{(I - \alpha A^\prime A)(x_k - x^*)} \le \norm{I-\alpha A^\primeA}\norm{x_k - x^*}
\end{aligned}$$

```

```{exercise, q1108}
For strategy 4, what is $\alpha$?
```

```{solution}
Same as for strategy 3.
```

```{exercise, q1109}
Implement a single update of the gradient descent method for a user supplied alpha, assuming that A is invertible.
```

```{solution}
See the Julia chunk below
```

```julia
using LinearAlgebra

# Single update of the gradient descent method

function singleGradDescent(A,x,b, alpha)
    """
    b is constant vector.
    A is the coefficient matrix.
    x is current iterate.
    alpha is step size.
    """
    x = x + alpha * A'*(b-A*x)
    return x
end
```

```{exercise, q1110}
Now, implement four algorithms one for each of the four strategies of alpha discussed in class.
```

```{solution}
See Julia chunk below
```

```julia

## Gradient descent for Strategy 1: alpha = ||A'r_c||^2/||AA'r_c||^2
function gradientDescentStrat1(A,b,x,x0;epsilon=1e-15, maxIter=1000)
    # Get current residual
    rc = A*x0-b
    # Get current error
    ec = norm(x - x0)
    i = 1

    while norm(A'*rc) >= epsilon && i < maxIter
        alpha = norm(A'*rc)^2/norm(A*A'*rc)^2
        x0 = SingleStepGS(alpha,A,x0,b)
        rc = A*x0-b
        ec = vcat(ec, norm(x0 - x))
        i += 1
    end

    return x0, ec
end

## Gradient descent for Strategy 2: alpha = ||A'(x_c - x^*)||^2/||AA'(x_c-x^*)||^2
function gradientDescentStrat2(A,b,x,x0;epsilon=1e-15, maxIter=10000)
    # Get current residual
    rc = A*x0-b
    # Get current error
    ec = norm(x0 - x)
    i = 1

    while norm(rc) >= epsilon && i < maxIter
        alpha = norm(A'*ec[end])^2/norm(A*A'*ec[end])^2
        x0 = SingleStepGS(alpha,A,x0,b)
        rc = A*x0-b
        ec = vcat(ec, norm(x0 - x))
        i += 1
    end

    return x0, ec
end

## Gradient descent for Strategy 3: alpha = ||A||^2/||AA'||^2
function gradientDescentStrat3(A,b,x,x0;epsilon=1e-15, maxIter=10000)
    # Get current residual
    rc = A*x0-b
    # Get current error
    ec = norm(x0 - x)
    i = 1
    alpha = norm(A)^2/norm(A*A')^2

    while norm(rc) >= epsilon && i < maxIter
        x0 = SingleStepGS(alpha,A,x0,b)
        rc = A*x0-b
        ec = vcat(ec, norm(x0 - x))
        i += 1
    end

    return x0, ec
end

```

```{exercise, q1111}
Generate several test problems for your four algorithms, and compare their performance.
```

```{solution}
???
```

```{exercise, q1112}
In a narrative, explain which algorithm you would use and when you would use this algorithm.
```

```{solution}
???
```

**Understanding Gradient Descent's Dependence on the Condition Number**

```{exercise, q1113}
Write a function to generate 2 equations with 2 unknowns such that the coefficient matrix is dense and symmetric with user specified nonzero eigenvalues.
```

```julia
function generateProblemEigenvalues(eigenvalues)
    n = length(eigenvalues)
    B = rand(n,n)
    Q = qr(B).Q
    Lambda = diagm(0 => eigenvalues)
    A = Q*Lambda*Q'
    x = randn(n)
    b = A*x
    return A, x, b
end

```

```{exercise, q1114}
How are the eigenvalues related to the singular values?
```

```{solution}

In general, the eigenvalues of a matrix $A$ is the square root of the symmetric matrix $A^\prime A$. 

When $A$ is symmetric, they are the same. 

```

```{exercise, q1115}
How are the eigenvectors related to the left and right singular vectors?
```

```{solution}

The left singular vectors are the eigen vectors.

```

```{exercise, q1116}
Write a function to draw the level sets of an arbitrary residual function $f\left(x\right) = \norm{Ax - b}_2^2$.
```

```{solution}
???
```


```{exercise, q1117}
Run Gradient Descent using the step size (alpha) from Strategy 2 to solve a sequence of problems where the difference between the singular values of your problem increases in size. Plot the points that Gradient Descent visits (on your level set plot) as it finds its way to the solution.
```

```{solution}
???
```


```{exercise, q1118}
What do you observe? What is the impact of the singular values on gradient descent? Why does this make sense based on your graphs?
```

```{solution}
???
```


## Homework 12

```{exercise, q1201}
What conditions (if any) on $a_0, ..., a_{d-1}$ guarantee that an $A-$normalized, $A-$conjugated set $\{s_0, ..., s_{d-1}\}$ can be produced by Gram Schmidt?
```

```{solution}
We need the norm of the vectors to be not too small. If they are too small, numerical stability is not guarenteed, which means we might end up with vectors that are not A-conjugated or not A-normalized. 
```


## Homework 13

```{exercise, q1301}
Show that the procedure in \@ref(conjugatedgradients) is the same as the one implemented in the code below. 
```


```{solution}
Code below is annotated with descriptions step by step
```

```julia
#Gram-Schmidt Conjugation
function conjGS(A,V; ϵ = 1e-15)
    """Implements classical Gram-Schmidt
    A-conjugation over a set of vectors
    in the vector V. Returns A-conjugated
    vectors using set in V"""
    
    ## Define function to get A-norm of vector
    normA(x) = sqrt(dot(x,A*x))
    
    ## This will hold conjugated set
    Q = Vector(length(V))

    counter = 1
    for j = 1:length(V)
        if counter == 1
            #Check if zero vector
            normA(V[j]) <= ϵ && continue

            #Otherwise add to Conjugated Set
            Q[1] = V[j]/normA(V[j]) # -- get A-norm of r (here, v) and calculate first conjugated vector

            counter+= 1
        else
            #Project
            proj = map(λ -> dot(Q[λ],A*V[j])*Q[λ] ,1:counter-1) # -- calculate elements that go into sum of s_j
            a = V[j] - sum(proj) # -- take the difference...

            #Check if zero vector
            normA(a) <= ϵ && continue

            #Otherwise add to Conjugated Set
            Q[counter] = a/normA(a) # -- ... and normalize

            counter +=1
        end
    end
    return Q
end
```


```{exercise, q1302}
Prove that CG converges in finitely many steps (in exact arithmetic).
```

```{solution}
Shewchuk 7.1 (pages 24-25)
```


```{exercise, q1303}
Show that the limit in example \@ref(exm:gradient-example) holds. 
```

```{solution}

??? 

```


```{exercise, q1304}
Characterize the relationship between the Jacobian and the gradient.
```

```{solution}

If the Jacobian exists, then the gradient exists, and the relationship is as follows:

$$J(f;x) = \begin{bmatrix}
             \frac{\partial f_{1}}{\partial x_{1}}   &
	       \frac{\partial f_{1}}{\partial x_{2}} &
               \cdots                                &
               \frac{\partial f_{1}}{\partial x_{n}} \\
	     \vdots                                  &
	       \vdots                                &
               \cdots                                &
	       \vdots \\
             \frac{\partial f_{m}}{\partial x_{1}}   &
	       \frac{\partial f_{m}}{\partial x_{2}} &
	       \cdots                                &
	       \frac{\partial f_{m}}{\partial x_{n}}
	   \end{bmatrix} = \begin{bmatrix} \nabla f_1(x)' \\
       \vdots \\
       \nabla f_m(x)'
       \end{bmatrix}$$

Note that the inverse is not true.
  
https://calculus.subwiki.org/wiki/Existence_of_partial_derivatives_not_implies_differentiable
    
```

```{exercise, q1305}
Using IFT: when will $F(x) = 0$ have an isolated solution? (i.e. a solution where there is an open set containing that solution with no other solutions)
```

```{solution}
When $m = n$, the neighborhood will be one dimensional and be only one point. 
```

```{exercise, q1306}
What happens if $F(z^*) = 0$ and $\text{rank}(J(z^*)) < \min(n,m)$?
```

```{solution}

If $rank(J(z^*)) < min(m,n)$, then $J_1(z^*)$ is not full rank and thus will be a singular matrix

```

## Homework 14

```{exercise, q1401}
Proof theorem \@ref(thm:contracts). Why does $D$ have to be closed? (Hint: need $\lim_{k \to \infty} x_k \in D$)
```

```{solution}

1. Assume $x^*$ and $x_0$ satisfy $x^* = G(x^*)$ and $G(x_0) = x_0$. Then there exists an $\alpha \in (0,1)$ such that $\norm{G(x^*) - G(x_0)} \le \alpha\norm{x^* - x_0}$. But $\alpha \norm{x^* - x_0} = \alpha\norm{G(x^*) - G(x_0)}$, so $\norm{G(x^*) - G(x_0)} \le \alpha \norm{G(x^*) - G(x_0)}$. Since $\alpha \in (0,1)$, this can only hold if $\norm{G(x^*) - G(x_0)} = 0$, so $x^* = G(x^*) = G(x_0) = x_0$. 

2. For some $\alpha \in (0,1)$:

$$\begin{aligned}
  \norm{G(x_{k+1}) - x_{k+1}} &= \norm{G(x_{k+1}) - G(x_k)} \\
                              &\le \alpha\norm{x_{k+1}-x_k} \\ 
                              &= \alpha\norm{G(x_k)-G(x_{k-1})} \\
                              &= \alpha^2\norm{x_k-x_{k-1}} \\ 
                              &\vdots \\
                              &= \alpha^k\norm{x_1-x_0}.
\end{aligned}$$
  
Hence, $\norm{G(x_k - x_k)} \to 0$ as $k \to \infty$, so $\lim x_k = \lim G(x_k)$, i.e. $\lim x_k = x^*$. 

```

```{exercise, q1402}
Does this hold if we replace $R^m$ with an arbitrary metric space? (Hint: no, need completeness)
```

```{solution}
The metric space has to be complete. Otherwise we are not sure that the limit is still in $D$.
```

```{exercise, q1403}
Let $G(x) = 0.5(x+\frac{4}{x})$. Solve $x = G(x)$ by hand. What is $G(x)$ doing? 
```

```{solution}

$$\begin{aligned}
  G(x) &= x \Rightarrow \\
  0.5(x + \frac{4}{x}) &= x \Rightarrow \\
  -0.5x + \frac{2}{x}) &= 0 \Rightarrow \\
  -0.5x^2 + 2 &= 0 \Rightarrow \\
  x^2 &= 4 \Rightarrow \\
  x = \pm 2
\end{aligned}$$

```

```{exercise, q1404}
Show that $G(x)$ is a contraction.
```

```{solution}
A function $G: \R^n \mapsto \R^n$ is a contraction on a closed set $D$ if it satisfies:
  
1. $x \in D \implies G(X) \in D$
2. $\exists \alpha \in (0, 1)$ s.t. $\forall~x,y \in D, \norm{G(x) - G(y)} \le \alpha \norm{x - y}$

The function $G(x) = 0.5(x + \frac{4}{x}) = \frac{x}{2} + \frac{2}{x} = \frac{x^2 +4}{2x}$ is a contraction over the closed set $D_1 = [1, \infty)$ and the closed set $D_2 = (-\infty, -1]$. Proof:

1. $G(x) = \frac{x^2 +4}{2x} > 1 ~\forall~x \ge 1$. Similarly, $G(x) = \frac{x^2 +4}{2x} < -1 ~\forall~x \le -1$. So $x \in D_1 \implies G(x) \in D_1$ and $x \in D_2 \implies G(x) \in D_2$.

2. Since $G(x)$ is scalar valued, $\norm{G(x)} = \card{G(x)}$. 

  $\begin{align*}
  \card{G(x) - G(y)} &= \card{\frac{x}{2} + \frac{2}{x} - \frac{y}{2} - \frac{2}{y}} \\
              &= \card{\frac{1}{2}(x - y) + \frac{2}{x} - \frac{2}{y}}
  \end{align*}$
  
  Within $D_1$, $x=1$ and $y=1$ maximize this quantity, so

  $\begin{align*}
  \card{G(x) - G(y)} &\le \card{\frac{1}{2}(x - y) + 2 - 2} \\
                     &= \frac{1}{2}\card{x - y}.
  \end{align*}$
  
  Similarly, within $D_2$, $x=-1$ and $y=-1$ maximize this quantity, so

  $\begin{align*}
  \card{G(x) - G(y)} &\le \card{\frac{1}{2}(x - y) - 2 + 2} \\
                     &= \frac{1}{2}\card{x - y}.
  \end{align*}$
  
  So on both $D_1$ and $D_2$, $G$ is a contraction with $\alpha = \frac{1}{2}$.
```

```{exercise, q1405}
What is the impact of the singular values of $J(x^*)$ on the "localness" of the results in theorem \@ref(thm:with-assumptions)?
```

```{solution}

As seen in theorem \@ref(thm:with-assumptions), $\norm{x_c - x^*} \le \min\left(\rho^*, \frac{1}{2\gamma \norm{J(x^*)^{-1}}}\right)$. Assuming we are using $\norm{\cdot}_2$, $\norm{J(x^*)^{-1}}_2 = \frac{1}{\sigma_n}$, where $\sigma_n$ is the largest singular value. So, a larger maximum singular value means the result is "more local".

```

```{exercise, q1406}
Why is the first result from theorem \@ref(thm:with-assumptions) important?
```

```{solution}
???
```

```{exercise, q1407}
Proof and interpret the following:

Let $A,B \in \R^{n \times m}$ and assume $A$ is non-singular. If for some $\epsilon \in (0,1)$, $\norm{AB - I} < 1 - \epsilon$, then $B$ is invertible and 

1. $\norm{A - B^{-1}} < (1-\epsilon)\norm{B^{-1}}$,
2. $\norm{B^{-1}} \le \frac{1}{\epsilon} \norm{A}$.

```

```{solution}

Note that $\sum_{k=0}^N (AB-I)^k AB = (AB-I)^N AB + AB$. Since $\norm{AB-I} < 1-\epsilon$, $\sum_{k=0}^\infty (AB-I)^k AB = AB$, so $\sum_{k=0}^\infty (AB-I)^k$ is the inverse of $AB$. Let $D$ denote that sum. Then $AB = D^{-1}$, which implies $B=(DA)^{-1}$. So $B^{-1} = DA$. Hence, $B$ is invertible.

Furthermore, $\norm{A-B^{-1}} = \norm{(AB-I)B^{-1}} \le \norm{AB-I}\norm{B^{-1}} < (1-\epsilon)\norm{B^{-1}}$, and

$$\begin{aligned}
\norm{B^{-1}} &\le \norm{A} \norm{D} \\ 
&\le \norm{\sum_{k=0}^\infty \norm{AB - I}^k} \\
&< \norm{A} \sum_{k=0}^{\infty} (1- \epsilon)^k \\
&= \norm{A}\frac{1}{1-(1-\epsilon)} = \frac{1}{\epsilon}\norm{A},
\end{aligned}$$
  

```

```{exercise, q1408}
What happens if I use the mean value theorem instead of Fundamental Theorem of Calculus? (Hint: remember we cannot generalize 1D MVT to multi D).
```

```{solution}
Tuo
```

```{exercise, q1409}
Consider Newtons method. Using FTC, $e = x-x^*$, proof that if $e_0 = x_0 - x^*$, and $e$ sufficiently small, then

$$
  \frac{3}{5}\kappa \left(J(x^*)\right)^{-1} \frac{\norm{e}}{\norm{e_0}} \le \frac{\norm{F(x)}}{\norm{F(x_0)}} \le \frac{5}{3}\kappa \left(J(x^*)\right) \frac{\norm{e}}{\norm{e_0}}
$$

```

```{solution}
???
```

```{exercise, q1410}
How should we choose $\tau_a, \tau_r$? 
```

```{solution}
???
```

```{exercise, q1411}
Guiding principles?
```

```{solution}
???
```

## Homework 15

````{exercise, 1501}
Prove $|f(y+h) - f(y) - \nabla f(y) | \le \frac{l}{2} \norm{h}^2$, when the gradient of $f$ is Lipschitz continuous.
```

```{solution}
Tuo
```

```{exercise, 1502}
If $f$ can be calculated at any input with accuracy $l_f u$ ($l_f > 0$), show that the right choice of $\epsilon$ is proportional to $\sqrt{u}$. I.e. this choice of $\epsilon$ give the most accuracy. (See the section [Forward Difference])
```

```{solution}
???
```

```{exercise, 1503}
Choose a simple differentiable function and test a bunch of $\epsilon$ and compare to $\epsilon = \sqrt{u}$. 
```

```{solution}
???
```

```{exercise, 1504}
How is Forward Difference Approximation done for $f: \R^m \to \R^n$? 
```

```{solution}
???
```

```{exercise, 1505}
Verify equation \@ref(eq:shermanmorrison).
```

```{solution}

$$\begin{aligned}
   (A + uv^T)(A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u}) &= AA^{-1} - \frac{AA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} + uv^TA^{-1} - \frac{uv^TA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} \\
   &= I + uv^TA^{-1} - \frac{uv^TA^{-1}uv^TA^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} \\
   & = I + uv^TA^{-1} - \frac{u(1+v^TA^{-1}u)v^TA^{-1}}{1+v^TA^{-1}u} \\
   & = I
\end{aligned}$$

```

```{exercise, 1506}
Use equation \@ref(eq:shermanmorrison) to explicitly express $J_c^{-1}$ in terms of $J_{-}^{-1}$.
```

```{solution}

$$\begin{aligned}
(J_c)^{-1} &= (J_- + \frac{(y_c - J_-s_c)s_c^T}{s_c^Ts_c})^{-1} \\
&= J_-^{-1} - \frac{J_-^{-1}(y_c - J_-s_c)\frac{s_c^T}{s_c^Ts_c}J_-^{-1}}{1+\frac{s_c^T}{s_c^Ts_c}J_-^{-1}(y_c - J_-s_c)} \\
&= J_-^{-1} - \frac{J_-^{-1}y_cs_c^TJ_-^{-1} - s_cs_c^TJ_-^{-1}}{s_c^TJ_-^{-1}y_c}
\end{aligned}$$

```

```{exercise, 1507}
Do we ever need to store $J_c$? 
```

```{solution}

No. 

```

## Homework 16

```{exercise, 1601}
Let $F: \R^m \to \R^n$ be everywhere differentiable such that in any closed ball, the derivative is bounded. 

Show that in any closed ball, $F$ is Lipschitz continuous. 
```

```{solution}

Use Taylors theorem and let $L = \sup_{\omega \in B}||\nabla F(\omega)||$:

$$F(x) = F(y) + \nabla F(y + \gamma(y-x))^T(y-x),\,\, for\,\, some\,\, \gamma \in (0,1)$$

$$\begin{aligned} ||F(x)-F(y)|| &= ||\nabla F(y + \gamma(y-x))^T(y-x)|| \\
& \leq ||\nabla F(y + \gamma(y-x))||\,||(y-x)|| \\
& \leq \sup_{\omega \in B}||\nabla F(\omega)||\,||(y-x)|| \\
& \leq L||(y-x)||
\end{aligned}$$

```

```{exercise, 1602}
Is $f(x) = |x|$ semi-smooth?
```

```{solution}
Yes. 

$f$ is locally Lipschitz for all $x \in \R$. For all $x \neq 0$, we can find a ball such that for all $y \in B$, $\text{sign}(y) = \text{sign}(x)$. So, if $x > 0, y,z \in B$, $|f(z) - f(y)| = |z - y|$. If $x < 0, z,y \in B$, $|f(z) - f(y)| = |-z + y| = |z-y|$. 

If $x = 0$, we can find $z,y \in B$ such that $\text{sign}(z) \neq \text{sign}(y)$. Assume, without loss of generality, that $z < 0 < y$. Then, $|f(z) - f(y)| = |-z - y| \le |z - y|$. So, in any case, $f$ is Lipshitz. 

To verify second condition, let $\epsilon > 0$. If $x \neq 0$, pick a ball $B$ around $x$ such that $\text{sign}(y) = 1$ for all $y \in B$. If $x > 0$, $\partial f(u) = 1$ for all $u \in B$. $\norm{f(u) - f(x) - v(u-x)} = \norm{u-x-u+x} = 0$ ($v = 1$). If $x < 0$, $\partial f(u) = -1$ for all $u \in B$. Then $\norm{f(u) - f(x) - v(u-x)} = \norm{-u+x+u-x} = 0$ ($v = -1$). If $x=0$, any ball around $x$ will contain both positive and negative values. In either case, $\norm{f(u) - f(x) - v(u-x)} = 0$ ($v = \text{sign}(u), x=0$). 

```

```{exercise, 1603}
Is $f(x) = \log(1+|x|)$ semi-smooth?
```

```{solution}
No, $f$ is not locally Lipschitz.
```

```{exercise, 1604}

Prove that Newtons Method converges Q superlinearly. 

```

```{solution}
???
```

### Newton's Method

The goal of this exercise is to implement Newtons method and its variants to solve a nonlinear system, and to acquaint you with their properties.

You will solve for the zeros of $F\left(x,y\right)$ which has components given by

$$\begin{aligned}
  F_1(x,y) &= (1.5 - x + xy)(y-1) + (2.25 - x + xy^2)(y^2 - 1) + (2.625 - x + xy^3)(y^3-1) \\
  F_2(x,y) &= (1.5 - x + xy)x + (2.25 - x + xy^2)(2xy) + (2.625 - x + xy^3)(3y^2x).
\end{aligned}$$  
    
Stay within $[-4.5, 4.5]^2$. 
  
Compare the following methods: Newtons Method, Chord Method, Forward Difference Method, Broyden Method.

You will need to also do line search. The code I supplied implements Newtons method with a slightly different version of line search. Implement the one I discussed in class as well.

```{exercise, 1605}
Compare the performance of the different methods and the different line search methods.
```

```{exercise, 1606}
Track the step length for each method. What happens to the step length as you get closer to the solution. (Make some plots)
```

## Homework 17

Proof or find counter example for the following four problems:

```{exercise, q1701}
If $\theta^*$ is a global minimizer, it is a local minimizer.
```

```{solution}
Tuo
```

```{exercise, q1702}
If $\theta^*$ is a strict global minimizer, it is the only global minimizer.
```

```{solution}
Tuo
```

```{exercise, q1703}
If $\theta^*$ is an isolated global minimizer, it is the only global minimizer.
```

```{solution}
Tuo
```

```{exercise, q1704}
If $\theta^*$ is a strict local minimizer, it is isolated. #there exists a neighborhood around $\theta^*$ such that $\theta^*$ is isolated in said neighborhood.
```

```{solution}
Tuo
```

```{exercise, q1705}
If $\theta^*$ is an isolated minimizer, then $\theta^*$ is a strict minimizer. 
```

```{solution}
Tuo
```

```{exercise, q1706}
Is the condition in lemma \@ref(lem:firstordernec) sufficient?
```

```{solution}
No. $f(x) = x^3$.
```

```{exercise, q1707}
Fill in the blanks of lemma \@ref(lem:convexdiff)!
```

## Homework 18

Proof or find counter examples for each of the following three problems:

```{exercise, q1801}
Let $f \in C^1$. 

$f$ is convex if and only if for all $x,y \in \R^p:\ \left(\nabla f(x) - \nabla f(y)\right)(x-y) \ge 0$. 
```

```{solution, s1801}

$(\Rightarrow)$

Use this Lemma: Suppose $f \in C^1$, $f$ is convex $\iff$ $f(y) \geq f(x) + \nabla f(x)^T(y-x)$, $\forall x,y \in \mathbb{R}^p$.

We have, 

$$f(y) \geq f(x) + \nabla f(x)^T(y-x)$$
$$f(x) \geq f(y) + \nabla f(y)^T(x-y)$$

Sum these two inequalities together 

$$ f(y) + f(x) \geq f(x)+f(y)+ (\nabla f(x) - \nabla f(y))^T(y-x)$$

Thus, 
$$(\nabla f(x)-\nabla f(y))^T(x-y) \geq 0$$

$(\Leftarrow)$

For the converse part, we need to use Taylor Theorem:
$$f(x+p) = f(x) + \int_{0}^{1}\nabla f(x+\gamma p)^Tp d\gamma$$
$$f(x+p) = f(x) + \nabla f(x+\gamma p)^Tp, \,\,\, for\,some\, \gamma \in (0,1)$$

If $\forall x, y\in \mathbb{R}^p$, $(\nabla f(x)-\nabla f(y))^T(x-y) \geq 0$, Let $\lambda \in [0,1]$, $z = \lambda x + (1 - \lambda)y$,

$$
\begin{aligned} f(x) &= f(z) + \int_{0}^{1}\nabla f(z+\gamma (z-x))^T(z-x) d\gamma \\
&= f(z) + \int_{0}^{1}\nabla f(z+\gamma (z-x))^T (1-\lambda)(x-y) d\gamma 
\end{aligned}$$

Similarly,

$$\begin{aligned}
f(y) & = f(z) + \int_{0}^{1}\nabla f(z+\gamma (z-y))^T(z-y) d\gamma \\
& = f(z) + \int_{0}^{1}\nabla f(z+\gamma (z-y))^T \lambda (y-x) d\gamma
\end{aligned}$$

Let the first inequality times $\lambda$ and let the second one times $(1-\lambda)$ we will get

$$\begin{aligned}
\lambda f(x) + (1-\lambda) f(y) &= f(z) - \lambda (1-\lambda) \int_{0}^{1} (f(z+\gamma (z-x)) - f(z+\gamma (z-y)))^T(x-y) d\gamma \\
& = f(z) - \lambda (1-\lambda) \int_{0}^{1} (f(z+\gamma (z-x)) - f(z+\gamma (z-y)))^T[(z+\gamma (z-y))-(z+\gamma (z-x)))] \frac{1}{\gamma} d\gamma  \\
& \leq f(z) \\
& = f(\lambda x + (1 - \lambda)y)
\end{aligned}$$

```

```{exercise, q1802}
Let $f \in C^2$. 

$f$ is convex if and only if for all $x \in \R^p:\ \nabla^2 f(x) \ge 0$.
```

```{solution}
We need to use the second other taylor theorem:

$$f(x + p) = f(x) + \nabla f(x)^Tp + \frac{1}{2} p^T\nabla^2 f(x+\gamma p)p,\,\,\, some\,\gamma \in (0,1)$$

and this lemma:

Suppose $f \in C^1$, $f$ is convex $\iff$ $f(y) \geq f(x) + \nabla f(x)^T(y-x)$, $\forall x,y \in \mathbb{R}^p$.

$(\Rightarrow)$

For this part of proof, refer to the book Convex Optimization (Stephen Boyd), Page 71 and Exercise 3.8

I attached a picture in the following cell

$(\Leftarrow)$

From taylor theorem: 

$$\begin{aligned}
f(y) &= f(x) + \nabla f(x)^T(y-x) + \frac{1}{2} (y-x)^T\nabla^2 f(x+\gamma(y-x))(y-x)\\
& \geq f(x) + \nabla f(x)^T(y-x)
\end{aligned}$$

```

```{exercise, q1803}
Let $f \in C^1$. 

$f$ is strongly convex with $\sigma > 0$ if and only if $\left(\nabla f(x) - \nabla f(y) \right)^\prime(x-y) \ge \sigma \norm{x-y}^2_2$. (Hint: integral form of Taylor's Theorem)

```

```{solution}
Tuo has a reference.

We need to use this lemma:

Suppose $f \in C^1$, $f$ is strongly convex $\iff$ $f(y) \geq f(x) + \nabla f(x)^T(y-x) + \frac{1}{2}\sigma ||x-y||_2^2$, $\forall x,y \in \mathbb{R}^p$.
 
$(\Rightarrow)$
 
Use the Lemma twice and sum them together to get the desired result.
 
$(\Leftarrow)$

Same idea as solution to \@ref(exr:q1801).
  
```

```{exercise, q1804}
(1) implies (2) of lemma \@ref(lem:convexlipsch). Application of Taylor's theorem in integral form.
```

```{solution}
Tuo
```

```{exercise, q1805}
(3) implies (4) of lemma \@ref(lem:convexlipsch).  
```

```{solution}
Tuo
```

```{exercise, q1806}

Let $f \in C^2$ be a convex function with a Lipschitz continuous gradient. Then $L > 0$ implies $\nabla^2 f(x) \le L$.

```

```{solution}
Tuo
```

## Homework 19

```{exercise, q1901}

Shows that 

$$
  \alpha_k^{cp} = \left \{ \begin{array}{rl} \min\left(\frac{\norm{g_k}_2^2}{g_k^\prime B_k g_k}, \frac{\Delta_k}{\norm{g_k}_2^2}) & \text{ if } g_k^\prime B_k g_k > 0 \\ \frac{\Delta_k}{\norm{g_k}}_2  & \text{ otherwise} \end{array}\right .
$$
      
minimizes equation \@ref(eq:trustregion)

```